<!DOCTYPE html><html lang="en" class="" style="scroll-padding:60px"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width,initial-scale=1"/><title>Mitigating prediction drift in harmful algal bloom forecasting: A hybrid deep learning and ensemble Kalman filter framework</title><meta property="og:title" content="Mitigating prediction drift in harmful algal bloom forecasting: A hybrid deep learning and ensemble Kalman filter framework"/><meta name="generator" content="mystmd"/><meta name="keywords" content="harmful algae blooms, red tides, machine learning, deep learning, ensemble Kalman filter, LSTM, uncertainty analysis, data assimilation, Florida"/><link rel="stylesheet" href="/build/_assets/app-MOQGDXHO.css"/><link rel="stylesheet" href="/build/_assets/thebe-core-VKVHG5VY.css"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jupyter-matplotlib@0.11.3/css/mpl_widget.css"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.css"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous"/><link rel="icon" href="/favicon.ico"/><link rel="stylesheet" href="/myst-theme.css"/><script>
  const savedTheme = localStorage.getItem("myst:theme");
  const theme = window.matchMedia("(prefers-color-scheme: light)").matches ? 'light' : 'dark';
  const classes = document.documentElement.classList;
  const hasAnyTheme = classes.contains('light') || classes.contains('dark');
  if (!hasAnyTheme) classes.add(savedTheme ?? theme);
</script></head><body class="m-0 transition-colors duration-500 bg-white dark:bg-stone-900"><div class="myst-skip-to-article fixed top-1 left-1 h-[0px] w-[0px] focus-within:z-40 focus-within:h-auto focus-within:w-auto bg-white overflow-hidden focus-within:p-2 focus-within:ring-1" aria-label="skip to content options"><a href="#skip-to-frontmatter" class="myst-skip-to-link block px-2 py-1 text-black underline">Skip to article frontmatter</a><a href="#skip-to-article" class="myst-skip-to-link block px-2 py-1 text-black underline">Skip to article content</a></div><dialog id="myst-no-css" style="position:fixed;left:0px;top:0px;width:100vw;height:100vh;font-size:4rem;padding:1rem;color:black;background:white"><strong>Site not loading correctly?</strong><p>This may be due to an incorrect <code>BASE_URL</code> configuration. See<!-- --> <a href="https://mystmd.org/guide/deployment#deploy-base-url">the MyST Documentation</a> <!-- -->for reference.</p><script>
    (() => {
            // Test for has-styling variable set by the MyST stylesheet
            const node = document.currentScript.parentNode;
            const hasCSS = window.getComputedStyle(node).getPropertyValue("--has-styling");
            if (hasCSS === ""){
                    node.showModal();
            }

    })()
</script></dialog><div class="myst-top-nav bg-white/80 backdrop-blur dark:bg-stone-900/80 shadow dark:shadow-stone-700 p-3 md:px-8 sticky w-screen top-0 z-30 h-[60px]"><nav class="myst-top-nav-bar flex items-center justify-between flex-nowrap max-w-[1440px] mx-auto"><div class="flex flex-row xl:min-w-[19.5rem] mr-2 sm:mr-7 justify-start items-center shrink-0"><div class="block xl:hidden"><button class="myst-top-nav-menu-button flex items-center justify-center border-stone-400 text-stone-800 hover:text-stone-900 dark:text-stone-200 hover:dark:text-stone-100 w-10 h-10"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" width="1.5rem" height="1.5rem"><path fill-rule="evenodd" d="M3 6.75A.75.75 0 0 1 3.75 6h16.5a.75.75 0 0 1 0 1.5H3.75A.75.75 0 0 1 3 6.75ZM3 12a.75.75 0 0 1 .75-.75h16.5a.75.75 0 0 1 0 1.5H3.75A.75.75 0 0 1 3 12Zm0 5.25a.75.75 0 0 1 .75-.75h16.5a.75.75 0 0 1 0 1.5H3.75a.75.75 0 0 1-.75-.75Z" clip-rule="evenodd"></path></svg><span class="sr-only">Open Menu</span></button></div><a class="myst-home-link flex items-center ml-3 dark:text-white w-fit md:ml-5 xl:ml-7" href="/"><span class="text-md sm:text-xl tracking-tight sm:mr-5">Made with MyST</span></a></div><div class="flex items-center flex-grow w-auto"><div class="flex-grow hidden text-md lg:block"></div><div class="flex-grow block"></div><button type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:R75cp:" data-state="closed" class="myst-search-bar flex items-center h-10 aspect-square sm:w-64 text-left text-gray-600 dark:text-gray-300 border border-gray-300 dark:border-gray-600 rounded-lg bg-gray-50 dark:bg-gray-700 myst-search-bar-disabled hover:ring-blue-500 dark:hover:ring-blue-500 hover:border-blue-500 dark:hover:border-blue-500"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="p-2.5 h-10 w-10 aspect-square"><path fill-rule="evenodd" d="M10.5 3.75a6.75 6.75 0 1 0 0 13.5 6.75 6.75 0 0 0 0-13.5ZM2.25 10.5a8.25 8.25 0 1 1 14.59 5.28l4.69 4.69a.75.75 0 1 1-1.06 1.06l-4.69-4.69A8.25 8.25 0 0 1 2.25 10.5Z" clip-rule="evenodd"></path></svg><span class="myst-search-text-placeholder hidden sm:block grow">Search</span><div aria-hidden="true" class="myst-search-shortcut items-center hidden mx-1 font-mono text-sm text-gray-600 dark:text-gray-300 sm:flex gap-x-1"><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none hide-mac">CTRL</kbd><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none show-mac">⌘</kbd><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none ">K</kbd><script>
;(() => {
const script = document.currentScript;
const root = script.parentElement;

const isMac = /mac/i.test(
      window.navigator.userAgentData?.platform ?? window.navigator.userAgent,
    );
root.querySelectorAll(".hide-mac").forEach(node => {node.classList.add(isMac ? "hidden" : "block")});
root.querySelectorAll(".show-mac").forEach(node => {node.classList.add(!isMac ? "hidden" : "block")});
})()</script></div></button><button class="myst-theme-button theme rounded-full aspect-square border border-stone-700 dark:border-white hover:bg-neutral-100 border-solid overflow-hidden text-stone-700 dark:text-white hover:text-stone-500 dark:hover:text-neutral-800 w-10 h-10 mx-3" title="Toggle theme between light and dark mode" aria-label="Toggle theme between light and dark mode"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="myst-theme-moon-icon h-full w-full p-0.5 hidden dark:block"><path fill-rule="evenodd" d="M9.528 1.718a.75.75 0 0 1 .162.819A8.97 8.97 0 0 0 9 6a9 9 0 0 0 9 9 8.97 8.97 0 0 0 3.463-.69.75.75 0 0 1 .981.98 10.503 10.503 0 0 1-9.694 6.46c-5.799 0-10.5-4.7-10.5-10.5 0-4.368 2.667-8.112 6.46-9.694a.75.75 0 0 1 .818.162Z" clip-rule="evenodd"></path></svg><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="myst-theme-sun-icon h-full w-full p-0.5 dark:hidden"><path stroke-linecap="round" stroke-linejoin="round" d="M12 3v2.25m6.364.386-1.591 1.591M21 12h-2.25m-.386 6.364-1.591-1.591M12 18.75V21m-4.773-4.227-1.591 1.591M5.25 12H3m4.227-4.773L5.636 5.636M15.75 12a3.75 3.75 0 1 1-7.5 0 3.75 3.75 0 0 1 7.5 0Z"></path></svg></button><div class="block sm:hidden"><div class="myst-action-menu relative" data-headlessui-state=""><div><button class="myst-action-menu-button flex text-sm bg-transparent rounded-full focus:outline-none" id="headlessui-menu-button-:Rr5cp:" type="button" aria-haspopup="menu" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Open Menu</span><div class="flex items-center text-stone-200 hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" width="2rem" height="2rem" class="p-1"><path fill-rule="evenodd" d="M10.5 6a1.5 1.5 0 1 1 3 0 1.5 1.5 0 0 1-3 0Zm0 6a1.5 1.5 0 1 1 3 0 1.5 1.5 0 0 1-3 0Zm0 6a1.5 1.5 0 1 1 3 0 1.5 1.5 0 0 1-3 0Z" clip-rule="evenodd"></path></svg></div></button></div></div></div><div class="hidden sm:block"><a class="inline-block px-4 py-2 mx-1 mt-0 leading-none border rounded text-md border-stone-700 dark:border-white text-stone-700 dark:text-white hover:text-stone-500 dark:hover:text-neutral-800 hover:bg-neutral-100" href="/build/data_weekly_interpol-99ca3242edec19edf7b1cbdf70a6aed8.csv">Download Dataset</a></div></div></nav></div><div class="myst-primary-sidebar fixed xl:article-grid grid-gap xl:w-screen xl:pointer-events-none overflow-auto max-xl:min-w-[300px] hidden z-10" style="top:60px"><div class="myst-primary-sidebar-pointer pointer-events-auto xl:col-margin-left flex-col overflow-hidden hidden xl:flex"><div class="myst-primary-sidebar-nav flex-grow py-6 overflow-y-auto primary-scrollbar"><nav aria-label="Navigation" class="myst-primary-sidebar-topnav overflow-y-hidden transition-opacity ml-3 xl:ml-0 mr-3 max-w-[350px] lg:hidden"><div class="w-full px-1 dark:text-white font-medium"></div></nav><div class="my-3 border-b-2 lg:hidden"></div><nav aria-label="Table of Contents" class="myst-primary-sidebar-toc flex-grow overflow-y-hidden transition-opacity ml-3 xl:ml-0 mr-3 max-w-[350px]"><div class="myst-toc w-full px-1 dark:text-white"><a title="Mitigating prediction drift in harmful algal bloom forecasting: A hybrid deep learning and ensemble Kalman filter framework" aria-current="page" class="block break-words focus:outline outline-blue-200 outline-2 rounded myst-toc-item p-2 my-1 rounded-lg hover:bg-slate-300/30 font-bold active" href="/">Mitigating prediction drift in harmful algal bloom forecasting: A hybrid deep learning and ensemble Kalman filter framework</a></div></nav></div><div class="myst-primary-sidebar-footer flex-none py-6 transition-all duration-700 translate-y-6 opacity-0"><a class="myst-made-with-myst flex mx-auto text-gray-700 w-fit hover:text-blue-700 dark:text-gray-200 dark:hover:text-blue-400" href="https://mystmd.org/made-with-myst" target="_blank" rel="noreferrer"><svg style="width:24px;height:24px" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100" stroke="none"><g id="icon"><path fill="currentColor" d="M23.8,54.8v-3.6l4.7-0.8V17.5l-4.7-0.8V13H36l13.4,31.7h0.2l13-31.7h12.6v3.6l-4.7,0.8v32.9l4.7,0.8v3.6h-15
          v-3.6l4.9-0.8V20.8H65L51.4,53.3h-3.8l-14-32.5h-0.1l0.2,17.4v12.1l5,0.8v3.6H23.8z"></path><path fill="#F37726" d="M47,86.9c0-5.9-3.4-8.8-10.1-8.8h-8.4c-5.2,0-9.4-1.3-12.5-3.8c-3.1-2.5-5.4-6.2-6.8-11l4.8-1.6
          c1.8,5.6,6.4,8.6,13.8,8.8h9.2c6.4,0,10.8,2.5,13.1,7.5c2.3-5,6.7-7.5,13.1-7.5h8.4c7.8,0,12.7-2.9,14.6-8.7l4.8,1.6
          c-1.4,4.9-3.6,8.6-6.8,11.1c-3.1,2.5-7.3,3.7-12.4,3.8H63c-6.7,0-10,2.9-10,8.8"></path></g></svg><span class="self-center ml-2 text-sm">Made with MyST</span></a></div></div></div><main class="article-grid grid-gap"><article class="article-grid subgrid-gap col-screen article content"><div class="hidden"></div><div id="skip-to-frontmatter" aria-label="article frontmatter" class="myst-fm-block mb-8 pt-9"><div class="myst-fm-block-header flex items-center mb-5 h-6 text-sm font-light"><div class="flex-grow"></div><div class="myst-fm-block-badges"></div><div class="myst-fm-downloads-dropdown relative flex inline-block mx-1 grow-0" data-headlessui-state=""><button class="myst-fm-downloads-button relative ml-2 -mr-1" id="headlessui-menu-button-:Rs8ucp:" type="button" aria-haspopup="menu" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Downloads</span><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.25rem" height="1.25rem" class="myst-fm-downloads-icon"><title>Download</title><path stroke-linecap="round" stroke-linejoin="round" d="M3 16.5v2.25A2.25 2.25 0 0 0 5.25 21h13.5A2.25 2.25 0 0 0 21 18.75V16.5M16.5 12 12 16.5m0 0L7.5 12m4.5 4.5V3"></path></svg></button></div></div><h1 class="myst-fm-block-title mb-0">Outline</h1><header class="myst-fm-authors-affiliations mt-4 not-prose"><div class="myst-fm-authors-list"><span class="myst-fm-author font-semibold text-sm myst-fm-author-item inline-block myst-fm-author-comma text-comma"><button class="myst-fm-author-popover focus:shadow-[0_0_0_2px] focus:shadow-black outline-none hover:underline" aria-label="Author Details" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:Rj8ucp:" data-state="closed"><span class="myst-fm-author-name">Victor M. Blanco</span></button></span><span class="myst-fm-author font-semibold text-sm myst-fm-author-item inline-block myst-fm-author-comma text-comma"><button class="myst-fm-author-popover focus:shadow-[0_0_0_2px] focus:shadow-black outline-none hover:underline" aria-label="Author Details" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:Rl8ucp:" data-state="closed"><span class="myst-fm-author-name">Daniel Krutky</span></button></span><span class="myst-fm-author font-semibold text-sm myst-fm-author-item inline-block myst-fm-author-comma text-comma"><button class="myst-fm-author-popover focus:shadow-[0_0_0_2px] focus:shadow-black outline-none hover:underline" aria-label="Author Details" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:Rn8ucp:" data-state="closed"><span class="myst-fm-author-name">Peter Nguyen</span></button></span><span class="myst-fm-author font-semibold text-sm myst-fm-author-item inline-block myst-fm-author-comma text-comma"><button class="myst-fm-author-popover focus:shadow-[0_0_0_2px] focus:shadow-black outline-none hover:underline" aria-label="Author Details" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:Rp8ucp:" data-state="closed"><span class="myst-fm-author-name">Ahmed S. Elshall</span></button></span><span class="myst-fm-author font-semibold text-sm myst-fm-author-item inline-block myst-fm-author-comma text-comma"><button class="myst-fm-author-popover focus:shadow-[0_0_0_2px] focus:shadow-black outline-none hover:underline" aria-label="Author Details" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:Rr8ucp:" data-state="closed"><span class="myst-fm-author-name">Ming Ye</span></button></span><span class="myst-fm-author font-semibold text-sm myst-fm-author-item inline-block"><button class="myst-fm-author-popover focus:shadow-[0_0_0_2px] focus:shadow-black outline-none hover:underline" aria-label="Author Details" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:Rt8ucp:" data-state="closed"><span class="myst-fm-author-name">Michael L. Parsons</span></button></span></div></header></div><div class="block my-10 lg:sticky lg:z-10 lg:h-0 lg:pt-0 lg:my-0 lg:ml-10 lg:col-margin-right" style="top:60px"><nav></nav></div><div id="skip-to-article"></div><div id="dc526dc0" class="myst-jp-nb-block relative group/block"><p>This project presents a novel integration of Deep Learning and Sequential Data Assimilation to forecast Karenia brevis (red tide) blooms. While LSTMs are powerful at capturing non-linear temporal dependencies, they often suffer from “drift” when applied to long-term forecasting without mid-course corrections. Our framework solves this by treating the LSTM as the “transition model” within an Ensemble Kalman Filter (EnKF) loop.</p><p>Research Objectives
The primary goal of this research is to develop a robust, self-correcting forecasting system for harmful algal blooms along the Florida coast.</p><p>Architecture Design: Construct an LSTM neural network optimized for high-dimensional environmental time-series and extreme class imbalance.</p><p>Hybrid Integration: Develop a mathematical bridge that allows LSTM state outputs to be assimilated and corrected by an Ensemble Kalman Filter (EnKF) in real-time.</p><p>Uncertainty Quantification: Implement Monte Carlo (MC) Dropout to shift from deterministic “point-forecasts” to probabilistic risk assessments.</p><p>Ecological Interpretability: Quantify the sensitivity of the model to specific environmental drivers like nutrient loading and river discharge using SHAP values.</p><p>Research Questions
To evaluate the effectiveness of this hybrid framework, this project seeks to answer:</p><p>Can data assimilation mitigate LSTM drift? To what extent does the periodic injection of physical observations via EnKF improve the long-term stability of K. brevis forecasts?</p><p>How does ecological memory impact accuracy? Does the inclusion of long-term rolling aggregates and multi-day lags significantly outperform models relying only on immediate environmental snapshots?</p><p>What is the “Reliability-Resolution” trade-off? How well does the MC Dropout ensemble capture the actual variance of bloom occurrences?</p><p>Which environmental drivers dominate the model’s decision-making? Does the model prioritize nutrient concentrations, physical transport (discharge), or biological persistence when predicting a bloom?</p><p>Technical Roadmap</p><ol start="1"><li><p>Data Foundation &amp; Enhanced Engineering
The predictive power of this framework rests on capturing the “ecological memory” of the Florida coast.</p></li></ol><p>Feature Engineering (Parts 1-4): We move beyond raw concentrations to create Rolling Aggregates (7, 14, and 30-day windows) and Time-Lagged Variables. This allows the model to “see” nutrient accumulation trends and delayed river discharge impacts.</p><p>Robust Preprocessing: Using RobustScaler, we ensure that extreme outliers—common in nutrient data during hurricane or high-flow events—do not skew the model’s weight distribution.</p><ol start="2"><li><p>The LSTM Predictive Engine
Architecture &amp; Tuning (Parts 5-9): We implement a stacked LSTM architecture designed to process 3D temporal sequences. To combat extreme class imbalance, we utilize Custom Class Weighting in the loss function to penalize the misclassification of rare bloom events more heavily.</p></li></ol><p>Hyperparameter Optimization: We utilize Bayesian optimization to determine the ideal hidden layer depth, dropout rates, and optimal sequence length (lookback).</p><ol start="3"><li><p>The Hybrid Loop: Data Assimilation (EnKF)
This is the core innovation: anchoring deep learning predictions to physical reality.</p></li></ol><p>The Forecast Step: The LSTM generates a state estimate for the next time step.</p><p>The Assimilation Step (Parts 10-12): When a new physical observation (e.g., a nitrogen measurement) becomes available, the EnKF calculates the Kalman Gain. This corrects the model’s internal state variables before the next recursive forecast, effectively resetting the “drift.”</p><p>MC Dropout: By keeping dropout active during inference, we generate an ensemble of predictions to quantify Epistemic Uncertainty.</p><ol start="4"><li><p>Advanced Evaluation &amp; Explainability
Probabilistic Evaluation (Part 13): We evaluate using Brier Skill Scores to measure the calibration of our probability forecasts and Precision-Recall Curves to assess bloom detection reliability.</p></li></ol><p>Opening the Black Box (Parts 14-15): * SHAP Analysis: Identifying which specific nutrients are driving “high probability” forecasts.</p><p>TLCC Analysis: Validating the lead-time of features to ensure the LSTM is learning biologically plausible causal relationships.</p></div><div id="vtgtg-lx-k5t" class="myst-jp-nb-block relative group/block"><h2 id="library-installation" class="relative group"><span class="heading-text">Library installation</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#library-installation" title="Link to this Section" aria-label="Link to this Section">¶</a></h2></div><div id="iuwfypopie1" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div id="iuwfypopie1-code" class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">!pip install tensorflow.keras
!pip install keras_tuner
import json
import os

import joblib
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import shap
import tensorflow as tf
from sklearn.impute import SimpleImputer
from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score,
                             brier_score_loss, accuracy_score, precision_score,
                             recall_score, f1_score)
from sklearn.metrics import (precision_recall_curve)
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.utils.class_weight import compute_class_weight
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.metrics import BinaryAccuracy, AUC, Recall, Precision
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
try:
    from tqdm.notebook import tqdm
    print(&quot;Using tqdm.notebook for progress bars.&quot;)
except ImportError:
    try:
        from tqdm import tqdm
        print(&quot;Using standard tqdm for progress bars.&quot;)
    except ImportError:
        print(&quot;Warning: tqdm not installed. Progress bars will not be shown.&quot;)
        # Define a dummy tqdm function if not installed
        def tqdm(iterable=None, *args, **kwargs):
            if iterable is None:
                # Handle case where tqdm is called without an iterable
                class DummyTqdm:
                    def __enter__(self): return self
                    def __exit__(self, *args): pass
                    def update(self, n=1): pass
                    def close(self): pass
                    def set_description(self, desc): pass
                return DummyTqdm()
            else:
                return iterable

print(&quot;Imported libraries.&quot;)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" id="iuwfypopie1_-outputs" data-mdast-node-id="y94gKTDEspEHt5sU5JZpX" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="id-2ngbxksqi50i" class="myst-jp-nb-block relative group/block"><h3 id="part-1-configuration-setup" class="relative group"><span class="heading-text">Part 1: Configuration &amp; Setup</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#part-1-configuration-setup" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><ul><li><p>Define all file paths (input data, saved components like scaler/model).</p></li><li><p>Set core parameters: <code>SEQUENCE_LENGTH</code>, <code>FORECAST_HORIZON</code>, <code>BLOOM_THRESHOLD</code>.</p></li><li><p>Define feature lists: <code>BASE_FEATURES</code>, <code>ENKF_STATE_VARS</code>, <code>ENHANCED_KB_LAGS</code> (optional).</p></li><li><p><strong>Control Flags/Options:</strong></p><ul><li><p><code>USE_ENHANCED_FEATURES</code>: Boolean (True/False) to switch between basic and enhanced feature engineering.</p></li><li><p><code>SCALER_TYPE</code>: String (‘Standard’ or ‘Robust’) to choose the scaler.</p></li><li><p><code>USE_CLASS_WEIGHT</code>: Boolean (True/False) to enable/disable class weighting during training.</p></li><li><p><code>PERFORM_TUNING</code>: Boolean (True/False) to run hyperparameter tuning or use defaults.</p></li></ul></li><li><p>Set tuning parameters: <code>MAX_TRIALS</code>, <code>TUNER_EPOCHS</code>.</p></li><li><p>Set training parameters: <code>EPOCHS</code>, <code>BATCH_SIZE</code>, <code>PATIENCE</code>.</p></li><li><p>Set EnKF parameters: <code>N_ENKF</code> (ensemble size), define noise matrix estimation approach (e.g., ‘basic_stats’ or ‘manual’).</p></li><li><p>Set MC Dropout parameters: <code>N_MC_SAMPLES</code>.</p></li></ul></div><div id="vp1-kgorslgp" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div id="vp1-kgorslgp-code" class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># Set these flags to control the workflow execution
# === Feature Engineering ===
# Set to True to use detailed lags (like RF paper), False: Use basic lags
USE_ENHANCED_FEATURES = True

# === Preprocessing ===
# Options: &#x27;Robust&#x27; or &#x27;Standard&#x27;
SCALER_TYPE = &#x27;Robust&#x27;

# === Training ===
# Set to True to apply class weighting during LSTM training
USE_CLASS_WEIGHT = True
# Set to True to run KerasTuner hyperparameter search, False to use default HPs
PERFORM_TUNING = True

# === Advanced Steps ===
# Set to True to run EnKF data assimilation during testing/forecasting
PERFORM_ENKF = True</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" id="vp1-kgorslgp-outputs" data-mdast-node-id="_zS-G4kyLNSzrzojOa8ms" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="b5euahjksrfs" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div id="b5euahjksrfs-code" class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># --- File Paths ---
INPUT_DATA_PATH = &#x27;data_weekly_interpolated.csv&#x27;
# Directory for optional external files (e.g., Caloosahatchee) - Ensure this exists if used
EXTERNAL_DATA_DIR = &#x27;external_data/&#x27;
# Directory to save results, models, scalers (use a distinct name)
OUTPUT_DIR = &#x27;output_refactored/&#x27;
# Construct filenames dynamically based on config where appropriate
scaler_suffix = SCALER_TYPE.lower()
feature_suffix = &#x27;enhanced&#x27; if USE_ENHANCED_FEATURES else &#x27;basic&#x27;
SCALER_FILENAME = os.path.join(OUTPUT_DIR, f&#x27;red_tide_scaler_{scaler_suffix}.joblib&#x27;)
FEATURE_LIST_FILENAME = os.path.join(OUTPUT_DIR, f&#x27;red_tide_feature_list_{feature_suffix}.joblib&#x27;)
# Template for sequence files: {horizon}{feature_suffix}
SEQUENCES_FILENAME_TEMPLATE = os.path.join(OUTPUT_DIR, &#x27;sequences_horizon{}wk_{}.npz&#x27;)
# Template for model checkpoint files: {model_type} e.g., baseline_weighted
# Using .keras extension for saving the full model (architecture + weights + optimizer state)
MODEL_CHECKPOINT_TEMPLATE = os.path.join(OUTPUT_DIR, &#x27;best_lstm_model_{}.keras&#x27;)
# Use .weights.h5 if saving only weights (e.g., for subclassed models like physics-informed)
MODEL_WEIGHTS_CHECKPOINT_TEMPLATE = os.path.join(OUTPUT_DIR, &#x27;best_lstm_model_{}.weights.h5&#x27;)

TUNER_PROJECT_DIR = &#x27;keras_tuner_dir_refactored&#x27; # Use a new name
TUNER_PROJECT_NAME = &#x27;red_tide_lstm_tuning&#x27;

# Create output directory if it doesn&#x27;t exist
os.makedirs(OUTPUT_DIR, exist_ok=True)
print(f&quot;Output directory: {OUTPUT_DIR}&quot;)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" id="b5euahjksrfs-outputs" data-mdast-node-id="r_D-DzDmGcWAkXWbvy65F" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>Output directory: output_refactored/
</span></code></pre></div></div></div></div><div id="id-6wge-lhas0r" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div id="id-6wge-lhas0r-code" class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># --- Core Parameters ---
DATETIME_COL = &#x27;time&#x27;
TARGET_COL = &#x27;kb&#x27; # Original cell count column
TARGET_BINARY_COL = &#x27;bloom_target&#x27; # Binary target column name
BLOOM_THRESHOLD = 1e5 # Cells/L
SEQUENCE_LENGTH = 12  # Default sequence length (can be tuned)
FORECAST_HORIZON = 1   # Default forecast horizon (1-week or 4-week)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" id="6wge-lhas0r_-outputs" data-mdast-node-id="pJcpAjoInJKYEpZqCh3Sq" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="bqofhfoqiryw" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div id="bqofhfoqiryw-code" class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># --- Feature Engineering Configuration ---
BASE_FEATURES = [ # Features from the core dataset to consider initially
    &#x27;zos&#x27;, &#x27;water_temp&#x27;,
    &#x27;peace_discharge&#x27;, &#x27;peace_TN&#x27;, &#x27;peace_TP&#x27;,
    &#x27;wind_u&#x27;, &#x27;wind_v&#x27;
]
# Define basic lag configuration
BASIC_LAG_CONFIG = {
    &#x27;kb_lags&#x27;: [1, 2],
    &#x27;env_lags&#x27;: list(range(1, 7)) # e.g., 1 to 6 weeks
}
# Define enhanced lag configuration (matching RF paper more closely)
ENHANCED_LAG_CONFIG = {
    # &#x27;kb_lags&#x27;: [1, 2],
    &#x27;kb_rolling_windows&#x27;: [(1, 4), (5, 8), (9, 12)], # (start_lag, end_lag) for mean &amp; prop
    &#x27;discharge_rolling_window&#x27;: 4, # e.g., 4-week avg discharge lag 1
    &#x27;env_lags&#x27;: [1] # Lags for other env vars in enhanced mode
}

# --- Data Splitting ---
TRAIN_SPLIT_RATIO = 0.70
VALIDATION_SPLIT_RATIO = 0.15
# Test split is the remainder

# --- LSTM Model Default Hyperparameters (Used if PERFORM_TUNING is False) ---
DEFAULT_LSTM_UNITS = 64
DEFAULT_DROPOUT_RATE = 0.3
DEFAULT_LEARNING_RATE = 0.001

# --- KerasTuner Configuration (Used if PERFORM_TUNING is True) ---
TUNER_MAX_TRIALS = 10
TUNER_EPOCHS = 30
TUNER_BATCH_SIZE = 32

# --- Training Parameters ---
TRAIN_EPOCHS = 50
TRAIN_BATCH_SIZE = 32
TRAIN_PATIENCE = 10 # For EarlyStopping

# --- EnKF Configuration ---
ENKF_STATE_VARS = [
    &#x27;peace_discharge&#x27;, &#x27;peace_TN&#x27;, &#x27;peace_TP&#x27;, &#x27;kb&#x27;,&#x27;wind_u&#x27;, &#x27;wind_v&#x27;
]
# Variables to assimilate
N_ENKF = 50 # Ensemble size
# Noise Estimation Approach (&#x27;basic_stats&#x27; uses train set stats, &#x27;manual&#x27; requires defining R_diag, Q_diag below)
ENKF_NOISE_ESTIMATION = &#x27;manual&#x27;
# Manual noise variances (used only if ENKF_NOISE_ESTIMATION = &#x27;manual&#x27;) - Define placeholder values
MANUAL_R_DIAG = [
    (100.0 * 0.01)**2, # Discharge (Trust the gauge)
    (0.2 * 0.01)**2,   # TN (Trust the sample)
    (0.02 * 0.01)**2,  # TP
    (5.0 * 0.05)**2,   # Wind U (Wind is noisy, maybe 5% error)
    (5.0 * 0.05)**2,   # Wind V
    (0.1 * 0.01)**2    # ZOS (Trust the satellite) # Example observation noise variances
    ]
MANUAL_Q_DIAG = [
    (150.0 * 2.0)**2,  # Discharge can spike massively
    (0.3 * 2.0)**2,    # TN spikes
    (0.03 * 2.0)**2,   # TP spikes
    (10.0 * 1.5)**2,   # Wind changes direction rapidly
    (10.0 * 1.5)**2,   # Wind V
    (0.1 * 1.0)**2     # ZOS changes
]

# --- MC Dropout Configuration ---
N_MC_SAMPLES = 50


# --- Print Setup Summary ---
print(&quot;\n--- Workflow Configuration Summary ---&quot;)
print(f&quot;Enhanced Features Enabled: {USE_ENHANCED_FEATURES}&quot;)
print(f&quot;Scaler Type Selected: {SCALER_TYPE}&quot;)
print(f&quot;Class Weighting Enabled: {USE_CLASS_WEIGHT}&quot;)
print(f&quot;Hyperparameter Tuning Enabled: {PERFORM_TUNING}&quot;)
print(f&quot;EnKF Enabled: {PERFORM_ENKF}&quot;)
print(f&quot;Sequence Length: {SEQUENCE_LENGTH}&quot;)
print(f&quot;Forecast Horizon: {FORECAST_HORIZON} week(s)&quot;)
if PERFORM_TUNING:
    print(f&quot;Tuner Max Trials: {TUNER_MAX_TRIALS}, Epochs per Trial: {TUNER_EPOCHS}&quot;)
else:
    print(f&quot;Using Default LSTM HPs: Units={DEFAULT_LSTM_UNITS}, Dropout={DEFAULT_DROPOUT_RATE}, LR={DEFAULT_LEARNING_RATE}&quot;)
if PERFORM_ENKF:
    print(f&quot;EnKF Ensemble Size: {N_ENKF}, Noise Estimation: {ENKF_NOISE_ESTIMATION}&quot;)
print(&quot;------------------------------------&quot;)

# Check if optional modules were imported if flags are set
if PERFORM_TUNING and kt is None:
    print(&quot;\nWarning: KerasTuner (kt) not imported/installed, but PERFORM_TUNING is True. Tuning will be skipped.&quot;)
    PERFORM_TUNING = False # Disable tuning if library not available
if PERFORM_ENKF and &#x27;EnsembleKalmanFilter&#x27; not in locals():
     # We will define EnKF class later, but good to note dependency
     print(&quot;\nNote: EnKF flag is True, ensure EnsembleKalmanFilter class is defined/imported later.&quot;)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" id="bqofhfoqiryw-outputs" data-mdast-node-id="AcWurW_a4vlmGCr2EVOO-" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>
--- Workflow Configuration Summary ---
Enhanced Features Enabled: True
Scaler Type Selected: Robust
Class Weighting Enabled: True
Hyperparameter Tuning Enabled: True
EnKF Enabled: True
Sequence Length: 12
Forecast Horizon: 1 week(s)
Tuner Max Trials: 10, Epochs per Trial: 30
EnKF Ensemble Size: 50, Noise Estimation: manual
------------------------------------

Note: EnKF flag is True, ensure EnsembleKalmanFilter class is defined/imported later.
</span></code></pre></div></div></div></div><div id="hkujscl3jrj2" class="myst-jp-nb-block relative group/block"><h3 id="part-2-data-loading-initial-processing" class="relative group"><span class="heading-text">Part 2: Data Loading &amp; Initial Processing</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#part-2-data-loading-initial-processing" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p>This cell defines and executes a function to load the raw weekly data, handle the datetime index, calculate U/V wind components, and create the binary target variable based on the bloom threshold. It uses the configuration parameters defined in Part 1.</p><p>Explanation:</p><ol start="1"><li><p>calculate_wind_components(df): Takes a DataFrame, calculates U and V wind components if ‘wind_direction’ and ‘wind_speed’ exist, drops the original columns, and returns the modified DataFrame and a list of the newly added column names. Includes basic NaN handling for the calculation.</p></li><li><p>create_target(...): Takes the DataFrame and target configuration, ensures the original target column is numeric, drops rows where the target is NaN (important for supervised learning), creates the binary bloom_target column, prints the class distribution, and returns the DataFrame.</p></li><li><p>load_and_prepare_data(...): Orchestrates the initial steps: loads the CSV, sets the datetime index, calls calculate_wind_components, calls create_target, and returns the resulting DataFrame (df_initial). Includes error handling for file not found or missing columns.</p></li><li><p>merge_external_data(...) (Optional Placeholder): Provides a structure for loading and merging additional datasets based on a dictionary of file paths. It performs a left merge to keep all original data points. Note: This function needs actual file paths and assumes external CSVs have a compatible datetime index.</p></li><li><p>Main Execution Block (if <strong>name</strong> == “<strong>main</strong>”:): Calls load_and_prepare_data using the configuration variables. Includes a commented-out section showing how merge_external_data would be called if needed. Prints final info about the df_initial DataFrame. Includes basic NameError handling in case Part 1 wasn’t run.</p></li></ol><p>After running this cell, the df_initial DataFrame should contain the core data with wind components calculated and the binary bloom target created, ready for the feature engineering steps in Part 3.</p></div><div id="jamzelqjjs5m" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div id="jamzelqjjs5m-code" class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># Ensure configuration variables from Part 1 are accessible in the environment
# Example: INPUT_DATA_PATH, DATETIME_COL, TARGET_COL, BLOOM_THRESHOLD, TARGET_BINARY_COL

def calculate_wind_components(df):
    &quot;&quot;&quot;Calculates U and V wind components if columns exist.&quot;&quot;&quot;
    if &#x27;wind_direction&#x27; in df.columns and &#x27;wind_speed&#x27; in df.columns:
        print(&quot;Calculating wind U/V components...&quot;)
        # Ensure numeric types, handle potential NaNs before calculation
        df[&#x27;wind_direction&#x27;] = pd.to_numeric(df[&#x27;wind_direction&#x27;], errors=&#x27;coerce&#x27;)
        df[&#x27;wind_speed&#x27;] = pd.to_numeric(df[&#x27;wind_speed&#x27;], errors=&#x27;coerce&#x27;)

        # Temporarily fill NaNs with 0 for calculation if they exist
        wind_cols_to_check = [&#x27;wind_direction&#x27;, &#x27;wind_speed&#x27;]
        if df[wind_cols_to_check].isnull().any().any():
            print(&quot;Warning: NaNs found in wind direction/speed. Temporarily filling with 0 for component calculation.&quot;)
            df[wind_cols_to_check] = df[wind_cols_to_check].fillna(0)

        wind_dir_rad = np.deg2rad(df[&#x27;wind_direction&#x27;])
        wind_speed = df[&#x27;wind_speed&#x27;]
        # Meteorological convention: wind direction &#x27;coming from&#x27;
        df[&#x27;wind_u&#x27;] = -wind_speed * np.sin(wind_dir_rad)
        df[&#x27;wind_v&#x27;] = -wind_speed * np.cos(wind_dir_rad)
        # Drop original wind columns
        df = df.drop(columns=[&#x27;wind_direction&#x27;, &#x27;wind_speed&#x27;])
        print(&quot;Calculated wind U/V components and dropped original columns.&quot;)
        added_cols = [&#x27;wind_u&#x27;, &#x27;wind_v&#x27;]
    else:
        print(&quot;Warning: &#x27;wind_direction&#x27; or &#x27;wind_speed&#x27; not found. Skipping component calculation.&quot;)
        added_cols = []
    return df, added_cols

def create_target(df, target_col, bloom_threshold, target_binary_col):
    &quot;&quot;&quot;Creates the binary target variable.&quot;&quot;&quot;
    if target_col not in df.columns:
        print(f&quot;Error: Target column &#x27;{target_col}&#x27; not found.&quot;)
        return None # Return None if target column is missing

    print(f&quot;Creating binary target &#x27;{target_binary_col}&#x27; using threshold {bloom_threshold:.0f} cells/L...&quot;)
    df[target_col] = pd.to_numeric(df[target_col], errors=&#x27;coerce&#x27;)

    # Handle potential NaNs in target before comparison by dropping rows
    initial_rows = len(df)
    if df[target_col].isnull().any():
        print(f&quot;Warning: NaNs found in target column &#x27;{target_col}&#x27;. Dropping rows with NaN target.&quot;)
        df.dropna(subset=[target_col], inplace=True)
        print(f&quot;Dropped {initial_rows - len(df)} rows with NaN in &#x27;{target_col}&#x27;. New shape: {df.shape}&quot;)

    df[target_binary_col] = (df[target_col] &gt;= bloom_threshold).astype(int)
    print(f&quot;Target distribution (%):\n{df[target_binary_col].value_counts(normalize=True) * 100}&quot;)
    return df

def load_and_prepare_data(filepath, datetime_col, target_col, bloom_threshold, target_binary_col):
    &quot;&quot;&quot;Loads data, handles datetime, calculates wind components, creates target.&quot;&quot;&quot;
    print(f&quot;Loading data from: {filepath}&quot;)
    try:
        df = pd.read_csv(filepath)
        # Handle datetime index
        if datetime_col not in df.columns:
            raise ValueError(f&quot;Datetime column &#x27;{datetime_col}&#x27; not found.&quot;)
        df[datetime_col] = pd.to_datetime(df[datetime_col])
        df = df.sort_values(datetime_col).set_index(datetime_col)
        print(f&quot;Data loaded successfully. Shape: {df.shape}, Time range: {df.index.min()} to {df.index.max()}&quot;)
        print(f&quot;Initial NaN counts:\n{df.isnull().sum()}&quot;)

        # Calculate wind components
        df, wind_cols = calculate_wind_components(df)

        # Create binary target
        df = create_target(df, target_col, bloom_threshold, target_binary_col)

        if df is not None:
            print(&quot;\n--- Initial Data Preparation Complete ---&quot;)
            print(f&quot;DataFrame shape after initial processing: {df.shape}&quot;)
            print(&quot;Columns:&quot;, df.columns.tolist())
            print(&quot;\nFirst 5 rows:&quot;)
            print(df.head())
        return df

    except FileNotFoundError:
        print(f&quot;Error: Input data file not found at {filepath}&quot;)
        return None
    except ValueError as ve:
        print(f&quot;ValueError during data preparation: {ve}&quot;)
        return None
    except Exception as e:
        print(f&quot;An unexpected error occurred during data loading/preparation: {e}&quot;)
        return None

# --- Optional: Function Placeholder for Merging External Data ---
def merge_external_data(df, external_files_dict):
    &quot;&quot;&quot;
    Loads and merges external data (e.g., Caloosahatchee Q/NOx) into the main dataframe.
    Args:
        df (pd.DataFrame): The main dataframe with datetime index.
        external_files_dict (dict): Dictionary where keys are column names (e.g., &#x27;caloos_Q&#x27;)
                                     and values are file paths to the external CSV data.
                                     External CSVs must have a compatible datetime index.
    Returns:
        pd.DataFrame: DataFrame with external data merged (left join).
    &quot;&quot;&quot;
    print(&quot;\n--- Merging External Data (Placeholder) ---&quot;)
    if not external_files_dict:
        print(&quot;No external data files specified.&quot;)
        return df

    df_merged = df.copy()
    for col_name, file_path in external_files_dict.items():
        try:
            print(f&quot;Loading external data for &#x27;{col_name}&#x27; from &#x27;{file_path}&#x27;...&quot;)
            # Assuming external CSV has datetime index named same as DATETIME_COL or is the index
            df_ext = pd.read_csv(file_path, index_col=DATETIME_COL, parse_dates=True) # Adjust index_col if needed
            df_ext = df_ext[[col_name]] # Keep only the specified column
            # Perform left merge
            df_merged = df_merged.merge(df_ext, left_index=True, right_index=True, how=&#x27;left&#x27;)
            print(f&quot;Merged &#x27;{col_name}&#x27;. NaN count: {df_merged[col_name].isnull().sum()}&quot;)
        except FileNotFoundError:
            print(f&quot;Warning: External data file not found: {file_path}. Skipping &#x27;{col_name}&#x27;.&quot;)
        except KeyError:
             print(f&quot;Warning: Column &#x27;{col_name}&#x27; not found in file {file_path}. Skipping.&quot;)
        except Exception as e:
            print(f&quot;Error merging external file {file_path}: {e}&quot;)

    print(&quot;External data merging complete.&quot;)
    return df_merged


# --- Execute Data Loading and Preparation ---
# Ensure config variables from Part 1 are defined before running this
if __name__ == &quot;__main__&quot;:
    try:
        df_initial = load_and_prepare_data(
            INPUT_DATA_PATH,
            DATETIME_COL,
            TARGET_COL,
            BLOOM_THRESHOLD,
            TARGET_BINARY_COL
        )

        if df_initial is not None:
            # Display basic info about the prepared dataframe
            print(&quot;\n--- Dataframe after initial processing (df_initial) ---&quot;)
            df_initial.info()
        else:
            print(&quot;\nData loading/preparation failed.&quot;)

    except NameError as ne:
         print(f&quot;NameError: A configuration variable might be missing. Did Part 1 run correctly? Error: {ne}&quot;)
    except Exception as e:
         print(f&quot;An unexpected error occurred in main execution block: {e}&quot;)

</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" id="jamzelqjjs5m-outputs" data-mdast-node-id="EDaiOi_gW4ZDB4TG937lW" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>Loading data from: data_weekly_intepolated.csv
Data loaded successfully. Shape: (1617, 9), Time range: 1993-01-04 00:00:00 to 2023-12-25 00:00:00
Initial NaN counts:
kb                 0
zos                0
salinity           0
water_temp         0
wind_direction     0
wind_speed         0
peace_discharge    0
peace_TN           0
peace_TP           0
dtype: int64
Calculating wind U/V components...
Calculated wind U/V components and dropped original columns.
Creating binary target &#x27;bloom_target&#x27; using threshold 100000 cells/L...
Target distribution (%):
bloom_target
0    73.09833
1    26.90167
Name: proportion, dtype: float64

--- Initial Data Preparation Complete ---
DataFrame shape after initial processing: (1617, 10)
Columns: [&#x27;kb&#x27;, &#x27;zos&#x27;, &#x27;salinity&#x27;, &#x27;water_temp&#x27;, &#x27;peace_discharge&#x27;, &#x27;peace_TN&#x27;, &#x27;peace_TP&#x27;, &#x27;wind_u&#x27;, &#x27;wind_v&#x27;, &#x27;bloom_target&#x27;]

First 5 rows:
               kb       zos   salinity  water_temp  peace_discharge  peace_TN  \
time                                                                            
1993-01-04  333.0  0.012906  33.043478        26.8            202.0    8.2000   
1993-01-11  667.0  0.015614  33.065217        27.0            423.0   10.1000   
1993-01-18  667.0  0.021702  33.086957        27.1           1470.0   12.0000   
1993-01-25    0.0  0.015950  33.108696        26.8           1450.0   10.0475   
1993-02-01    0.0  0.008977  33.130435        26.5           1490.0    8.0950   

            peace_TP     wind_u     wind_v  bloom_target  
time                                                      
1993-01-04  1.999091  -8.170215 -11.245336             0  
1993-01-11  1.934545 -14.303751   7.605439             0  
1993-01-18  1.870000 -15.407116   5.006075             0  
1993-01-25  1.870500 -11.840127   4.309454             0  
1993-02-01  1.871000  -4.233633 -16.980175             0  

--- Dataframe after initial processing (df_initial) ---
&lt;class &#x27;pandas.core.frame.DataFrame&#x27;&gt;
DatetimeIndex: 1617 entries, 1993-01-04 to 2023-12-25
Data columns (total 10 columns):
 #   Column           Non-Null Count  Dtype  
---  ------           --------------  -----  
 0   kb               1617 non-null   float64
 1   zos              1617 non-null   float64
 2   salinity         1617 non-null   float64
 3   water_temp       1617 non-null   float64
 4   peace_discharge  1617 non-null   float64
 5   peace_TN         1617 non-null   float64
 6   peace_TP         1617 non-null   float64
 7   wind_u           1617 non-null   float64
 8   wind_v           1617 non-null   float64
 9   bloom_target     1617 non-null   int64  
dtypes: float64(9), int64(1)
memory usage: 139.0 KB
</span></code></pre></div></div></div></div><div id="fdv8rabhjny3" class="myst-jp-nb-block relative group/block"><h3 id="part-3-feature-engineering" class="relative group"><span class="heading-text">Part 3: Feature Engineering</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#part-3-feature-engineering" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p>This part defines and executes a function create_features that adds lagged features to the initially processed DataFrame (df_initial from Part 2). It uses the configuration flags set in Part 1 (USE_ENHANCED_FEATURES) to determine whether to create basic lags or the more detailed lags inspired by the Random Forest paper.</p><p>This cell defines the feature engineering function and applies it to the df_initial DataFrame. It creates lagged versions of environmental variables and, optionally, more detailed lagged and rolling aggregate features for K. brevis counts.</p><p>Explanation:</p><ol start="1"><li><p>Configuration: It first defines or ensures access to the necessary configuration variables from Part 1 (like USE_ENHANCED_FEATURES, lag definitions, feature lists).</p></li><li><p>create_features(...) Function:</p><ul><li><p>Takes the initial DataFrame (df_initial) and configuration details as input.</p></li><li><p>Selects either BASIC_LAG_CONFIG or ENHANCED_LAG_CONFIG based on the USE_ENHANCED_FEATURES flag.</p></li><li><p>KB Lags: Creates simple weekly lags (_L1, _L2) for the raw kb column.</p></li><li><p>KB Rolling Aggregates (Enhanced Only): Calculates rolling mean (_M1_mean, _M2_mean, etc.) and rolling proportion of bloom weeks (_M1_prop, _M2_prop, etc.) over specified windows (e.g., 4, 8, 12 weeks), shifted back appropriately.</p></li><li><p>Discharge Rolling Average (Enhanced Only): Calculates a rolling average (e.g., 4-week) for the primary discharge column, lagged by 1 week.</p></li><li><p>Environmental Lags: Creates simple weekly lags for all other specified environmental/hydrological variables (BASE_FEATURES). The number of lags depends on whether basic or enhanced mode is selected.</p></li><li><p>NaN Handling: Tracks the maximum lag introduced by any operation and drops the corresponding number of initial rows from the DataFrame to ensure sequences are complete.</p></li><li><p>All-NaN Column Check: Includes a check to identify and optionally drop columns that might become entirely NaN after lagging (important for sparse data).</p></li><li><p>Returns the final DataFrame (df_processed) with all engineered features.</p></li></ul></li><li><p>Main Execution: Calls the create_features function with the appropriate arguments based on the configuration flags. Prints the head and tail of the resulting df_processed DataFrame.</p></li></ol><p>After running this cell, df_processed will contain the data ready for splitting (Part 4) and subsequent preprocessing/modeling steps. The number of columns will vary depending on whether basic or enhanced features were generated.</p></div><div id="dswtkijrjo8j" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div id="dswtkijrjo8j-code" class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># --- Configuration (Ensure these are defined from Part 1 or redefined here) ---
TARGET_COL = &#x27;kb&#x27;
TARGET_BINARY_COL = &#x27;bloom_target&#x27;
USE_ENHANCED_FEATURES = True # Set based on Part 1 config

# Define lag configurations (can be pulled from Part 1 config)
BASIC_LAG_CONFIG = {
    &#x27;kb_lags&#x27;: [1, 2],
    &#x27;env_lags&#x27;: list(range(1, 7))
}
ENHANCED_LAG_CONFIG = {
    &#x27;kb_lags&#x27;: [1, 2],
    &#x27;kb_rolling_windows&#x27;: [(1, 4), (5, 8), (9, 12)], # (start_lag, end_lag)
    &#x27;discharge_rolling_window&#x27;: 4, # Window size for rolling avg discharge
    &#x27;env_lags&#x27;: [1] # Lags for other env vars in enhanced mode
}
# Features to lag (environmental/hydrological) - should include wind components if created
BASE_FEATURES = [
    &#x27;zos&#x27;, &#x27;water_temp&#x27;,
    &#x27;peace_discharge&#x27;, &#x27;peace_TN&#x27;, &#x27;peace_TP&#x27;,
    &#x27;wind_u&#x27;, &#x27;wind_v&#x27;
]

# --- Function Definition ---
def create_features(df, target_col, target_binary_col, base_features, use_enhanced=False, basic_cfg=None, enhanced_cfg=None):
    &quot;&quot;&quot;Creates lagged and rolling features based on configuration.&quot;&quot;&quot;
    if df is None:
        print(&quot;Error in create_features: Input DataFrame is None.&quot;)
        return None

    print(f&quot;\n--- Creating Features (Enhanced Mode: {use_enhanced}) ---&quot;)
    df_featured = df.copy()
    all_created_feature_cols = list(df.columns) # Start with existing columns

    # Select config based on flag
    if use_enhanced:
        cfg = enhanced_cfg if enhanced_cfg else ENHANCED_LAG_CONFIG
        env_lags = cfg.get(&#x27;env_lags&#x27;, [1])
        kb_lags = cfg.get(&#x27;kb_lags&#x27;, [])
        kb_rolling_windows = cfg.get(&#x27;kb_rolling_windows&#x27;, [])
        discharge_rolling_window = cfg.get(&#x27;discharge_rolling_window&#x27;, None)
        print(&quot;Using ENHANCED feature configuration.&quot;)
    else:
        cfg = basic_cfg if basic_cfg else BASIC_LAG_CONFIG
        env_lags = cfg.get(&#x27;env_lags&#x27;, [])
        kb_lags = cfg.get(&#x27;kb_lags&#x27;, [])
        kb_rolling_windows = [] # No rolling features in basic mode
        discharge_rolling_window = None
        print(&quot;Using BASIC feature configuration.&quot;)

    max_lag_needed = 0 # Track the maximum lag introduced

    # --- Lagged K. brevis Features (Raw Counts) ---
    if target_col in df.columns and kb_lags:
        print(f&quot;Creating lagged features for target: {target_col}...&quot;)
        df[target_col] = pd.to_numeric(df[target_col], errors=&#x27;coerce&#x27;)
        for lag in kb_lags:
            col_name = f&#x27;{target_col}_L{lag}&#x27;
            df_featured[col_name] = df[target_col].shift(lag)
            all_created_feature_cols.append(col_name)
            max_lag_needed = max(max_lag_needed, lag)
        print(f&quot;  Created KB weekly lags: L{&#x27;, L&#x27;.join(map(str, kb_lags))}&quot;)
    elif kb_lags:
        print(f&quot;Warning: Target column &#x27;{target_col}&#x27; not found for lagging.&quot;)

    # --- Rolling K. brevis Features (Enhanced Mode Only) ---
    if use_enhanced and kb_rolling_windows and target_col in df.columns and target_binary_col in df.columns:
        print(&quot;Creating rolling aggregate features for KB...&quot;)
        target_binary_1e5 = df[target_binary_col]
        monthly_lags_created = []
        max_roll_lag = 0
        for i, (start_lag, end_lag) in enumerate(kb_rolling_windows):
            window_size = end_lag - start_lag + 1
            month_lag_id = f&#x27;M{i+1}&#x27;
            monthly_lags_created.append(month_lag_id)
            max_roll_lag = max(max_roll_lag, end_lag) # Max lag needed for rolling window

            # Mean of raw kb over the window, shifted back
            col_name_mean = f&#x27;{target_col}_{month_lag_id}_mean&#x27;
            df_featured[col_name_mean] = df[target_col].rolling(window=window_size, min_periods=1).mean().shift(start_lag)
            all_created_feature_cols.append(col_name_mean)

            # Proportion of bloom weeks (target=1) over the window, shifted back
            col_name_prop = f&#x27;{target_binary_col}_{month_lag_id}_prop&#x27;
            df_featured[col_name_prop] = target_binary_1e5.rolling(window=window_size, min_periods=1).mean().shift(start_lag)
            all_created_feature_cols.append(col_name_prop)
        print(f&quot;  Created approx KB monthly lags (mean, prop_bloom): {&#x27;, &#x27;.join(monthly_lags_created)}&quot;)
        max_lag_needed = max(max_lag_needed, max_roll_lag) # Rolling needs lookback up to end_lag

    # --- Rolling Discharge Feature (Enhanced Mode Only) ---
    discharge_col = &#x27;peace_discharge&#x27; # Or potentially &#x27;caloos_Q&#x27; if prioritized/available
    if use_enhanced and discharge_rolling_window and discharge_col in df.columns:
         print(f&quot;Creating rolling average feature for {discharge_col}...&quot;)
         window_size = discharge_rolling_window
         # Typically want average over past month, lagged by 1 week
         start_lag = 1
         col_name_discharge_roll = f&#x27;{discharge_col}_{window_size}w_avg_L{start_lag}&#x27;
         df_featured[col_name_discharge_roll] = df[discharge_col].rolling(window=window_size, min_periods=1).mean().shift(start_lag)
         all_created_feature_cols.append(col_name_discharge_roll)
         max_lag_needed = max(max_lag_needed, window_size + start_lag -1) # Max lookback needed
         print(f&quot;  Created {col_name_discharge_roll}&quot;)


    # --- Lagged Environmental/Hydrological Features ---
    if env_lags:
        print(&quot;\nCreating lagged environmental/hydrological features...&quot;)
        lagged_env_cols_added_names = []
        max_env_lag = 0
        for feature in base_features:
             if feature in df.columns:
                df[feature] = pd.to_numeric(df[feature], errors=&#x27;coerce&#x27;) # Ensure numeric
                for lag in env_lags:
                    col_name = f&#x27;{feature}_L{lag}&#x27;
                    df_featured[col_name] = df[feature].shift(lag)
                    all_created_feature_cols.append(col_name)
                    max_env_lag = max(max_env_lag, lag)
                lagged_env_cols_added_names.append(feature)
             else:
                print(f&quot;  Warning: Feature &#x27;{feature}&#x27; not found for lagging.&quot;)
        if lagged_env_cols_added_names:
            print(f&quot;  Lagged features created for: {&#x27;, &#x27;.join(lagged_env_cols_added_names)} using lags L{&#x27;, L&#x27;.join(map(str, env_lags))}&quot;)
        max_lag_needed = max(max_lag_needed, max_env_lag)

    # --- Drop Rows with NaNs from Lagging/Rolling ---
    print(f&quot;\nMaximum lag/window introduced: {max_lag_needed} weeks.&quot;)
    initial_rows = len(df_featured)
    if max_lag_needed &gt; 0:
        df_processed = df_featured.iloc[max_lag_needed:].copy()
        print(f&quot;Dropped first {max_lag_needed} rows due to NaNs generated by feature engineering.&quot;)
        print(f&quot;Shape after dropping initial NaNs: {df_processed.shape}&quot;)
    else:
        df_processed = df_featured.copy()
        print(&quot;No lagging applied or max lag was 0, no initial rows dropped.&quot;)

    # Final check for all-NaN columns (can happen with rolling/shifting if data is sparse)
    all_nan_cols = df_processed.columns[df_processed.isnull().all()].tolist()
    if all_nan_cols:
        print(f&quot;\nWarning: The following columns consist entirely of NaNs and will be dropped: {all_nan_cols}&quot;)
        df_processed = df_processed.dropna(axis=1, how=&#x27;all&#x27;)
        print(f&quot;Shape after dropping all-NaN columns: {df_processed.shape}&quot;)

    # Ensure no duplicate columns (though unlikely with this structure)
    df_processed = df_processed.loc[:,~df_processed.columns.duplicated()]

    print(&quot;\n--- Feature Engineering Complete ---&quot;)
    print(f&quot;Final number of columns (features + targets): {len(df_processed.columns)}&quot;)
    return df_processed

# --- Execute Feature Engineering ---
# Ensure df_initial exists from Part 2 and config variables from Part 1
if __name__ == &quot;__main__&quot;:
    try:
        if &#x27;df_initial&#x27; in locals() and df_initial is not None:
            # Determine which config to use based on the flag from Part 1
            lag_config_to_use = ENHANCED_LAG_CONFIG if USE_ENHANCED_FEATURES else BASIC_LAG_CONFIG
            features_to_lag_list = BASE_FEATURES # Modify if external data was added

            df_processed = create_features(
                df_initial,
                TARGET_COL,
                TARGET_BINARY_COL,
                features_to_lag_list,
                use_enhanced=USE_ENHANCED_FEATURES,
                basic_cfg=BASIC_LAG_CONFIG,
                enhanced_cfg=ENHANCED_LAG_CONFIG
            )

            if df_processed is not None:
                print(&quot;\nFirst 5 rows of processed data (df_processed):&quot;)
                display(df_processed.head())
                print(&quot;\nLast 5 rows:&quot;)
                display(df_processed.tail())
                print(f&quot;\nFinal shape of df_processed: {df_processed.shape}&quot;)
            else:
                print(&quot;\nFeature engineering failed.&quot;)
        else:
            print(&quot;\nError: df_initial not found or is None. Please run Part 2 first.&quot;)

    except NameError as ne:
         print(f&quot;NameError: A configuration variable might be missing. Did Part 1 run correctly? Error: {ne}&quot;)
    except Exception as e:
         print(f&quot;An unexpected error occurred in main execution block: {e}&quot;)

</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" id="dswtkijrjo8j-outputs" data-mdast-node-id="VPQczK_2lMEKFyeTT7tM9" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>
--- Creating Features (Enhanced Mode: True) ---
Using ENHANCED feature configuration.
Creating lagged features for target: kb...
  Created KB weekly lags: L1, L2
Creating rolling aggregate features for KB...
  Created approx KB monthly lags (mean, prop_bloom): M1, M2, M3
Creating rolling average feature for peace_discharge...
  Created peace_discharge_4w_avg_L1

Creating lagged environmental/hydrological features...
  Lagged features created for: zos, water_temp, peace_discharge, peace_TN, peace_TP, wind_u, wind_v using lags L1

Maximum lag/window introduced: 12 weeks.
Dropped first 12 rows due to NaNs generated by feature engineering.
Shape after dropping initial NaNs: (1605, 26)

--- Feature Engineering Complete ---
Final number of columns (features + targets): 26

First 5 rows of processed data (df_processed):
</span></code></pre></div></div><div><div class="p-2.5">Loading...</div></div><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>
Last 5 rows:
</span></code></pre></div></div><div><div class="p-2.5">Loading...</div></div><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>
Final shape of df_processed: (1605, 26)
</span></code></pre></div></div></div></div><div id="sxraeziyj0el" class="myst-jp-nb-block relative group/block"><h3 id="part-4-data-splitting-chronological" class="relative group"><span class="heading-text">Part 4: Data Splitting (Chronological)</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#part-4-data-splitting-chronological" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p>This part defines and executes a function to split the feature-engineered DataFrame (df_processed from Part 3) into training, validation, and testing sets based on time. A chronological split is essential for time-series data to ensure the model is trained on past data and evaluated on future data, simulating a real-world forecasting scenario.</p><p>This cell defines the split_data_chronological function and applies it to df_processed. It uses the TRAIN_SPLIT_RATIO and VALIDATION_SPLIT_RATIO defined in Part 1 to divide the data.</p><p>Explanation:</p><ol start="1"><li><p>Function split_data_chronological:</p><ul><li><p>Takes the feature-engineered DataFrame (df_processed) and the desired train/validation ratios as input.</p></li><li><p>Includes checks to ensure the input DataFrame is valid and has a datetime index.</p></li><li><p>Calculates the number of samples for each set based on the ratios.</p></li><li><p>Performs the split using integer-location based indexing (iloc) which respects the chronological order. .copy() is used to avoid potential SettingWithCopyWarning later.</p></li><li><p>Prints the size and date range of each resulting subset.</p></li><li><p>Includes an assertion to double-check that the splits don’t overlap.</p></li><li><p>Returns the three DataFrames: train_df, validation_df, test_df.</p></li></ul></li><li><p>Main Execution:</p><ul><li><p>Calls the function using df_processed (output of Part 3) and the ratios defined in the configuration (Part 1).</p></li><li><p>Stores the results in train_df, validation_df, and test_df. These variables will be used in subsequent steps (scaling, sequence creation, EnKF).</p></li><li><p>Prints the head of train_df for verification.</p></li></ul></li></ol><p>After running this cell, you will have the data divided into the necessary subsets for training, validation (tuning/early stopping), and final testing, while preserving the temporal order. The next step (Part 5) will handle imputation and scaling, fitting the necessary objects only on train_df.</p></div><div id="id-47chavmsjxe" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div id="id-47chavmsjxe-code" class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># --- Configuration (Ensure these are defined from Part 1 or redefined here) ---
TRAIN_SPLIT_RATIO = 0.70
VALIDATION_SPLIT_RATIO = 0.15
# Test split ratio is implicitly calculated

# --- Function Definition ---
def split_data_chronological(df, train_ratio, val_ratio):
    &quot;&quot;&quot;
    Splits the DataFrame chronologically into train, validation, and test sets.

    Args:
        df (pd.DataFrame): The input DataFrame with a datetime index, sorted chronologically.
        train_ratio (float): The proportion of data to use for training (e.g., 0.7).
        val_ratio (float): The proportion of data to use for validation (e.g., 0.15).

    Returns:
        tuple: A tuple containing train_df, validation_df, test_df (pd.DataFrames),
               or (None, None, None) if splitting fails.
    &quot;&quot;&quot;
    if df is None or df.empty:
        print(&quot;Error in split_data_chronological: Input DataFrame is None or empty.&quot;)
        return None, None, None
    if not isinstance(df.index, pd.DatetimeIndex):
         print(&quot;Error: DataFrame index must be a DatetimeIndex.&quot;)
         return None, None, None
    if not df.index.is_monotonic_increasing:
         print(&quot;Warning: DataFrame index is not sorted chronologically. Sorting now...&quot;)
         df = df.sort_index()

    print(&quot;\n--- Splitting Data into Train, Validation, and Test Sets (Chronological) ---&quot;)
    n_total = len(df)
    n_train = int(n_total * train_ratio)
    n_validation = int(n_total * val_ratio)
    n_test = n_total - n_train - n_validation

    # Ensure calculated splits are valid
    if n_train &lt;= 0 or n_validation &lt;= 0 or n_test &lt;= 0:
         print(f&quot;Error: Not enough data ({n_total} rows) to create non-empty train/validation/test splits.&quot;)
         print(f&quot;Calculated splits: Train={n_train}, Val={n_validation}, Test={n_test}&quot;)
         return None, None, None
    if n_train + n_validation + n_test != n_total:
         print(&quot;Error: Split ratios do not sum correctly.&quot;)
         return None, None, None

    # Perform chronological split using iloc
    train_df = df.iloc[:n_train].copy()
    validation_df = df.iloc[n_train : n_train + n_validation].copy()
    test_df = df.iloc[n_train + n_validation :].copy() # Takes the rest

    print(f&quot;Total samples: {n_total}&quot;)
    print(f&quot;Training set:   {len(train_df)} samples (Index: {train_df.index.min()} to {train_df.index.max()})&quot;)
    print(f&quot;Validation set: {len(validation_df)} samples (Index: {validation_df.index.min()} to {validation_df.index.max()})&quot;)
    print(f&quot;Test set:       {len(test_df)} samples (Index: {test_df.index.min()} to {test_df.index.max()})&quot;)

    # Basic check for overlap
    assert train_df.index.max() &lt; validation_df.index.min(), &quot;Train/Validation sets overlap!&quot;
    assert validation_df.index.max() &lt; test_df.index.min(), &quot;Validation/Test sets overlap!&quot;

    print(&quot;Data splitting complete.&quot;)
    return train_df, validation_df, test_df

# --- Execute Data Splitting ---
# Ensure df_processed exists from Part 3
if __name__ == &quot;__main__&quot;:
    try:
        if &#x27;df_processed&#x27; in locals() and df_processed is not None:
            # Use ratios defined in Part 1 config
            train_df, validation_df, test_df = split_data_chronological(
                df_processed,
                TRAIN_SPLIT_RATIO,
                VALIDATION_SPLIT_RATIO
            )

            if train_df is not None:
                # Display head of training data as confirmation
                print(&quot;\nHead of Training Data (train_df):&quot;)
                display(train_df.head())
                # Keep test_df (unscaled) for later use with EnKF
                print(&quot;\nUnscaled test_df also created for EnKF observations.&quot;)
            else:
                print(&quot;\nData splitting failed.&quot;)
        else:
            print(&quot;\nError: df_processed not found or is None. Please run Part 3 first.&quot;)
            # Define placeholders to prevent errors if run out of order in notebook
            train_df, validation_df, test_df = None, None, None

    except NameError as ne:
         print(f&quot;NameError: A configuration variable might be missing. Did Part 1 run correctly? Error: {ne}&quot;)
    except Exception as e:
         print(f&quot;An unexpected error occurred in main execution block: {e}&quot;)
         train_df, validation_df, test_df = None, None, None

</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" id="47chavmsjxe--outputs" data-mdast-node-id="xtRDyJFT6jhVq9K15J9Ju" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>
--- Splitting Data into Train, Validation, and Test Sets (Chronological) ---
Total samples: 1605
Training set:   1123 samples (Index: 1993-03-29 00:00:00 to 2014-09-29 00:00:00)
Validation set: 240 samples (Index: 2014-10-06 00:00:00 to 2019-05-06 00:00:00)
Test set:       242 samples (Index: 2019-05-13 00:00:00 to 2023-12-25 00:00:00)
Data splitting complete.

Head of Training Data (train_df):
</span></code></pre></div></div><div><div class="p-2.5">Loading...</div></div><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>
Unscaled test_df also created for EnKF observations.
</span></code></pre></div></div></div></div><div id="zjz8x96ukdht" class="myst-jp-nb-block relative group/block"><h3 id="part-5-preprocessing-imputation-scaling" class="relative group"><span class="heading-text">Part 5: Preprocessing (Imputation &amp; Scaling)</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#part-5-preprocessing-imputation-scaling" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p>This step takes the chronologically split dataframes (train_df, validation_df, test_df from Part 4) and performs two crucial preprocessing tasks:</p><p>Imputation: Handles any remaining missing values (NaNs) in the feature columns. It’s vital to fit the imputer only on the training data.
Scaling: Scales the numerical features using the scaler type specified in the configuration (RobustScaler or StandardScaler). The scaler is fit only on the training data.
The fitted scaler and the final list of feature columns used are saved.</p><p>This cell defines the preprocess_data function to handle imputation and scaling, fitting the necessary transformers only on the training set. It then applies this function to the split dataframes generated in Part 4.</p><p>Explanation of Fixes:</p><ul><li><p>feature_suffix Definition: The NameError occurred because feature_suffix (which depends on USE_ENHANCED_FEATURES) was used to construct FEATURE_LIST_FILENAME before the main execution block where it would be defined.</p></li><li><p>Solution: The code now defines feature_suffix inside the if <strong>name</strong> == “<strong>main</strong>”: block before calling preprocess_data. Filename templates (scaler_fname_template, flist_fname_template) are passed to the function, and the final filenames are constructed inside the function using the passed feature_suffix.</p></li><li><p>Configuration Checks: Added more robust checks at the beginning of the main execution block to ensure all necessary configuration variables and dataframes exist before proceeding.</p></li><li><p>Error Handling: Added more specific error messages and try-except blocks within the preprocess_data function for robustness during imputation and scaling transforms.</p></li><li><p>Clarity: Minor adjustments to print statements for better clarity.
Now, when you run this corrected Part 5 cell (after ensuring Part 1 and Part 4 have run successfully in your session), it should correctly define feature_suffix, construct the filenames, perform the preprocessing, save the components, and store the results in the train_scaled_df, validation_scaled_df, etc., variables without the NameError.</p></li></ul></div><div id="tyrg0mctkhf7" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div id="tyrg0mctkhf7-code" class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># --- Part 5: Preprocessing (Imputation &amp; Scaling) ---

def preprocess_data(train_df, validation_df, test_df, target_col, scaler_type=&#x27;Robust&#x27;,
                    output_dir=&#x27;output/&#x27;, scaler_fname_template=None, flist_fname_template=None, feature_suffix=&#x27;basic&#x27;):

    # 1. Validate Inputs
    if any(df is None or df.empty for df in [train_df, validation_df, test_df]):
        print(&quot;Error: One or more input DataFrames are invalid.&quot;)
        return (None,) * 5

    print(f&quot;\n--- Preprocessing Data (Imputation &amp; Scaling: {scaler_type}) ---&quot;)

    # 2. Identify Features (Exclude Target)
    # We use training data to determine features
    feature_candidates = [c for c in train_df.columns if c != target_col]
    final_feature_columns = train_df[feature_candidates].select_dtypes(include=np.number).columns.tolist()

    if not final_feature_columns:
        print(&quot;Error: No numeric features found.&quot;)
        return (None,) * 5

    print(f&quot;Identified {len(final_feature_columns)} numeric features.&quot;)

    # 3. Initialize Transformers
    imputer = SimpleImputer(strategy=&#x27;mean&#x27;)

    if scaler_type.lower() == &#x27;standard&#x27;:
        scaler = StandardScaler()
    else:
        scaler = RobustScaler() # Default to Robust

    # 4. Fit Transformers (ONLY on Training Data)
    train_proc_df = train_df.copy()

    # Imputation Fit
    impute_needed = train_proc_df[final_feature_columns].isnull().values.any()
    if impute_needed:
        print(&quot;Fitting imputer on training data...&quot;)
        imputer.fit(train_proc_df[final_feature_columns])
    else:
        print(&quot;No missing values in training set. Skipping imputer fitting.&quot;)

    # Scaling Fit (Fit on imputed training data)
    # If we imputed, we must transform the temp training data to fit the scaler correctly
    temp_train_features = imputer.transform(train_proc_df[final_feature_columns]) if impute_needed else train_proc_df[final_feature_columns]

    print(f&quot;Fitting {scaler_type}Scaler on training data...&quot;)
    scaler.fit(temp_train_features)

    # 5. Transform All Sets (Loop to remove redundancy)
    processed_dfs = []
    datasets = [train_df, validation_df, test_df]
    set_names = [&#x27;Train&#x27;, &#x27;Validation&#x27;, &#x27;Test&#x27;]

    for name, df in zip(set_names, datasets):
        df_copy = df.copy()
        try:
            # Apply Imputation
            if impute_needed:
                df_copy[final_feature_columns] = imputer.transform(df_copy[final_feature_columns])

            # Apply Scaling
            df_copy[final_feature_columns] = scaler.transform(df_copy[final_feature_columns])
            processed_dfs.append(df_copy)
        except Exception as e:
            print(f&quot;Error processing {name} set: {e}&quot;)
            return (None,) * 5

    train_scaled, val_scaled, test_scaled = processed_dfs
    print(&quot;Transformation complete.&quot;)

    # 6. Save Components
    try:
        os.makedirs(output_dir, exist_ok=True)
        # Use provided templates or defaults
        s_path = scaler_fname_template.format(scaler_type.lower()) if scaler_fname_template else f&quot;{output_dir}/scaler.joblib&quot;
        f_path = flist_fname_template.format(feature_suffix) if flist_fname_template else f&quot;{output_dir}/features.joblib&quot;

        joblib.dump(scaler, s_path)
        joblib.dump(final_feature_columns, f_path)
        print(f&quot;Saved scaler and feature list to {output_dir}&quot;)
    except Exception as e:
        print(f&quot;Warning: Could not save scaler/features: {e}&quot;)

    return train_scaled, val_scaled, test_scaled, scaler, final_feature_columns

# --- Execution Block ---
if __name__ == &quot;__main__&quot;:
    # Assumes train_df, validation_df, test_df exist from Part 4
    # Assumes config variables like SCALER_TYPE exist from Part 1

    if &#x27;train_df&#x27; in locals():
        train_scaled_df, validation_scaled_df, test_scaled_df, scaler, final_feature_columns_used = preprocess_data(
            train_df, validation_df, test_df,
            TARGET_BINARY_COL,
            scaler_type=SCALER_TYPE,
            output_dir=OUTPUT_DIR,
            scaler_fname_template=SCALER_FILENAME.replace(f&quot;_{SCALER_TYPE.lower()}&quot;, &quot;_{}&quot;), # Dynamic template
            flist_fname_template=FEATURE_LIST_FILENAME.replace(f&quot;_{feature_suffix}&quot;, &quot;_{}&quot;),
            feature_suffix=feature_suffix
        )</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" id="tyrg0mctkhf7-outputs" data-mdast-node-id="_9m8BO7kRvfzDIck0OfAl" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>
--- Preprocessing Data (Imputation &amp; Scaling: Robust) ---
Identified 25 numeric features.
No missing values in training set. Skipping imputer fitting.
Fitting RobustScaler on training data...
Transformation complete.
Saved scaler and feature list to output_refactored/
</span></code></pre></div></div></div></div><div id="bryqj-kjkoh2" class="myst-jp-nb-block relative group/block"><h3 id="part-6-sequence-creation" class="relative group"><span class="heading-text">Part 6: Sequence Creation</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#part-6-sequence-creation" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p>This step takes the scaled dataframes (train_scaled_df, validation_scaled_df, test_scaled_df) and transforms them into the sequence format required by LSTM/GRU models. Each sequence (X) will contain SEQUENCE_LENGTH consecutive time steps of features, and the corresponding target (y) will be the bloom state FORECAST_HORIZON steps after the end of the sequence.</p><p>This cell defines the create_sequences_from_df function and applies it to the scaled training, validation, and test dataframes generated in Part 5. It uses the SEQUENCE_LENGTH and FORECAST_HORIZON parameters defined in Part 1. The resulting NumPy arrays (X_train, y_train, etc.) are saved to a file.</p><p>Explanation:</p><ol start="1"><li><p>Function create_sequences_from_df:</p><ul><li><p>Takes a scaled DataFrame, the list of feature columns to use, the target column name, sequence length, and forecast horizon.</p></li><li><p>Includes checks for valid input and sufficient data length.</p></li><li><p>Iterates through the data, creating sequences (X) of length sequence_length and corresponding targets (y) taken forecast_horizon steps after the end of each sequence.</p></li><li><p>Returns the sequences and targets as NumPy arrays.</p></li></ul></li><li><p>Main Execution:</p><ul><li><p>Checks that the necessary scaled DataFrames (train_scaled_df, etc.) and the feature list (final_feature_columns_used) exist from Part 5.</p></li><li><p>Calls create_sequences_from_df separately for the training, validation, and test sets.</p></li><li><p>Prints the shapes of the resulting NumPy arrays (X_train, y_train, etc.).</p></li><li><p>Saves the generated sequences and the feature list used to create them into a single .npz file for easy loading later. The filename dynamically includes the forecast horizon and feature type based on the configuration in Part 1.</p></li></ul></li></ol><p>After running this cell, you will have the sequence arrays (X_train, y_train, X_val, y_val, X_test, y_test) ready for the LSTM model definition (Part 7) and subsequent training/tuning steps.</p></div><div id="kk0kaglqkthx" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div id="kk0kaglqkthx-code" class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">def create_sequences_from_df(df, seq_length, target_col_idx=0, pred_step=1, return_targets=True):
    &quot;&quot;&quot;
    Creates sequences for LSTM model from a DataFrame.
    &quot;&quot;&quot;
    xs = []
    ys = []

    # Convert dataframe to numpy if needed
    data = df.values if hasattr(df, &#x27;values&#x27;) else df

    # Loop through data
    # We stop earlier to account for the prediction step ahead (pred_step)
    for i in range(len(data) - seq_length - pred_step + 1):
        x = data[i:(i + seq_length)]
        xs.append(x)

        if return_targets:
            # Target is &#x27;pred_step&#x27; steps after the sequence ends
            y = data[i + seq_length + pred_step - 1, target_col_idx]
            ys.append(y)

    return np.array(xs), np.array(ys)

if __name__ == &quot;__main__&quot;:
    if &#x27;train_scaled_df&#x27; in locals() and train_scaled_df is not None:

        # Dictionary to automate the loop
        data_splits = {
            &#x27;train&#x27;: train_scaled_df,
            &#x27;val&#x27;:   validation_scaled_df,
            &#x27;test&#x27;:  test_scaled_df
        }

        sequences = {}
        print(f&quot;\n--- Creating Sequences (Horizon: {FORECAST_HORIZON}, Seq Length: {SEQUENCE_LENGTH}) ---&quot;)

        for name, df in data_splits.items():
            print(f&quot;Generating {name} sequences...&quot;)

            # --- CRITICAL FIX START ---
            # 1. Determine which columns to use.
            # We must ensure the TARGET column is included in the data so we can extract &#x27;y&#x27;.
            if &#x27;final_feature_columns_used&#x27; in locals():
                # Make a copy of the list so we don&#x27;t modify the original variable
                cols_to_use = list(final_feature_columns_used)

                # If target is missing from features, add it so we can create &#x27;y&#x27;
                if TARGET_BINARY_COL not in cols_to_use:
                    print(f&quot;  Note: Adding &#x27;{TARGET_BINARY_COL}&#x27; back to dataframe for sequence generation.&quot;)
                    cols_to_use.append(TARGET_BINARY_COL)

                df_subset = df[cols_to_use]
            else:
                df_subset = df
            # --- CRITICAL FIX END ---

            # 2. Find the integer index of the target column inside this new subset
            try:
                target_idx = df_subset.columns.get_loc(TARGET_BINARY_COL)
            except KeyError:
                raise KeyError(f&quot;Target column &#x27;{TARGET_BINARY_COL}&#x27; not found in the dataframe. Check spelling!&quot;)

            # 3. Call the function
            X, y = create_sequences_from_df(
                df_subset,              # The Data (Features + Target)
                SEQUENCE_LENGTH,        # Window size
                target_idx,             # Index of the target column
                FORECAST_HORIZON,       # Prediction step
                True                    # Return targets?
            )

            # Store in dictionary
            sequences[f&#x27;X_{name}&#x27;] = X
            sequences[f&#x27;y_{name}&#x27;] = y
            print(f&quot;  {name}: X={X.shape}, y={y.shape}&quot;)

        # Unpack for later parts
        X_train, y_train = sequences[&#x27;X_train&#x27;], sequences[&#x27;y_train&#x27;]
        X_val, y_val     = sequences[&#x27;X_val&#x27;], sequences[&#x27;y_val&#x27;]
        X_test, y_test   = sequences[&#x27;X_test&#x27;], sequences[&#x27;y_test&#x27;]

        # Save to file
        if X_train.size &gt; 0:
            suffix = feature_suffix if &#x27;feature_suffix&#x27; in locals() else &quot;&quot;
            fname = SEQUENCES_FILENAME_TEMPLATE.format(FORECAST_HORIZON, suffix)

            # Save arrays and the column list for reference
            np.savez(fname, **sequences, feature_columns=df_subset.columns)
            print(f&quot;\nSaved sequences to {fname}&quot;)

    else:
        print(&quot;Skipping Part 6: Scaled data not found.&quot;)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" id="kk0kaglqkthx-outputs" data-mdast-node-id="0ozrHJ3qDqCmJjs3yeppV" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>
--- Creating Sequences (Horizon: 1, Seq Length: 12) ---
Generating train sequences...
  Note: Adding &#x27;bloom_target&#x27; back to dataframe for sequence generation.
  train: X=(1111, 12, 26), y=(1111,)
Generating val sequences...
  Note: Adding &#x27;bloom_target&#x27; back to dataframe for sequence generation.
  val: X=(228, 12, 26), y=(228,)
Generating test sequences...
  Note: Adding &#x27;bloom_target&#x27; back to dataframe for sequence generation.
  test: X=(230, 12, 26), y=(230,)

Saved sequences to output_refactored/sequences_horizon1wk_enhanced.npz
</span></code></pre></div></div></div></div><div id="d79etju0ka0m" class="myst-jp-nb-block relative group/block"><h3 id="part-7-model-definition-lstm" class="relative group"><span class="heading-text">Part 7: Model Definition (LSTM).</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#part-7-model-definition-lstm" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p>This part defines the function build_lstm_model responsible for creating the LSTM network architecture using TensorFlow/Keras. The function is designed to be flexible: it can use default hyperparameters defined in Part 1, or it can accept a hp object from KerasTuner (which we’ll use later in Part 8) to create models with varying hyperparameters during the tuning process.</p><p>This cell defines the build_lstm_model function. It specifies the layers (LSTM, Dropout, Dense) and allows for hyperparameter configuration either through defaults or a KerasTuner object. It then demonstrates building the model with default hyperparameters, assuming the input shape is known from Part 6.</p><p>Explanation:</p><ol start="1"><li><p>Function build_lstm_model:</p><ul><li><p>Takes input_shape (required) and an optional KerasTuner hp object.</p></li><li><p>Hyperparameter Handling: If hp is provided (during tuning), it defines hyperparameters using <a target="_blank" rel="noreferrer" href="http://hp.Int" class="link">hp.Int<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="link-icon"><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a>, hp.Float, hp.Choice. If hp is None (when building the default or final model), it uses the DEFAULT_ variables defined in Part 1 (with fallbacks just in case).</p></li><li><p>Architecture: Defines a two-layer LSTM structure with Dropout. You can easily modify this (e.g., change to GRU, add Dense layers, use Bidirectional) by editing this function.</p></li><li><p>Compilation: Compiles the model inside the function using Adam optimizer and binary cross-entropy loss. This is convenient for KerasTuner.</p></li></ul></li><li><p>Main Execution (if <strong>name</strong> == “<strong>main</strong>”:):</p><ul><li><p>Checks if X_train exists (from Part 6) to get the required input_shape.</p></li><li><p>Calls build_lstm_model with hp=None to create an instance using the default hyperparameters.</p></li><li><p>Prints the model summary.</p></li></ul></li></ol><p>After running this cell, the function build_lstm_model is defined and ready to be used either by KerasTuner (Part 8) or directly for training the default/final model (Part 9). The lstm_model_default variable holds an example compiled model instance (useful for checking).</p></div><div id="ztwksvfhkel6" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div id="ztwksvfhkel6-code" class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">def build_lstm_model(input_shape, hp=None):
    &quot;&quot;&quot;
    Builds and compiles an LSTM model.

    Args:
        input_shape (tuple): The shape of the input data (time_steps, features).
        hp (KerasTuner.HyperParameters, optional): Hyperparameters for tuning.
                                                   If None, uses defaults.
    Returns:
        model: A compiled Keras model.
    &quot;&quot;&quot;

    # ---------------------------------------------------------
    # 1. Hyperparameter Definition (Dual Mode)
    # ---------------------------------------------------------
    # If &#x27;hp&#x27; is provided, we are in Tuning Mode.
    # If &#x27;hp&#x27; is None, we use the global defaults (defined in Part 1).

    if hp:
        # Tuning Mode: Ask KerasTuner to try different values
        units_1 = hp.Int(&#x27;lstm_units_1&#x27;, min_value=32, max_value=128, step=32)
        units_2 = hp.Int(&#x27;lstm_units_2&#x27;, min_value=16, max_value=64, step=16)
        dropout_rate = hp.Float(&#x27;dropout_rate&#x27;, min_value=0.1, max_value=0.5, step=0.1)
        learning_rate = hp.Choice(&#x27;learning_rate&#x27;, values=[1e-2, 1e-3, 1e-4])
    else:
        # Default Mode: Use variables from Part 1 (with safety fallbacks)
        units_1 = globals().get(&#x27;DEFAULT_LSTM_UNITS&#x27;, 64)
        units_2 = globals().get(&#x27;DEFAULT_LSTM_UNITS&#x27;, 32) # Using same default or half
        dropout_rate = globals().get(&#x27;DEFAULT_DROPOUT&#x27;, 0.2)
        learning_rate = globals().get(&#x27;DEFAULT_LEARNING_RATE&#x27;, 0.001)

    # ---------------------------------------------------------
    # 2. Model Architecture
    # ---------------------------------------------------------
    model = Sequential()

    # Input Layer
    model.add(Input(shape=input_shape))

    # LSTM Layer 1 (Must return sequences to feed the next LSTM layer)
    model.add(LSTM(units=units_1, return_sequences=True))
    model.add(Dropout(dropout_rate))

    # LSTM Layer 2 (return_sequences=False because next is Dense)
    model.add(LSTM(units=units_2, return_sequences=False))
    model.add(Dropout(dropout_rate))

    # Output Layer (1 unit for Binary Classification: Bloom vs No Bloom)
    model.add(Dense(1, activation=&#x27;sigmoid&#x27;))

    # ---------------------------------------------------------
    # 3. Compilation
    # ---------------------------------------------------------
    model.compile(
        optimizer=Adam(learning_rate=learning_rate),
        loss=&#x27;binary_crossentropy&#x27;,
        metrics=[BinaryAccuracy(name=&#x27;accuracy&#x27;), AUC(name=&#x27;auc&#x27;), Recall(name=&#x27;recall&#x27;),Precision(name=&#x27;precision&#x27;)]
    )

    return model

# --- Main Execution (Test Block) ---
if __name__ == &quot;__main__&quot;:
    print(&quot;--- Testing Model Definition (Step 7) ---&quot;)

    # We need X_train to know the input shape
    if &#x27;X_train&#x27; in locals() and X_train is not None:

        # Determine shape: (Sequence Length, Number of Features)
        # X_train shape is typically (Samples, Time Steps, Features)
        # We need the last two dimensions for input_shape
        input_shape_test = (X_train.shape[1], X_train.shape[2])
        print(f&quot;Input Shape detected: {input_shape_test}&quot;)

        # Build a &quot;Default&quot; model (hp=None) to verify it works
        try:
            model_test = build_lstm_model(input_shape_test, hp=None)
            print(&quot;\nModel built successfully!&quot;)
            model_test.summary()
        except Exception as e:
            print(f&quot;Error building model: {e}&quot;)

    else:
        print(&quot;Warning: X_train not found. Run Step 6 first to test this function.&quot;)
        print(&quot;Function &#x27;build_lstm_model&#x27; is defined but not tested.&quot;)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" id="ztwksvfhkel6-outputs" data-mdast-node-id="aPigXkpNouOhOaCTfyiXr" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>--- Testing Model Definition (Step 7) ---
Input Shape detected: (12, 26)

Model built successfully!
</span></code></pre></div></div><div><div class="p-2.5">Loading...</div></div><div><div class="p-2.5">Loading...</div></div><div><div class="p-2.5">Loading...</div></div><div><div class="p-2.5">Loading...</div></div><div><div class="p-2.5">Loading...</div></div></div></div><div id="id-zkfytzkkocz" class="myst-jp-nb-block relative group/block"><h3 id="part-8-hyperparameter-tuning-with-kerastuner-conditional" class="relative group"><span class="heading-text">Part 8: Hyperparameter Tuning with KerasTuner (Conditional)</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#part-8-hyperparameter-tuning-with-kerastuner-conditional" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p>This cell sets up and runs the KerasTuner search process if the PERFORM_TUNING flag (from Part 1) is set to True. It uses the build_lstm_model function (from Part 7) as the hypermodel builder and searches for the best combination of LSTM units, dropout rate, and learning rate based on validation accuracy.</p><p>Explanation:</p><ol start="1"><li><p>Conditional Execution: The entire cell’s logic is wrapped in if can_tune:, which checks if PERFORM_TUNING was set to True in Part 1 and if the keras_tuner library (kt) was successfully imported. If not, it prints a message and skips tuning.</p></li><li><p>Prerequisite Checks: Inside the if block, it checks if the necessary sequence data (X_train, y_train, etc.) and the build_lstm_model function exist before proceeding.</p></li><li><p>Tuner Setup:</p><ul><li><p>Creates a keras_tuner.RandomSearch instance (you could switch to kt.Hyperband for potentially faster convergence).</p></li><li><p>Passes a lambda function lambda hp: build_lstm_model(input_shape_tune, hp=hp) as the hypermodel builder.</p></li><li><p>This ensures the build_lstm_model function receives the tuner’s hp object to define the model architecture with tunable parameters.</p></li><li><p>Sets the objective to ‘val_accuracy’ (tune for best accuracy on the validation set).</p></li><li><p>Configures max_trials, directory, project_name, etc.</p></li></ul></li><li><p>Run Search:</p><ul><li><p>Calls <a target="_blank" rel="noreferrer" href="http://tuner.search" class="link">tuner.search<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="link-icon"><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a>(), passing the training and validation data.</p></li><li><p>Uses a dedicated EarlyStopping callback with potentially shorter patience for the tuning phase itself.</p></li></ul></li><li><p>Retrieve Best Hyperparameters:</p><ul><li><p>After the search completes, tuner.get_best_hyperparameters(num_trials=1)[0] retrieves the HyperParameters object corresponding to the best trial.</p></li><li><p>The values for the tuned hyperparameters (e.g., lstm_units_1, dropout_rate, learning_rate) are extracted and printed.</p></li></ul></li></ol><p>The best_hps variable stores this object for use in the next step (Part 9).
Includes error handling in case the tuner fails or doesn’t return results.
If PERFORM_TUNING is False, this cell will simply print a message and set best_hps to None. The next step (Part 9) will then know to use the default hyperparameters defined in Part 1.</p></div><div id="asm54ga-ko4e" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div id="asm54ga-ko4e-code" class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre"># 1. Initialize Variables
best_hps = None
best_model_from_tuner = None

# 2. Check Prerequisites
# We need X_train (Data) and build_lstm_model (Function)
can_tune = True

if &#x27;X_train&#x27; not in locals() or X_train is None:
    print(&quot;Error: X_train not found. Please run Step 6 (Data Generation) first.&quot;)
    can_tune = False

if &#x27;build_lstm_model&#x27; not in locals():
    print(&quot;Error: build_lstm_model not found. Please run Step 7 (Model Definition) first.&quot;)
    can_tune = False

if &#x27;PERFORM_TUNING&#x27; not in locals() or not PERFORM_TUNING:
    print(&quot;Skipping Tuning: PERFORM_TUNING flag is False or missing.&quot;)
    can_tune = False

# 3. Run Tuner
if can_tune:
    print(&quot;\n--- Setting up KerasTuner ---&quot;)

    # Define Input Shape for the builder
    # Shape = (Time Steps, Features) -&gt; (X_train.shape[1], X_train.shape[2])
    input_shape_tune = (X_train.shape[1], X_train.shape[2])
    print(f&quot;Tuning Input Shape: {input_shape_tune}&quot;)

    # Initialize the RandomSearch Tuner
    # Note: We use objective=&#x27;val_auc&#x27; to match the name=&#x27;auc&#x27; in Step 7
    tuner = kt.RandomSearch(
        lambda hp: build_lstm_model(input_shape_tune, hp=hp),
        objective=kt.Objective(&quot;val_auc&quot;, direction=&quot;max&quot;),
        max_trials=TUNER_MAX_TRIALS,       # Defined in Step 1 (e.g., 10 or 20)
        executions_per_trial=2,            # Run each trial twice to reduce luck
        directory=TUNER_PROJECT_DIR,       # &quot;hab_tuning&quot;
        project_name=TUNER_PROJECT_NAME,   # &quot;bloom_prediction&quot;
        overwrite=True                     # Start fresh every time
    )

    tuner.search_space_summary()

    print(&quot;\n--- Starting Search ---&quot;)

    # Early Stopping strictly for the tuning phase (speed things up)
    tuner_early_stopping = EarlyStopping(monitor=&#x27;val_loss&#x27;, patience=4, verbose=0)

    try:
        # CRITICAL FIX: Pass Numpy arrays directly.
        # Do not convert to tf.Tensor manually here.
        tuner.search(
            X_train, y_train,
            epochs=TUNER_EPOCHS,           # Defined in Step 1 (e.g., 10 or 20)
            batch_size=TUNER_BATCH_SIZE,   # Defined in Step 1 (e.g., 32)
            validation_data=(X_val, y_val),
            callbacks=[tuner_early_stopping],
            verbose=1
        )

        print(&quot;\n--- Hyperparameter Search Finished ---&quot;)

        # 4. Retrieve Best Results
        best_hps_list = tuner.get_best_hyperparameters(num_trials=1)

        if best_hps_list:
            best_hps = best_hps_list[0]
            print(&quot;Best Hyperparameters Found:&quot;)
            print(f&quot;  LSTM Units 1:  {best_hps.values.get(&#x27;lstm_units_1&#x27;)}&quot;)
            print(f&quot;  LSTM Units 2:  {best_hps.values.get(&#x27;lstm_units_2&#x27;)}&quot;)
            print(f&quot;  Dropout Rate:  {best_hps.values.get(&#x27;dropout_rate&#x27;)}&quot;)
            print(f&quot;  Learning Rate: {best_hps.values.get(&#x27;learning_rate&#x27;)}&quot;)

            # (Optional) You can build the best model immediately if you want
            # best_model = tuner.hypermodel.build(best_hps)
        else:
            print(&quot;Warning: Tuner finished but returned no hyperparameters.&quot;)
            best_hps = None

    except Exception as e:
        print(f&quot;CRITICAL ERROR during tuning: {e}&quot;)
        best_hps = None

else:
    print(&quot;Tuner skipped.&quot;)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" id="asm54ga_ko4e-outputs" data-mdast-node-id="8BnHSxuJVAND4ZDkXiCln" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>Trial 10 Complete [00h 00m 12s]
val_auc: 0.9256471693515778

Best val_auc So Far: 0.9290720522403717
Total elapsed time: 00h 02m 41s

--- Hyperparameter Search Finished ---
Best Hyperparameters Found:
  LSTM Units 1:  128
  LSTM Units 2:  32
  Dropout Rate:  0.1
  Learning Rate: 0.01
</span></code></pre></div></div></div></div><div id="n5nvbjk-leet" class="myst-jp-nb-block relative group/block"><h3 id="part-9-model-training" class="relative group"><span class="heading-text">Part 9: Model Training</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#part-9-model-training" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p>This cell defines the main training function and executes it. It uses the configuration flags from Part 1 to determine whether to use class weights and whether to use default or tuned hyperparameters (from best_hps variable potentially created in Part 8). It saves the best model weights found during training.</p><p>Explanation:</p><ol start="1"><li><p>Function train_model:</p><ul><li><p>Takes training/validation data, input shape, configuration flags (use_class_weight), optional tuned hyperparameters (best_hps), training parameters (epochs, batch_size, patience), and paths/names for saving.</p></li><li><p>Builds Model: Calls build_lstm_model (from Part 7), passing best_hps if it’s available (meaning tuning was done in Part 8), otherwise hp=None is passed and the function uses defaults.</p></li><li><p>Class Weights: If use_class_weight is True, it calculates the weights using compute_class_weight on y_train.</p></li><li><p>Compiles: Ensures the model is compiled with the correct learning rate (either default or from best_hps).</p></li><li><p>Callbacks: Sets up EarlyStopping and ModelCheckpoint. Note that ModelCheckpoint now saves the entire model in the .keras format (recommended over .h5), as we are not using the custom subclassed model in this part. The filename includes the model_type_name.</p></li><li><p>Fits Model: Calls <a target="_blank" rel="noreferrer" href="http://model.fit" class="link">model.fit<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="link-icon"><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 6H5.25A2.25 2.25 0 0 0 3 8.25v10.5A2.25 2.25 0 0 0 5.25 21h10.5A2.25 2.25 0 0 0 18 18.75V10.5m-10.5 6L21 3m0 0h-5.25M21 3v5.25"></path></svg></a>(), passing the class_weight dictionary if applicable.</p></li><li><p>Loads Best: After training, it explicitly loads the best model saved by ModelCheckpoint to ensure the returned model represents the best validation performance.</p></li><li><p>Returns: The trained model object and the training history.</p></li></ul></li><li><p>Main Execution:</p><ul><li><p>Checks that necessary data (X_train, etc.) and configuration flags exist.</p></li><li><p>Determines the model_type_name based on whether tuning was performed and class weights are used (e.g., “baseline”, “baseline_weighted”, “tuned”, “tuned_weighted”).</p></li><li><p>Calls the train_model function with the appropriate arguments.</p></li><li><p>Stores the returned trained model in the trained_model variable.</p></li><li><p>Plots the training/validation loss and accuracy from the returned history object.</p></li><li><p>Sets a general model_ready_for_eval flag for the next step (Part 10: Baseline Evaluation).</p></li></ul></li></ol><p>After running this cell, the trained_model variable will hold the trained LSTM model (either using default or tuned hyperparameters, and potentially trained with class weights), ready for evaluation on the test set in Part 10. The best version of this model is also saved to a file.</p></div><div id="pzozx1kvj7t0" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div id="pzozx1kvj7t0-code" class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">def train_final_model(x_train, y_train, x_val, y_val,
                      input_shape,
                      use_class_weight=False,
                      best_hps=None,
                      epochs=50,
                      batch_size=32,
                      patience=10,
                      checkpoint_path_template=&#x27;output/model_{}.keras&#x27;,
                      model_name=&#x27;baseline&#x27;):

    print(f&quot;\n--- Training Final Model: {model_name} ---&quot;)

    # 1. Build Model
    # We assume build_lstm_model is available from Step 7
    if &#x27;build_lstm_model&#x27; not in globals():
        raise NameError(&quot;build_lstm_model function not defined. Please run Step 7.&quot;)

    model = build_lstm_model(input_shape, hp=best_hps)
    if model is None:
        return None, None

    # 2. Calculate Class Weights (Critical for Blooms)
    class_weights_dict = None
    if use_class_weight:
        print(&quot;Calculating class weights for imbalanced data...&quot;)
        try:
            # Flatten to ensure 1D array for weight calculation
            y_flat = y_train.flatten().astype(int)
            classes = np.unique(y_flat)
            weights = compute_class_weight(class_weight=&#x27;balanced&#x27;, classes=classes, y=y_flat)
            class_weights_dict = dict(zip(classes, weights))
            print(f&quot;  Class weights: {class_weights_dict}&quot;)
        except Exception as e:
            print(f&quot;  Error calculating weights: {e}. Using default.&quot;)

    # 3. Compile Model with BETTER METRICS
    # We add AUC, Precision, and Recall to see if it actually finds blooms
    learning_rate = best_hps.get(&#x27;learning_rate&#x27;) if best_hps else DEFAULT_LEARNING_RATE

    # Re-compile to ensure metrics and optimizer are fresh
    model.compile(
        optimizer=Adam(learning_rate=learning_rate),
        loss=BinaryCrossentropy(),
        metrics=[
            BinaryAccuracy(name=&#x27;accuracy&#x27;),
            AUC(name=&#x27;auc&#x27;),           # &lt;--- Critical for Imbalanced Data
            Precision(name=&#x27;precision&#x27;),
            Recall(name=&#x27;recall&#x27;)
        ]
    )
    print(f&quot;Model compiled (LR={learning_rate}). Monitoring AUC/Precision/Recall.&quot;)

    # 4. Setup Callbacks
    model_path = checkpoint_path_template.format(model_name)

    # EarlyStopping: restore_best_weights=True ensures &#x27;model&#x27; var is perfect at the end
    early_stopping = EarlyStopping(
        monitor=&#x27;val_loss&#x27;,
        patience=patience,
        verbose=1,
        restore_best_weights=True
    )

    model_checkpoint = ModelCheckpoint(
        filepath=model_path,
        monitor=&#x27;val_loss&#x27;,
        save_best_only=True,
        verbose=1
    )

    # 5. Train
    print(f&quot;\nStarting training for {epochs} epochs...&quot;)

    # Ensure raw numpy arrays (safer than Tensors here)
    x_train = np.array(x_train).astype(&#x27;float32&#x27;)
    y_train = np.array(y_train).astype(&#x27;float32&#x27;)
    x_val = np.array(x_val).astype(&#x27;float32&#x27;)
    y_val = np.array(y_val).astype(&#x27;float32&#x27;)

    history = model.fit(
        x_train, y_train,
        epochs=epochs,
        batch_size=batch_size,
        validation_data=(x_val, y_val),
        callbacks=[early_stopping, model_checkpoint],
        class_weight=class_weights_dict,
        verbose=1
    )

    print(f&quot;\n--- Training Finished. Best model saved to {model_path} ---&quot;)

    # Because restore_best_weights=True, &#x27;model&#x27; is already the best version.
    return model, history


# --- Execute Training ---
if __name__ == &quot;__main__&quot;:
    try:
        # Check prerequisites
        if &#x27;X_train&#x27; not in locals(): raise NameError(&quot;Run Step 6 (Sequence Creation) first.&quot;)

        # Define Input Shape
        input_shape_train = (X_train.shape[1], X_train.shape[2])

        # Naming logic
        model_name = &quot;tuned&quot; if (PERFORM_TUNING and best_hps) else &quot;baseline&quot;
        if USE_CLASS_WEIGHT: model_name += &quot;_weighted&quot;

        # TRAIN
        trained_model, training_history = train_final_model(
            X_train, y_train, X_val, y_val,
            input_shape=input_shape_train,
            use_class_weight=USE_CLASS_WEIGHT,
            best_hps=best_hps if PERFORM_TUNING else None,
            epochs=TRAIN_EPOCHS,
            batch_size=TRAIN_BATCH_SIZE,
            patience=TRAIN_PATIENCE,
            checkpoint_path_template=MODEL_CHECKPOINT_TEMPLATE,
            model_name=model_name
        )

        if trained_model:
            # --- PUBLICATION QUALITY PLOTTING ---
            print(f&quot;\nGeneratng publication-quality plots for {model_name}...&quot;)

            import matplotlib.pyplot as plt
            import os  # &lt;--- Need this to create folders

            # 1. CRITICAL FIX: Create the output directory if it doesn&#x27;t exist
            os.makedirs(&#x27;output&#x27;, exist_ok=True)

            # 2. Global Style Settings for Academic Papers
            plt.rcdefaults()
            params = {
                &#x27;font.family&#x27;: &#x27;serif&#x27;,
                &#x27;axes.titlesize&#x27;: 18,
                &#x27;axes.labelsize&#x27;: 16,
                &#x27;xtick.labelsize&#x27;: 14,
                &#x27;ytick.labelsize&#x27;: 14,
                &#x27;legend.fontsize&#x27;: 14,
                &#x27;figure.figsize&#x27;: (10, 6),
                &#x27;lines.linewidth&#x27;: 2.5
            }
            plt.rcParams.update(params)

            # --- FIGURE 1: LOSS CURVE ---
            plt.figure(dpi=300)

            plt.plot(training_history.history[&#x27;loss&#x27;], label=&#x27;Training Loss&#x27;, color=&#x27;#1f77b4&#x27;, linestyle=&#x27;-&#x27;)
            plt.plot(training_history.history[&#x27;val_loss&#x27;], label=&#x27;Validation Loss&#x27;, color=&#x27;#d62728&#x27;, linestyle=&#x27;--&#x27;)

            plt.title(&#x27;Model Loss over Epochs&#x27;)
            plt.xlabel(&#x27;Epoch&#x27;)
            plt.ylabel(&#x27;Binary Crossentropy Loss&#x27;)
            plt.legend(frameon=True, fancybox=False, edgecolor=&#x27;black&#x27;)
            plt.grid(True, linestyle=&#x27;:&#x27;, alpha=0.6)
            plt.tight_layout()

            # Save safely now that folder exists
            plt.savefig(f&quot;output/plot_loss_{model_name}.png&quot;, bbox_inches=&#x27;tight&#x27;)
            plt.show()
            print(f&quot;Saved: output/plot_loss_{model_name}.png&quot;)

            # --- FIGURE 2: AUC / PERFORMANCE CURVE ---
            plt.figure(dpi=300)

            if &#x27;val_auc&#x27; in training_history.history:
                metric_name = &#x27;AUC&#x27;
                train_data = training_history.history[&#x27;auc&#x27;]
                val_data = training_history.history[&#x27;val_auc&#x27;]
                color_train = &#x27;#2ca02c&#x27;
                color_val = &#x27;#ff7f0e&#x27;
            else:
                metric_name = &#x27;Accuracy&#x27;
                train_data = training_history.history[&#x27;accuracy&#x27;]
                val_data = training_history.history[&#x27;val_accuracy&#x27;]
                color_train = &#x27;black&#x27;
                color_val = &#x27;gray&#x27;

            plt.plot(train_data, label=f&#x27;Training {metric_name}&#x27;, color=color_train, linestyle=&#x27;-&#x27;)
            plt.plot(val_data, label=f&#x27;Validation {metric_name}&#x27;, color=color_val, linestyle=&#x27;--&#x27;)

            plt.title(f&#x27;Model {metric_name} Performance&#x27;)
            plt.xlabel(&#x27;Epoch&#x27;)
            plt.ylabel(f&#x27;{metric_name} Score&#x27;)
            plt.legend(frameon=True, fancybox=False, edgecolor=&#x27;black&#x27;)
            plt.grid(True, linestyle=&#x27;:&#x27;, alpha=0.6)
            plt.tight_layout()

            plt.savefig(f&quot;output/plot_{metric_name.lower()}_{model_name}.png&quot;, bbox_inches=&#x27;tight&#x27;)
            plt.show()
            print(f&quot;Saved: output/plot_{metric_name.lower()}_{model_name}.png&quot;)

            model_ready_for_eval = True
        else:
            model_ready_for_eval = False

    except Exception as e:
         print(f&quot;Error: {e}&quot;)
         model_ready_for_eval = False</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" id="pzozx1kvj7t0-outputs" data-mdast-node-id="UJjVsCVTMDcr8MfqzX50C" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>
--- Training Final Model: tuned_weighted ---
Calculating class weights for imbalanced data...
  Class weights: {np.int64(0): np.float64(0.6527614571092832), np.int64(1): np.float64(2.1365384615384615)}
Model compiled (LR=0.01). Monitoring AUC/Precision/Recall.

Starting training for 50 epochs...
Epoch 1/50
</span><span style="font-weight:bold">33/35</span><span> </span><span style="color:rgb(0, 187, 0)">━━━━━━━━━━━━━━━━━━</span><span style="color:rgb(255,255,255)">━━</span><span> </span><span style="font-weight:bold">0s</span><span> 7ms/step - accuracy: 0.7266 - auc: 0.7623 - loss: 0.5681 - precision: 0.4174 - recall: 0.6844
Epoch 1: val_loss improved from inf to 0.47479, saving model to output_refactored/best_lstm_model_tuned_weighted.keras
</span><span style="font-weight:bold">35/35</span><span> </span><span style="color:rgb(0, 187, 0)">━━━━━━━━━━━━━━━━━━━━</span><span> </span><span style="font-weight:bold">3s</span><span> 28ms/step - accuracy: 0.7291 - auc: 0.7679 - loss: 0.5629 - precision: 0.4231 - recall: 0.6897 - val_accuracy: 0.7149 - val_auc: 0.9209 - val_loss: 0.4748 - val_precision: 0.5921 - val_recall: 0.9677
Epoch 2/50
</span><span style="font-weight:bold">33/35</span><span> </span><span style="color:rgb(0, 187, 0)">━━━━━━━━━━━━━━━━━━</span><span style="color:rgb(255,255,255)">━━</span><span> </span><span style="font-weight:bold">0s</span><span> 7ms/step - accuracy: 0.8109 - auc: 0.9159 - loss: 0.3887 - precision: 0.5893 - recall: 0.9056
Epoch 2: val_loss improved from 0.47479 to 0.38012, saving model to output_refactored/best_lstm_model_tuned_weighted.keras
</span><span style="font-weight:bold">35/35</span><span> </span><span style="color:rgb(0, 187, 0)">━━━━━━━━━━━━━━━━━━━━</span><span> </span><span style="font-weight:bold">0s</span><span> 10ms/step - accuracy: 0.8106 - auc: 0.9141 - loss: 0.3908 - precision: 0.5869 - recall: 0.9001 - val_accuracy: 0.7851 - val_auc: 0.9227 - val_loss: 0.3801 - val_precision: 0.6930 - val_recall: 0.8495
Epoch 3/50
</span><span style="font-weight:bold">33/35</span><span> </span><span style="color:rgb(0, 187, 0)">━━━━━━━━━━━━━━━━━━</span><span style="color:rgb(255,255,255)">━━</span><span> </span><span style="font-weight:bold">0s</span><span> 7ms/step - accuracy: 0.8125 - auc: 0.8956 - loss: 0.4167 - precision: 0.5762 - recall: 0.7902
Epoch 3: val_loss did not improve from 0.38012
</span><span style="font-weight:bold">35/35</span><span> </span><span style="color:rgb(0, 187, 0)">━━━━━━━━━━━━━━━━━━━━</span><span> </span><span style="font-weight:bold">0s</span><span> 9ms/step - accuracy: 0.8129 - auc: 0.8958 - loss: 0.4158 - precision: 0.5765 - recall: 0.7911 - val_accuracy: 0.7500 - val_auc: 0.9261 - val_loss: 0.5218 - val_precision: 0.6200 - val_recall: 1.0000
Epoch 4/50
</span><span style="font-weight:bold">34/35</span><span> </span><span style="color:rgb(0, 187, 0)">━━━━━━━━━━━━━━━━━━━</span><span style="color:rgb(255,255,255)">━</span><span> </span><span style="font-weight:bold">0s</span><span> 6ms/step - accuracy: 0.8166 - auc: 0.9158 - loss: 0.3768 - precision: 0.5881 - recall: 0.8814
Epoch 4: val_loss did not improve from 0.38012
</span><span style="font-weight:bold">35/35</span><span> </span><span style="color:rgb(0, 187, 0)">━━━━━━━━━━━━━━━━━━━━</span><span> </span><span style="font-weight:bold">0s</span><span> 9ms/step - accuracy: 0.8169 - auc: 0.9158 - loss: 0.3764 - precision: 0.5876 - recall: 0.8810 - val_accuracy: 0.7982 - val_auc: 0.9179 - val_loss: 0.4197 - val_precision: 0.6975 - val_recall: 0.8925
Epoch 5/50
</span><span style="font-weight:bold">33/35</span><span> </span><span style="color:rgb(0, 187, 0)">━━━━━━━━━━━━━━━━━━</span><span style="color:rgb(255,255,255)">━━</span><span> </span><span style="font-weight:bold">0s</span><span> 6ms/step - accuracy: 0.8182 - auc: 0.9030 - loss: 0.3871 - precision: 0.5642 - recall: 0.8112
Epoch 5: val_loss did not improve from 0.38012
</span><span style="font-weight:bold">35/35</span><span> </span><span style="color:rgb(0, 187, 0)">━━━━━━━━━━━━━━━━━━━━</span><span> </span><span style="font-weight:bold">0s</span><span> 9ms/step - accuracy: 0.8201 - auc: 0.9041 - loss: 0.3855 - precision: 0.5687 - recall: 0.8137 - val_accuracy: 0.8026 - val_auc: 0.9248 - val_loss: 0.3869 - val_precision: 0.7143 - val_recall: 0.8602
Epoch 6/50
</span><span style="font-weight:bold">33/35</span><span> </span><span style="color:rgb(0, 187, 0)">━━━━━━━━━━━━━━━━━━</span><span style="color:rgb(255,255,255)">━━</span><span> </span><span style="font-weight:bold">0s</span><span> 6ms/step - accuracy: 0.8601 - auc: 0.9436 - loss: 0.3022 - precision: 0.6464 - recall: 0.8500
Epoch 6: val_loss did not improve from 0.38012
</span><span style="font-weight:bold">35/35</span><span> </span><span style="color:rgb(0, 187, 0)">━━━━━━━━━━━━━━━━━━━━</span><span> </span><span style="font-weight:bold">0s</span><span> 9ms/step - accuracy: 0.8589 - auc: 0.9429 - loss: 0.3041 - precision: 0.6444 - recall: 0.8512 - val_accuracy: 0.8070 - val_auc: 0.9126 - val_loss: 0.4281 - val_precision: 0.6992 - val_recall: 0.9247
Epoch 7/50
</span><span style="font-weight:bold">34/35</span><span> </span><span style="color:rgb(0, 187, 0)">━━━━━━━━━━━━━━━━━━━</span><span style="color:rgb(255,255,255)">━</span><span> </span><span style="font-weight:bold">0s</span><span> 6ms/step - accuracy: 0.8743 - auc: 0.9397 - loss: 0.3216 - precision: 0.6860 - recall: 0.8777
Epoch 7: val_loss did not improve from 0.38012
</span><span style="font-weight:bold">35/35</span><span> </span><span style="color:rgb(0, 187, 0)">━━━━━━━━━━━━━━━━━━━━</span><span> </span><span style="font-weight:bold">0s</span><span> 9ms/step - accuracy: 0.8737 - auc: 0.9396 - loss: 0.3214 - precision: 0.6845 - recall: 0.8772 - val_accuracy: 0.8070 - val_auc: 0.9049 - val_loss: 0.4391 - val_precision: 0.7130 - val_recall: 0.8817
Epoch 8/50
</span><span style="font-weight:bold">35/35</span><span> </span><span style="color:rgb(0, 187, 0)">━━━━━━━━━━━━━━━━━━━━</span><span> </span><span style="font-weight:bold">0s</span><span> 6ms/step - accuracy: 0.8945 - auc: 0.9622 - loss: 0.2541 - precision: 0.7110 - recall: 0.9191
Epoch 8: val_loss did not improve from 0.38012
</span><span style="font-weight:bold">35/35</span><span> </span><span style="color:rgb(0, 187, 0)">━━━━━━━━━━━━━━━━━━━━</span><span> </span><span style="font-weight:bold">0s</span><span> 9ms/step - accuracy: 0.8941 - auc: 0.9618 - loss: 0.2551 - precision: 0.7106 - recall: 0.9175 - val_accuracy: 0.7675 - val_auc: 0.8939 - val_loss: 0.4210 - val_precision: 0.6786 - val_recall: 0.8172
Epoch 9/50
</span><span style="font-weight:bold">34/35</span><span> </span><span style="color:rgb(0, 187, 0)">━━━━━━━━━━━━━━━━━━━</span><span style="color:rgb(255,255,255)">━</span><span> </span><span style="font-weight:bold">0s</span><span> 6ms/step - accuracy: 0.8959 - auc: 0.9683 - loss: 0.2292 - precision: 0.6880 - recall: 0.9288
Epoch 9: val_loss did not improve from 0.38012
</span><span style="font-weight:bold">35/35</span><span> </span><span style="color:rgb(0, 187, 0)">━━━━━━━━━━━━━━━━━━━━</span><span> </span><span style="font-weight:bold">0s</span><span> 9ms/step - accuracy: 0.8952 - auc: 0.9677 - loss: 0.2309 - precision: 0.6884 - recall: 0.9272 - val_accuracy: 0.7939 - val_auc: 0.8991 - val_loss: 0.4283 - val_precision: 0.7255 - val_recall: 0.7957
Epoch 10/50
</span><span style="font-weight:bold">35/35</span><span> </span><span style="color:rgb(0, 187, 0)">━━━━━━━━━━━━━━━━━━━━</span><span> </span><span style="font-weight:bold">0s</span><span> 6ms/step - accuracy: 0.9089 - auc: 0.9766 - loss: 0.1960 - precision: 0.7378 - recall: 0.9421
Epoch 10: val_loss did not improve from 0.38012
</span><span style="font-weight:bold">35/35</span><span> </span><span style="color:rgb(0, 187, 0)">━━━━━━━━━━━━━━━━━━━━</span><span> </span><span style="font-weight:bold">0s</span><span> 9ms/step - accuracy: 0.9084 - auc: 0.9763 - loss: 0.1970 - precision: 0.7368 - recall: 0.9415 - val_accuracy: 0.7675 - val_auc: 0.8948 - val_loss: 0.6039 - val_precision: 0.6493 - val_recall: 0.9355
Epoch 11/50
</span><span style="font-weight:bold">33/35</span><span> </span><span style="color:rgb(0, 187, 0)">━━━━━━━━━━━━━━━━━━</span><span style="color:rgb(255,255,255)">━━</span><span> </span><span style="font-weight:bold">0s</span><span> 6ms/step - accuracy: 0.8969 - auc: 0.9649 - loss: 0.2501 - precision: 0.7243 - recall: 0.9167
Epoch 11: val_loss did not improve from 0.38012
</span><span style="font-weight:bold">35/35</span><span> </span><span style="color:rgb(0, 187, 0)">━━━━━━━━━━━━━━━━━━━━</span><span> </span><span style="font-weight:bold">0s</span><span> 9ms/step - accuracy: 0.8975 - auc: 0.9647 - loss: 0.2498 - precision: 0.7251 - recall: 0.9172 - val_accuracy: 0.7807 - val_auc: 0.9039 - val_loss: 0.4826 - val_precision: 0.6720 - val_recall: 0.9032
Epoch 12/50
</span><span style="font-weight:bold">33/35</span><span> </span><span style="color:rgb(0, 187, 0)">━━━━━━━━━━━━━━━━━━</span><span style="color:rgb(255,255,255)">━━</span><span> </span><span style="font-weight:bold">0s</span><span> 6ms/step - accuracy: 0.9140 - auc: 0.9797 - loss: 0.1927 - precision: 0.7393 - recall: 0.9427
Epoch 12: val_loss did not improve from 0.38012
</span><span style="font-weight:bold">35/35</span><span> </span><span style="color:rgb(0, 187, 0)">━━━━━━━━━━━━━━━━━━━━</span><span> </span><span style="font-weight:bold">0s</span><span> 9ms/step - accuracy: 0.9143 - auc: 0.9795 - loss: 0.1926 - precision: 0.7418 - recall: 0.9412 - val_accuracy: 0.8070 - val_auc: 0.8981 - val_loss: 0.4655 - val_precision: 0.7248 - val_recall: 0.8495
Epoch 12: early stopping
Restoring model weights from the end of the best epoch: 2.

--- Training Finished. Best model saved to output_refactored/best_lstm_model_tuned_weighted.keras ---

Generatng publication-quality plots for tuned_weighted...
</span></code></pre></div></div><div data-name="safe-output-image"><img src="/build/b86e62b0e945d1d21567d642183f78cb.png" alt="&lt;Figure size 3000x1800 with 1 Axes&gt;"/></div><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>Saved: output/plot_loss_tuned_weighted.png
</span></code></pre></div></div><div data-name="safe-output-image"><img src="/build/d6cb9c7ac05f37493a8bf8222db5cc21.png" alt="&lt;Figure size 3000x1800 with 1 Axes&gt;"/></div><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>Saved: output/plot_auc_tuned_weighted.png
</span></code></pre></div></div></div></div><div id="swzcbfubl7br" class="myst-jp-nb-block relative group/block"><h3 id="part-10-baseline-evaluation" class="relative group"><span class="heading-text">Part 10: Baseline Evaluation</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#part-10-baseline-evaluation" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p>This cell defines the evaluation function and applies it to the model trained in Part 9 (trained_model). It calculates and prints various classification metrics, plots the confusion matrix and predictions over time, and assesses the model’s ability to predict bloom onsets.</p><p>Explanation:</p><ol start="1"><li><p>Function evaluate_model:</p><ul><li><p>Takes the model object, test data (x_test, y_test), the unscaled test dataframe test_df_unscaled (needed for correct timestamps), sequence length, forecast horizon, and a model_name string as input.</p></li><li><p>Predictions: Runs model.predict(x_test) to get probabilities. Converts probabilities to class predictions using a 0.5 threshold.</p></li><li><p>Metrics Calculation: Uses scikit-learn functions to calculate accuracy, precision, recall, F1-score (for the positive class ‘1’), AUC-ROC (using probabilities), and Brier score (using probabilities). Includes error handling for AUC calculation if only one class is present in y_test.</p></li><li><p>Reporting: Prints a formatted summary of the metrics and the full classification report.\</p></li><li><p>Confusion Matrix: Calculates and plots the confusion matrix using seaborn for better visualization.</p></li><li><p>Time Series Plot: Determines the correct start index in the test_df_unscaled index based on sequence length and forecast horizon to align the predictions (y_pred_prob) with the actual dates. Plots actuals vs. predicted probabilities. Includes checks for length mismatches.</p></li><li><p>Onset Analysis: Implements the logic to find actual bloom onsets (0 -&gt; 1 transitions in y_test) and checks if the model predicted a bloom (class 1) either in the week before the onset or during the onset week. Reports the hit rate.</p></li></ul></li><li><p>Main Execution:</p><ul><li><p>Checks if the necessary variables (trained_model, X_test, y_test, test_df, etc.) exist from previous steps.</p></li><li><p>Calls the evaluate_model function, passing the required arguments. Uses the model_type_trained variable (set at the end of Part 9) to label the output correctly.</p></li></ul></li></ol><p>After running this cell, you will have a comprehensive evaluation of the model trained in Part 9, including standard metrics, plots, and the crucial bloom onset performance assessment. This provides the baseline against which the EnKF-enhanced model (Part 12/13) will be compared.</p></div><div id="mbyyeropl-va" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div id="mbyyeropl-va-code" class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">def evaluate_model(model, x_test, y_test, test_df_unscaled,
                   seq_len, forecast_horizon, model_name=&quot;Model&quot;):
    &quot;&quot;&quot;
    Evaluates model with Optimal Threshold detection and Publication Plots.
    Saves figures to &#x27;output/&#x27; folder.
    &quot;&quot;&quot;
    if model is None or x_test is None or y_test is None:
        print(&quot;Error: Missing model or test data.&quot;)
        return

    # Ensure output directory exists
    os.makedirs(&#x27;output&#x27;, exist_ok=True)

    print(f&quot;\n=== Evaluating Model: {model_name} ===&quot;)

    # 1. Generate Probabilities
    print(&quot;1. Generating predictions...&quot;)
    try:
        # Get raw probabilities (0.0 to 1.0)
        y_pred_prob = model.predict(x_test, verbose=0)
        y_test_eval = y_test.reshape(-1, 1) # Ensure 2D shape
    except Exception as e:
        print(f&quot;Error during prediction: {e}&quot;)
        return

    # 2. Find Optimal Threshold (Fixed Math)
    # ------------------------------------------------------
    precision, recall, thresholds = precision_recall_curve(y_test_eval, y_pred_prob)

    # Calculate F-Score safely (avoid 0/0 division)
    numerator = 2 * precision * recall
    denominator = precision + recall
    with np.errstate(divide=&#x27;ignore&#x27;, invalid=&#x27;ignore&#x27;):
        fscore = np.divide(numerator, denominator)
    fscore = np.nan_to_num(fscore) # Replace NaNs with 0

    # Locate the index of the largest F score
    ix = np.argmax(fscore)

    # Safety check for index bounds
    if ix &gt;= len(thresholds):
        best_thresh = thresholds[-1]
    else:
        best_thresh = thresholds[ix]

    print(f&quot;\nOptimal Decision Threshold: {best_thresh:.4f} (Max F1: {fscore[ix]:.4f})&quot;)
    # ------------------------------------------------------

    # Apply Best Threshold
    y_pred_class = (y_pred_prob &gt;= best_thresh).astype(int)

    # 3. Calculate Metrics (at Best Threshold)
    print(&quot;\n2. Performance Metrics (at Optimal Threshold):&quot;)
    try:
        acc = accuracy_score(y_test_eval, y_pred_class)
        prec = precision_score(y_test_eval, y_pred_class, zero_division=0)
        rec = recall_score(y_test_eval, y_pred_class, zero_division=0)
        f1 = f1_score(y_test_eval, y_pred_class, zero_division=0)
        auc = roc_auc_score(y_test_eval, y_pred_prob)
        brier = brier_score_loss(y_test_eval, y_pred_prob)

        print(f&quot;  Accuracy:    {acc:.4f}&quot;)
        print(f&quot;  Precision:   {prec:.4f}&quot;)
        print(f&quot;  Recall:      {rec:.4f}&quot;)
        print(f&quot;  F1-Score:    {f1:.4f}&quot;)
        print(f&quot;  AUC-ROC:     {auc:.4f}&quot;)
        print(f&quot;  Brier Score: {brier:.4f}&quot;)

        print(f&quot;\nClassification Report:\n{classification_report(y_test_eval, y_pred_class, target_names=[&#x27;No Bloom&#x27;, &#x27;Bloom&#x27;])}&quot;)

    except Exception as e:
        print(f&quot;Metric calculation error: {e}&quot;)

    # 4. Publication-Quality Plots
    # ----------------------------
    # Apply Academic Style
    plt.rcdefaults()
    params = {&#x27;font.family&#x27;: &#x27;serif&#x27;, &#x27;figure.figsize&#x27;: (8, 6), &#x27;figure.dpi&#x27;: 300,
              &#x27;axes.labelsize&#x27;: 14, &#x27;axes.titlesize&#x27;: 16, &#x27;xtick.labelsize&#x27;: 12, &#x27;ytick.labelsize&#x27;: 12}
    plt.rcParams.update(params)

    # --- Plot A: Confusion Matrix ---
    try:
        cm = confusion_matrix(y_test_eval, y_pred_class)
        plt.figure()
        sns.heatmap(cm, annot=True, fmt=&#x27;d&#x27;, cmap=&#x27;Blues&#x27;, cbar=False,
                    xticklabels=[&#x27;Pred: No Bloom&#x27;, &#x27;Pred: Bloom&#x27;],
                    yticklabels=[&#x27;Actual: No Bloom&#x27;, &#x27;Actual: Bloom&#x27;],
                    annot_kws={&quot;size&quot;: 14})
        plt.title(f&#x27;Confusion Matrix ({model_name})\nThreshold: {best_thresh:.2f}&#x27;)
        plt.tight_layout()

        # SAVE FIGURE
        save_path = f&quot;output/confusion_matrix_{model_name}.png&quot;
        plt.savefig(save_path, bbox_inches=&#x27;tight&#x27;)
        print(f&quot;Saved: {save_path}&quot;)
        plt.show()
    except Exception as e:
        print(f&quot;CM Plot Error: {e}&quot;)

    # --- Plot B: Time Series Predictions ---
    print(f&quot;\n3. Plotting Time Series Predictions...&quot;)
    start_idx = seq_len + forecast_horizon - 1
    num_preds = len(y_pred_prob)

    if start_idx + num_preds &lt;= len(test_df_unscaled):
        dates = test_df_unscaled.index[start_idx : start_idx + num_preds]

        plt.figure(figsize=(12, 6))
        # Plot Actuals (Black Line)
        plt.plot(dates, y_test_eval[:num_preds], label=&#x27;Actual Bloom&#x27;, color=&#x27;black&#x27;, alpha=0.6, linewidth=1.5)
        # Plot Probabilities (Red Line)
        plt.plot(dates, y_pred_prob[:num_preds], label=&#x27;Predicted Probability&#x27;, color=&#x27;#d62728&#x27;, alpha=0.8, linewidth=1.5)
        # Threshold Line
        plt.axhline(best_thresh, color=&#x27;gray&#x27;, linestyle=&#x27;--&#x27;, label=f&#x27;Threshold ({best_thresh:.2f})&#x27;)

        plt.title(f&#x27;Bloom Predictions vs Actuals ({forecast_horizon}-Step Horizon)&#x27;)
        plt.ylabel(&#x27;Bloom Probability&#x27;)
        plt.xlabel(&#x27;Date&#x27;)
        plt.legend(frameon=True, fancybox=False, edgecolor=&#x27;black&#x27;, loc=&#x27;upper right&#x27;)
        plt.grid(True, linestyle=&#x27;:&#x27;, alpha=0.5)
        plt.tight_layout()

        # SAVE FIGURE
        save_path = f&quot;output/timeseries_preds_{model_name}.png&quot;
        plt.savefig(save_path, bbox_inches=&#x27;tight&#x27;)
        print(f&quot;Saved: {save_path}&quot;)
        plt.show()
    else:
        print(&quot;Warning: Could not align dates for plotting.&quot;)

    # 5. Bloom Onset Analysis (Custom Logic)
    print(f&quot;\n4. Bloom Onset Analysis (Did we catch the start?)&quot;)
    y_test_flat = y_test_eval.flatten()
    y_pred_flat = y_pred_class.flatten()

    onsets = np.where((y_test_flat[:-1] == 0) &amp; (y_test_flat[1:] == 1))[0] + 1
    total_onsets = len(onsets)

    hits = 0
    late_hits = 0

    if total_onsets &gt; 0:
        for idx in onsets:
            # Check window: [Week Before, Week Of, Week After]
            pred_before = y_pred_flat[idx-1] if idx &gt; 0 else 0
            pred_on     = y_pred_flat[idx]
            pred_after  = y_pred_flat[idx+1] if idx+1 &lt; len(y_pred_flat) else 0

            if pred_before == 1 or pred_on == 1:
                hits += 1 # Success (Early or On Time)
            elif pred_after == 1:
                late_hits += 1 # Late by 1 week

        print(f&quot;  Total Onsets: {total_onsets}&quot;)
        print(f&quot;  Caught Early/On-Time: {hits} ({hits/total_onsets:.1%})&quot;)
        print(f&quot;  Caught 1-Week Late:   {late_hits}&quot;)
        print(f&quot;  Missed Completely:    {total_onsets - hits - late_hits}&quot;)
    else:
        print(&quot;  No bloom onsets found in Test Data.&quot;)

# --- Main Execution ---
if __name__ == &quot;__main__&quot;:
    if &#x27;trained_model&#x27; in locals() and &#x27;X_test&#x27; in locals():
        # Ensure data is numpy float32
        X_test = np.array(X_test, dtype=&#x27;float32&#x27;)
        y_test = np.array(y_test, dtype=&#x27;float32&#x27;)

        evaluate_model(
            model=trained_model,
            x_test=X_test,
            y_test=y_test,
            test_df_unscaled=test_df, # Must be defined in Step 4
            seq_len=SEQUENCE_LENGTH,
            forecast_horizon=FORECAST_HORIZON,
            model_name=&quot;LSTM_Baseline&quot;
        )
    else:
        print(&quot;Error: Prerequisites (trained_model, X_test) not found.&quot;)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" id="mbyyeropl_va-outputs" data-mdast-node-id="DModX9QO6qivotM7RI_oG" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>
=== Evaluating Model: LSTM_Baseline ===
1. Generating predictions...

Optimal Decision Threshold: 0.7803 (Max F1: 0.8662)

2. Performance Metrics (at Optimal Threshold):
  Accuracy:    0.9087
  Precision:   0.8395
  Recall:      0.8947
  F1-Score:    0.8662
  AUC-ROC:     0.9532
  Brier Score: 0.1238

Classification Report:
              precision    recall  f1-score   support

    No Bloom       0.95      0.92      0.93       154
       Bloom       0.84      0.89      0.87        76

    accuracy                           0.91       230
   macro avg       0.89      0.91      0.90       230
weighted avg       0.91      0.91      0.91       230

Saved: output/confusion_matrix_LSTM_Baseline.png
</span></code></pre></div></div><div data-name="safe-output-image"><img src="/build/ef8770c7c1e7a96bc41ace227ceb41d8.png" alt="&lt;Figure size 2400x1800 with 1 Axes&gt;"/></div><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>
3. Plotting Time Series Predictions...
Saved: output/timeseries_preds_LSTM_Baseline.png
</span></code></pre></div></div><div data-name="safe-output-image"><img src="/build/0e88d802c5f2b8367e0a5d7ed6061a4d.png" alt="&lt;Figure size 3600x1800 with 1 Axes&gt;"/></div><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>
4. Bloom Onset Analysis (Did we catch the start?)
  Total Onsets: 10
  Caught Early/On-Time: 6 (60.0%)
  Caught 1-Week Late:   2
  Missed Completely:    2
</span></code></pre></div></div></div></div><div id="tycwxz1amxpp" class="myst-jp-nb-block relative group/block"><h3 id="part-11-enkf-setup" class="relative group"><span class="heading-text">Part 11: EnKF Setup</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#part-11-enkf-setup" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p>This cell defines the EnsembleKalmanFilter class (assuming it’s not in a separate enkf.py file for simplicity here, but importing is better practice) and a function setup_enkf to initialize the filter for the test period. It calculates the initial state, noise matrices (based on the ENKF_NOISE_ESTIMATION setting from Part 1), and identifies the necessary column indices.</p><p>Explanation:</p><ol start="1"><li><p>EnKF Class: Includes the EnsembleKalmanFilter class definition directly in the cell for convenience (assuming it’s not imported from enkf.py).</p></li><li><p>setup_enkf Function:</p><ul><li><p>Takes necessary configurations and dataframes as input.</p></li><li><p>Initial State/Covariance: Extracts the state vector values (x_initial) from the unscaled test_df at the correct time index (end of the first sequence’s lookback period). Calculates an initial covariance matrix P_initial based on relative uncertainty assumptions (these ratios are tunable).</p></li><li><p>Noise Matrices (R, Q): Calculates the observation noise R and process noise Q based on the noise_estimation_method.</p></li><li><p>If ‘manual’, it uses MANUAL_R_DIAG and MANUAL_Q_DIAG (defined in Part 1).</p></li><li><p>If ‘basic_stats’, it calculates variances based on the mean (for R) and standard deviation of weekly changes (for Q) from the unscaled train_df, scaled by tunable percentage factors. Includes fallback logic for constant columns.</p></li><li><p>State Indices: Finds the numerical index corresponding to each ENKF_STATE_VARS within the final_feature_columns_used list (which defines the order in the scaled data and LSTM input). This is crucial for correctly updating the LSTM input later.</p></li><li><p>Initialization: Creates an instance of the EnsembleKalmanFilter class using the calculated x_initial, P_initial, and other parameters. Stores the calculated R and Q matrices as attributes of the instance for easy access in the next step.</p></li><li><p>Returns: The initialized enkf object and the list of enkf_state_indices.</p></li></ul></li><li><p>Main Execution:</p><ul><li><p>Checks for prerequisite variables from previous steps.</p></li><li><p>Calls setup_enkf with the appropriate arguments from the configuration (Part 1) and dataframes (Parts 4, 5).</p></li><li><p>Stores the returned enkf_instance and enkf_indices for use in the forecasting loop (Part 12).</p></li></ul></li></ol><p>After running this cell, the enkf_instance is ready, configured with initial conditions and noise characteristics appropriate for the start of the test period. The enkf_indices list tells us which columns in the LSTM input sequence correspond to the variables being updated by the EnKF.</p></div><div id="z4exqf8cmclw" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div id="z4exqf8cmclw-code" class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">class EnsembleKalmanFilter:
    &quot;&quot;&quot;
    Stochastic Ensemble Kalman Filter (Burgers et al., 1998).
    Includes observation perturbation to maintain ensemble variance.
    &quot;&quot;&quot;
    def __init__(self, x_init, P_init, dim_z, N):
        &quot;&quot;&quot;
        Args:
            x_init: Initial state mean (dim_x,)
            P_init: Initial state covariance (dim_x, dim_x)
            dim_z:  Dimension of observations
            N:      Number of ensemble members
        &quot;&quot;&quot;
        self.dim_x = len(x_init)
        self.dim_z = dim_z
        self.N = N
        self.x = None # Current state mean
        self.P = None # Current state covariance

        # Initialize Ensemble
        # We enforce symmetry and positive-definiteness on P_init
        P_init = (P_init + P_init.T) / 2
        self.ensemble = self._multivariate_normal(x_init, P_init, N)

        # Calculate initial stats
        self.x = np.mean(self.ensemble, axis=0)
        self.P = np.cov(self.ensemble.T)

        print(f&quot;EnKF Initialized. Members: {N}, State Dim: {self.dim_x}&quot;)

    def predict(self, F_func, Q, dt=1):
        &quot;&quot;&quot;
        Propagate state forward: x = F(x) + Noise
        &quot;&quot;&quot;
        # 1. Apply Model Dynamics (The &quot;Physics&quot; or &quot;AI&quot;)
        # We apply F_func to every member of the ensemble
        self.ensemble = np.apply_along_axis(F_func, 1, self.ensemble)

        # 2. Add Process Noise (System Error)
        if Q is not None:
            Q = (Q + Q.T) / 2
            noise = self._multivariate_normal(np.zeros(self.dim_x), Q, self.N)
            self.ensemble += noise

        # Update stats
        self.x = np.mean(self.ensemble, axis=0)
        self.P = np.cov(self.ensemble.T)

    def update(self, z, R, H=None):
        &quot;&quot;&quot;
        Assimilate observation: x = x + K(z - Hx)
        &quot;&quot;&quot;
        if H is None: H = np.eye(self.dim_z, self.dim_x)

        # 1. Project Ensemble to Observation Space
        # Shape: (N, dim_z)
        Hx = self.ensemble @ H.T

        # 2. Perturb Observations (CRITICAL for Stochastic EnKF)
        # We treat the observation as a random variable, not a single truth
        R = (R + R.T) / 2
        obs_noise = self._multivariate_normal(np.zeros(self.dim_z), R, self.N)
        z_perturbed = z + obs_noise

        # 3. Calculate Innovation (Residual)
        # Difference between &quot;noisy measurement&quot; and &quot;predicted measurement&quot;
        D = z_perturbed - Hx

        # 4. Calculate Kalman Gain (K)
        # P_zz = H P H&#x27; + R
        # K = P H&#x27; (P_zz)^-1
        P_prior = np.cov(self.ensemble.T)
        P_zz = H @ P_prior @ H.T + R

        # Use Pseudo-Inverse for stability
        K = P_prior @ H.T @ np.linalg.pinv(P_zz)

        # 5. Update Ensemble
        # x_new = x_old + K * Innovation
        self.ensemble = self.ensemble + (D @ K.T)

        # Update stats
        self.x = np.mean(self.ensemble, axis=0)
        self.P = np.cov(self.ensemble.T)

        return self.x, self.P

    def _multivariate_normal(self, mean, cov, size):
        &quot;&quot;&quot;Helper to sample safely, adding jitter if matrix is singular.&quot;&quot;&quot;
        try:
            return np.random.multivariate_normal(mean, cov, size)
        except np.linalg.LinAlgError:
            # Add small &quot;jitter&quot; to diagonal to fix numerical instability
            print(&quot;Warning: Matrix not positive definite. Adding jitter.&quot;)
            epsilon = 1e-6 * np.eye(len(mean))
            return np.random.multivariate_normal(mean, cov + epsilon, size)


# --- 2. Setup Function (Robust with Manual Overrides) ---
def setup_enkf(enkf_state_vars, n_enkf, train_df, test_df_unscaled,
               seq_len, feature_columns, manual_r_diag=None, manual_q_diag=None):
    &quot;&quot;&quot;
    Configures the EnKF.
    Allows manual override of Q (Process) and R (Observation) noise diagonals.
    &quot;&quot;&quot;
    print(&quot;\n--- Setting up EnKF ---&quot;)

    # Validation
    if not all(col in test_df_unscaled.columns for col in enkf_state_vars):
        print(f&quot;Error: Missing state variables in test data.&quot;)
        return None, None, None, None

    dim_x = len(enkf_state_vars)

    # A. Initial State (x0)
    start_idx = seq_len - 1
    x_init = test_df_unscaled[enkf_state_vars].iloc[start_idx].values.astype(float)
    print(f&quot;Initial State Vector (t=0): {x_init}&quot;)

    # B. Initial Covariance (P0)
    P_init = np.diag((np.abs(x_init) * 0.30) ** 2) + 1e-6 * np.eye(dim_x)

    # C. Noise Matrices (Q and R)

    # --- R MATRIX (Observation Noise) ---
    if manual_r_diag is not None:
        if len(manual_r_diag) != dim_x:
            print(f&quot;Error: Manual R diag length ({len(manual_r_diag)}) does not match state vars ({dim_x})&quot;)
            return None, None, None, None
        print(&quot;-&gt; Using MANUAL R Matrix values.&quot;)
        R_matrix = np.diag(manual_r_diag)
    else:
        print(&quot;-&gt; Calculating Automatic R Matrix (1% of mean).&quot;)
        r_diags = []
        for col in enkf_state_vars:
            mean_val = train_df[col].mean()
            r_diags.append((abs(mean_val) * 0.01) ** 2)
        R_matrix = np.diag(r_diags)

    # --- Q MATRIX (Process Noise) ---
    if manual_q_diag is not None:
        if len(manual_q_diag) != dim_x:
            print(f&quot;Error: Manual Q diag length ({len(manual_q_diag)}) does not match state vars ({dim_x})&quot;)
            return None, None, None, None
        print(&quot;-&gt; Using MANUAL Q Matrix values.&quot;)
        Q_matrix = np.diag(manual_q_diag)
    else:
        print(&quot;-&gt; Calculating Automatic Q Matrix (based on volatility).&quot;)
        q_diags = []
        for col in enkf_state_vars:
            volatility = train_df[col].diff().std()
            if pd.isna(volatility) or volatility == 0: volatility = 1e-3
            q_diags.append((volatility * 2.0) ** 2)
        Q_matrix = np.diag(q_diags)

    print(&quot;Noise Matrices Finalized:&quot;)
    print(f&quot;  R (Obs Noise) diag: {np.diag(R_matrix)}&quot;)
    print(f&quot;  Q (Proc Noise) diag: {np.diag(Q_matrix)}&quot;)

    # D. Feature Indices
    try:
        enkf_indices = [feature_columns.index(var) for var in enkf_state_vars]
    except ValueError as e:
        print(f&quot;Error mapping variables to features: {e}&quot;)
        return None, None, None, None

    # E. Create Instance
    enkf = EnsembleKalmanFilter(x_init, P_init, dim_z=dim_x, N=n_enkf)
    enkf.R = R_matrix
    enkf.Q = Q_matrix

    return enkf, enkf_indices, R_matrix, Q_matrix

# --- 3. Execute ---
if __name__ == &quot;__main__&quot;:
    try:
        # Configuration check
        REQUIRED = [&#x27;ENKF_STATE_VARS&#x27;, &#x27;N_ENKF&#x27;, &#x27;train_df&#x27;, &#x27;test_df&#x27;,
                    &#x27;SEQUENCE_LENGTH&#x27;, &#x27;final_feature_columns_used&#x27;]

        for var in REQUIRED:
            if var not in locals(): raise NameError(f&quot;{var} is missing.&quot;)


        if (ENKF_NOISE_ESTIMATION == &#x27;manual&#x27;):
          MANUAL_Q = MANUAL_Q_DIAG
          MANUAL_R = MANUAL_R_DIAG
        else:
          MANUAL_Q = None
          MANUAL_R = None

        enkf_instance, enkf_indices, R_matrix_enkf, Q_matrix_enkf = setup_enkf(
            enkf_state_vars=ENKF_STATE_VARS,
            n_enkf=N_ENKF,
            train_df=train_df,
            test_df_unscaled=test_df,
            seq_len=SEQUENCE_LENGTH,
            feature_columns=final_feature_columns_used,
            manual_r_diag=MANUAL_R,
            manual_q_diag=MANUAL_Q
        )

        if enkf_instance:
            print(&quot;\nEnKF Setup Complete.&quot;)

    except Exception as e:
        print(f&quot;EnKF Setup Failed: {e}&quot;)

if &#x27;Q_matrix_enkf&#x27; in locals() and &#x27;R_matrix_enkf&#x27; in locals() and &#x27;ENKF_STATE_VARS&#x27; in locals():

    print(&quot;\nGenerating publication-quality heatmaps for Q and R matrices...&quot;)

    # --- STYLE SETTINGS (Academic Standard) ---
    plt.rcdefaults()
    params = {
        &#x27;font.family&#x27;: &#x27;serif&#x27;,
        &#x27;font.serif&#x27;: [&#x27;Times New Roman&#x27;, &#x27;Times&#x27;, &#x27;DejaVu Serif&#x27;],
        &#x27;axes.titlesize&#x27;: 18,
        &#x27;axes.labelsize&#x27;: 14,
        &#x27;xtick.labelsize&#x27;: 12,
        &#x27;ytick.labelsize&#x27;: 12,
        &#x27;figure.dpi&#x27;: 300
    }
    plt.rcParams.update(params)

    # Helper function to plot and save
    def plot_covariance_matrix(matrix, title, filename, labels, cmap=&#x27;Blues&#x27;):
        plt.figure(figsize=(8, 7))

        sns.heatmap(matrix, annot=True, fmt=&#x27;.2e&#x27;, cmap=cmap,
                    xticklabels=labels, yticklabels=labels,
                    square=True, cbar_kws={&#x27;label&#x27;: &#x27;Variance / Covariance&#x27;},
                    linewidths=1, linecolor=&#x27;black&#x27;)

        plt.title(title, pad=20)
        plt.xticks(rotation=45, ha=&#x27;right&#x27;)
        plt.yticks(rotation=0)
        plt.tight_layout()

        save_path = f&quot;output/{filename}&quot;
        plt.savefig(save_path, bbox_inches=&#x27;tight&#x27;)
        print(f&quot;Saved: {save_path}&quot;)
        plt.show()

    # --- 1. Plot Q Matrix (Process Noise) ---
    plot_covariance_matrix(
        Q_matrix_enkf,
        title=&quot;Process Noise Covariance ($Q$)&quot;,
        filename=&quot;matrix_Q_heatmap.png&quot;,
        labels=ENKF_STATE_VARS,
        cmap=&quot;Reds&quot;
    )

    # --- 2. Plot R Matrix (Observation Noise) ---
    plot_covariance_matrix(
        R_matrix_enkf,
        title=&quot;Observation Noise Covariance ($R$)&quot;,
        filename=&quot;matrix_R_heatmap.png&quot;,
        labels=ENKF_STATE_VARS,
        cmap=&quot;Blues&quot;
    )

else:
    print(&quot;Error: Q and R matrices (or state vars) not found. Run Step 11 first.&quot;)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" id="z4exqf8cmclw-outputs" data-mdast-node-id="euH3eMv2Sa7xtY1VWS-us" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>
--- Setting up EnKF ---
Initial State Vector (t=0): [ 2.98000000e+03  1.84000000e+00  6.50000000e-01  0.00000000e+00
  7.39418622e-01 -1.05741789e+01]
-&gt; Using MANUAL R Matrix values.
-&gt; Using MANUAL Q Matrix values.
Noise Matrices Finalized:
  R (Obs Noise) diag: [1.00e+00 4.00e-06 4.00e-08 6.25e-02 6.25e-02 1.00e-06]
  Q (Proc Noise) diag: [9.00e+04 3.60e-01 3.60e-03 2.25e+02 2.25e+02 1.00e-02]
EnKF Initialized. Members: 50, State Dim: 6

EnKF Setup Complete.

Generating publication-quality heatmaps for Q and R matrices...
Saved: output/matrix_Q_heatmap.png
</span></code></pre></div></div><div data-name="safe-output-image"><img src="/build/297a56677509f12f09f292ce01693c36.png" alt="&lt;Figure size 2400x2100 with 2 Axes&gt;"/></div><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>Saved: output/matrix_R_heatmap.png
</span></code></pre></div></div><div data-name="safe-output-image"><img src="/build/f383b5ede29d94c371eb8d0ec2739f41.png" alt="&lt;Figure size 2400x2100 with 2 Axes&gt;"/></div></div></div><div id="nb41vbfwmjoe" class="myst-jp-nb-block relative group/block"><h3 id="part-12-enkf-mc-dropout-forecasting-loop" class="relative group"><span class="heading-text">Part 12: EnKF + MC Dropout Forecasting Loop</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#part-12-enkf-mc-dropout-forecasting-loop" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p>This section implements the core forecasting engine, which synthesizes the data assimilation capabilities of the Ensemble Kalman Filter (EnKF) with the predictive power and uncertainty estimation of the LSTM. The loop iterates through the test period, sequentially updating environmental states and generating probabilistic forecasts.</p><h4 id="functionality-run-enkf-mc-forecast" class="relative group"><span class="heading-text">Functionality: <code>run_enkf_mc_forecast</code></span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#functionality-run-enkf-mc-forecast" title="Link to this Section" aria-label="Link to this Section">¶</a></h4><p>The forecasting simulation is encapsulated in the <code>run_enkf_mc_forecast</code> function, which manages the interaction between real-time observations and the deep learning model.</p><ol start="1"><li><p><strong>State Assimilation (EnKF):</strong> For each time step, the function retrieves the current observations from the unscaled test data. If <code>perform_enkf</code> is enabled, the EnKF instance performs a prediction and update step. This process corrects the hydrological state variables (such as river discharge and nutrient levels) by balancing the model’s internal state with actual observed data, effectively reducing the “drift” often found in long-term time-series forecasting.</p></li><li><p><strong>Input Sequence Modification:</strong> Once the EnKF provides an updated state, these values are passed through the fitted scaler. The function then takes the standard input sequence from <code>X_test</code> and replaces the state variables in the most recent time step with these assimilated values. This ensures the LSTM makes its next prediction based on the most accurate, ground-truth-adjusted data available.</p></li><li><p><strong>Probabilistic Prediction (MC Dropout):</strong></p></li></ol><ul><li><p><strong>Uncertainty Quantification:</strong> If <code>perform_mc</code> is active, the model performs multiple forward passes (defined by <code>n_mc_samples</code>) for the same input sequence. By keeping dropout layers active during inference (<code>training=True</code>), each pass produces a slightly different prediction, allowing us to capture the model’s predictive uncertainty.</p></li><li><p><strong>Standard Inference:</strong> If MC Dropout is disabled, the model performs a standard <code>predict()</code> call, providing a single deterministic output.</p></li></ul><h4 id="execution-and-robustness" class="relative group"><span class="heading-text">Execution and Robustness</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#execution-and-robustness" title="Link to this Section" aria-label="Link to this Section">¶</a></h4><p>The execution block verifies that all prerequisites—including the trained model, initialized EnKF, and noise matrices—are present before starting the loop.</p><ul><li><p><strong>Error Handling:</strong> The loop is wrapped in try-except blocks to manage potential issues during matrix inversions in the EnKF or scaling errors. If a critical failure occurs, the loop captures the error and returns the results generated up to that point.</p></li><li><p><strong>Result Trimming:</strong> To maintain data integrity, both the prediction results and the corresponding ground truth labels (<code>y_test</code>) are trimmed to the exact number of successfully completed iterations.</p></li></ul><p>Upon completion, the results are stored in <code>mc_predictions_results</code>. These raw probability outputs (either as single points or ensembles) serve as the foundation for the performance metrics and uncertainty analysis performed in <strong>Part 13: EnKF + MC Dropout Evaluation</strong>.</p></div><div id="frayqmepmkug" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div id="frayqmepmkug-code" class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">def run_enkf_mc_forecast(model, enkf_instance, enkf_indices, R_matrix, Q_matrix,
                         x_test, y_test, test_df_unscaled, scaler,
                         seq_len, perform_enkf=True, perform_mc=True, n_mc_samples=50):
    &quot;&quot;&quot;
    Optimized Forecasting Loop with Vectorized MC Dropout.
    &quot;&quot;&quot;
    # 1. Validation &amp; Setup
    if model is None or x_test is None: return None, None
    print(f&quot;\n--- Starting Forecast (EnKF={perform_enkf}, MC={perform_mc}) ---&quot;)

    num_steps = len(y_test)
    n_features = x_test.shape[2]

    # Pre-allocate output array
    # If MC is off, we still use shape (N, 1) for consistency
    samples_col = n_mc_samples if perform_mc else 1
    predictions = np.zeros((num_steps, samples_col))

    # 2. Pre-fetch Scaler Parameters (Speed Optimization)
    # Accessing scaler attributes inside the loop is slow. Do it once here.
    if perform_enkf:
        try:
            # Handle StandardScaler vs RobustScaler
            if hasattr(scaler, &#x27;mean_&#x27;):
                mu, sigma = scaler.mean_, scaler.scale_
            elif hasattr(scaler, &#x27;center_&#x27;):
                mu, sigma = scaler.center_, scaler.scale_
            else:
                print(&quot;Error: Scaler not fitted.&quot;); return None, None

            # Keep only the stats for the EnKF variables to avoid indexing in loop
            enkf_mu = mu[enkf_indices]
            enkf_sigma = sigma[enkf_indices]
        except Exception as e:
            print(f&quot;Scaler Error: {e}&quot;); return None, None

    # 3. Main Loop
    # EnKF must be sequential (step t depends on step t-1)
    for i in tqdm(range(num_steps), desc=&quot;Forecasting&quot;):

        # A. Current Timestamp Index
        # The test_df index corresponding to the *end* of the current sequence
        df_idx = i + seq_len - 1
        if df_idx &gt;= len(test_df_unscaled): break

        # B. EnKF Update Step
        updated_state = None
        if perform_enkf:
            # Get Observation (z)
            z = test_df_unscaled[ENKF_STATE_VARS].iloc[df_idx].values

            # 1. Predict (Move Ensemble Forward)
            # Note: In a full Hybrid EnKF, the &#x27;F&#x27; function would run the LSTM here.
            # Here we use the persistence assumption for the state transition.
            enkf_instance.predict(lambda x: x, Q=Q_matrix)

            # 2. Update (Correct with Observation)
            updated_state, _ = enkf_instance.update(z, R_matrix)

        # C. Update LSTM Input
        # We copy the sequence so we don&#x27;t overwrite the original data
        current_seq = x_test[i].copy() # Shape: (Seq_Len, Features)

        if perform_enkf and updated_state is not None:
            # Scale the updated state (Vectorized Math)
            # (Raw - Mean) / Scale
            scaled_state = (updated_state - enkf_mu) / enkf_sigma

            # Inject into the LAST time step of the sequence
            # We replace only the columns corresponding to EnKF variables
            current_seq[-1, enkf_indices] = scaled_state

        # D. LSTM Prediction (Vectorized MC Dropout)
        # Prepare Tensor
        # Shape: (1, Seq_Len, Features)
        input_tensor = tf.convert_to_tensor([current_seq], dtype=tf.float32)

        if perform_mc:
            # OPTIMIZATION: Tile the input N times to create a batch
            # New Shape: (n_mc_samples, Seq_Len, Features)
            batch = tf.tile(input_tensor, [n_mc_samples, 1, 1])

            # ONE single call to the model for all samples
            # training=True enables Dropout
            preds = model(batch, training=True)

            # Store results (flatten to 1D array of probabilities)
            predictions[i, :] = preds[:, 0].numpy()
        else:
            # Standard Inference (No Dropout)
            pred = model(input_tensor, training=False)
            predictions[i, 0] = pred[0, 0].numpy()

    return predictions, y_test[:len(predictions)]

# --- Execute ---
if __name__ == &quot;__main__&quot;:
    if &#x27;trained_model&#x27; in locals() and &#x27;enkf_instance&#x27; in locals():
        if PERFORM_ENKF and enkf_instance is None:
            print(&quot;Warning: PERFORM_ENKF is True, but &#x27;enkf_instance&#x27; not found.&quot;)
            print(&quot;Running in pure LSTM mode (EnKF disabled for this run).&quot;)
            # Temporarily disable for this function call to prevent crash
            current_perform_enkf = False
        else:
            current_perform_enkf = PERFORM_ENKF

        mc_preds, y_test_trimmed = run_enkf_mc_forecast(
            model=trained_model,
            enkf_instance=enkf_instance,
            enkf_indices=enkf_indices,
            R_matrix=R_matrix_enkf,
            Q_matrix=Q_matrix_enkf,
            x_test=X_test,
            y_test=y_test,
            test_df_unscaled=test_df,
            scaler=scaler, # Ensure this matches your scaler variable name
            seq_len=SEQUENCE_LENGTH,
            perform_enkf=True,         # Set to False to test Baseline again
            perform_mc=True,           # Set to True for Uncertainty
            n_mc_samples=50            # 50 is a good balance for speed/accuracy
        )

        if mc_preds is not None:
            print(f&quot;✅ Forecast Complete. Prediction Shape: {mc_preds.shape}&quot;)
    else:
        print(&quot;Error: Prerequisites (Part 9 Model or Part 11 EnKF) missing.&quot;)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" id="frayqmepmkug-outputs" data-mdast-node-id="0pos0XVd5svvsvO0XqzR3" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>
--- Starting Forecast (EnKF=True, MC=True) ---
</span></code></pre></div></div><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>Forecasting: 100%|██████████| 230/230 [00:05&lt;00:00, 42.55it/s]</span></code></pre></div></div><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>✅ Forecast Complete. Prediction Shape: (230, 50)
</span></code></pre></div></div><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>
</span></code></pre></div></div></div></div><div id="xd0ko6o6mwsw" class="myst-jp-nb-block relative group/block"><h3 id="part-13-enkf-mc-dropout-evaluation" class="relative group"><span class="heading-text">Part 13: EnKF + MC Dropout Evaluation</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#part-13-enkf-mc-dropout-evaluation" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p>This final analytical section focuses on quantifying the performance of the integrated forecasting system. By evaluating the predictions generated in Part 12, we assess how the combination of Ensemble Kalman Filter (EnKF) state updates and Monte Carlo (MC) Dropout uncertainty estimation improves the model’s ability to predict red tide events.</p><h4 id="core-evaluation-components" class="relative group"><span class="heading-text">Core Evaluation Components</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#core-evaluation-components" title="Link to this Section" aria-label="Link to this Section">¶</a></h4><p>The evaluation framework is designed to move beyond simple point-accuracy and instead provide a comprehensive view of model reliability and confidence.</p><ol start="1"><li><p><strong>Ensemble Statistics and Uncertainty:</strong>
For sessions where <code>PERFORM_MC_DROPOUT</code> was enabled, the evaluation calculates the mean and standard deviation across all MC samples for each time step. The mean serves as the primary probabilistic forecast, while the standard deviation provides a quantifiable measure of model uncertainty. This allows us to identify periods where the model is highly confident versus periods where environmental volatility leads to wider prediction intervals.</p></li><li><p><strong>Binary Classification Performance:</strong>
Since the ultimate goal is to predict bloom occurrences, the probabilistic outputs are converted into binary classifications using an optimized threshold. Key performance metrics include:</p></li></ol><ul><li><p><strong>Precision and Recall:</strong> Determining the model’s ability to correctly identify blooms while minimizing false alarms.</p></li><li><p><strong>F1-Score:</strong> Providing a balanced metric that accounts for the inherent class imbalance in red tide data.</p></li><li><p><strong>Confusion Matrix:</strong> Visualizing the distribution of True Positives, True Negatives, False Positives, and False Negatives.</p></li></ul><ol start="3"><li><p><strong>Probabilistic Analysis:</strong>
The evaluation includes the calculation of the <strong>Brier Score</strong> to measure the accuracy of the probability forecasts and the <strong>Area Under the Precision-Recall Curve (AUPRC)</strong>. These metrics are particularly useful for evaluating models where the event of interest (the bloom) is relatively rare compared to non-bloom periods.</p></li></ol><h4 id="visualizing-the-results" class="relative group"><span class="heading-text">Visualizing the Results</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#visualizing-the-results" title="Link to this Section" aria-label="Link to this Section">¶</a></h4><p>The section generates a series of diagnostic plots to illustrate the system’s effectiveness:</p><ul><li><p><strong>Forecast vs. Actuals:</strong> A time-series plot comparing the predicted probabilities (with uncertainty bands) against the actual observed bloom markers.</p></li><li><p><strong>EnKF Impact Analysis:</strong> Comparative visualizations showing how the state assimilation influenced the trajectory of the predictions compared to a baseline model without EnKF updates.</p></li></ul><p>By the end of this evaluation, we can determine the specific “value-add” of the EnKF + MC Dropout approach, providing a clear picture of how well the hybrid system handles the complexities of ecological forecasting.</p></div><div id="bqhjyg-mmy-5" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div id="bqhjyg-mmy-5-code" class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">from sklearn.calibration import calibration_curve
def evaluate_mc_predictions(mc_preds, y_test, test_df_unscaled, seq_len,
                            forecast_horizon, model_name=&quot;EnKF_MC&quot;,
                            output_dir=&quot;output&quot;, manual_threshold=None):
    &quot;&quot;&quot;
    Evaluates probabilistic predictions (Mean, Uncertainty, Calibration).

    Args:
        mc_preds: Array of shape (n_samples, n_mc_iterations) or (n_samples, 1)
        y_test: True labels (n_samples,)
        test_df_unscaled: DataFrame with timestamps
        seq_len: Input sequence length (for aligning dates)
        forecast_horizon: Forecast horizon (for aligning dates)
        model_name: String for labeling files/plots
        output_dir: Folder to save results
        manual_threshold: Float (0.0-1.0) to force a specific decision boundary.
                          If None, calculates optimal F1 threshold.
    &quot;&quot;&quot;
    print(f&quot;\n=== Evaluating Uncertainty: {model_name} ===&quot;)

    # 1. Setup Output Directory
    os.makedirs(output_dir, exist_ok=True)

    # Validation
    if mc_preds is None or y_test is None: return None

    # 2. Statistics Calculation
    # -------------------------------------------------------
    # Mean: The &quot;Best Guess&quot;
    y_mean = np.mean(mc_preds, axis=1)

    # Std Dev: The &quot;Disagreement&quot; (Model Uncertainty)
    y_std = np.std(mc_preds, axis=1)

    # 95% Confidence Interval
    ci_lower = np.percentile(mc_preds, 2.5, axis=1)
    ci_upper = np.percentile(mc_preds, 97.5, axis=1)

    # Entropy (Uncertainty)
    epsilon = 1e-10
    y_mean_clipped = np.clip(y_mean, epsilon, 1-epsilon)
    entropy = -(y_mean_clipped * np.log(y_mean_clipped) + (1-y_mean_clipped) * np.log(1-y_mean_clipped))

    # 3. Threshold Logic
    # -------------------------------------------------------
    if manual_threshold is not None:
        best_thresh = manual_threshold
        print(f&quot;Using Manual Decision Threshold: {best_thresh:.4f}&quot;)
    else:
        # Calculate Optimal F1 Threshold
        precision, recall, thresholds = precision_recall_curve(y_test, y_mean)
        numerator = 2 * precision * recall
        denominator = precision + recall
        with np.errstate(divide=&#x27;ignore&#x27;, invalid=&#x27;ignore&#x27;):
            fscore = np.nan_to_num(np.divide(numerator, denominator))

        ix = np.argmax(fscore)
        # Safety check if index is out of bounds
        best_thresh = thresholds[ix] if ix &lt; len(thresholds) else 0.5
        print(f&quot;Optimal Threshold (Auto-F1): {best_thresh:.4f} (Max F1: {fscore[ix]:.4f})&quot;)

    # Apply threshold
    y_pred_class = (y_mean &gt;= best_thresh).astype(int)

    # 4. Calculate Metrics
    # -------------------------------------------------------
    try:
        metrics = {
            &#x27;accuracy&#x27;: float(accuracy_score(y_test, y_pred_class)),
            &#x27;auc&#x27;: float(roc_auc_score(y_test, y_mean)),
            &#x27;brier&#x27;: float(brier_score_loss(y_test, y_mean)),
            &#x27;precision&#x27;: float(precision_score(y_test, y_pred_class, zero_division=0)),
            &#x27;recall&#x27;: float(recall_score(y_test, y_pred_class, zero_division=0)),
            &#x27;f1_score&#x27;: float(f1_score(y_test, y_pred_class, zero_division=0)),
            &#x27;avg_entropy&#x27;: float(np.mean(entropy)),
            &#x27;threshold_used&#x27;: float(best_thresh)
        }

        print(f&quot;\nPerformance Metrics:&quot;)
        print(f&quot;  Accuracy:    {metrics[&#x27;accuracy&#x27;]:.4f}&quot;)
        print(f&quot;  AUC-ROC:     {metrics[&#x27;auc&#x27;]:.4f}&quot;)
        print(f&quot;  Recall:      {metrics[&#x27;recall&#x27;]:.4f} (Caught {metrics[&#x27;recall&#x27;]*100:.1f}% of blooms)&quot;)
        print(f&quot;  Precision:   {metrics[&#x27;precision&#x27;]:.4f}&quot;)
        print(f&quot;  Brier Score: {metrics[&#x27;brier&#x27;]:.4f}&quot;)
    except Exception as e:
        print(f&quot;Error calculating metrics: {e}&quot;)
        metrics = {}

    # 5. PLOTTING (Publication Quality)
    # -------------------------------------------------------
    # Apply Style
    plt.rcdefaults()
    params = {&#x27;font.family&#x27;: &#x27;serif&#x27;, &#x27;figure.figsize&#x27;: (10, 6), &#x27;figure.dpi&#x27;: 300}
    plt.rcParams.update(params)

    # Plot A: Forecast Time Series
    start_idx = seq_len + forecast_horizon - 1
    num_preds = len(y_mean)

    if start_idx + num_preds &lt;= len(test_df_unscaled):
        dates = test_df_unscaled.index[start_idx : start_idx + num_preds]

        plt.figure()
        # Plot Actuals
        plt.plot(dates, y_test[:num_preds], color=&#x27;black&#x27;, alpha=0.6, label=&#x27;Actual Bloom&#x27;, linewidth=1.5)
        # Plot Mean Prediction
        plt.plot(dates, y_mean[:num_preds], color=&#x27;#d62728&#x27;, label=&#x27;Ensemble Mean&#x27;, linewidth=2)
        # Plot Confidence Interval
        plt.fill_between(dates, ci_lower[:num_preds], ci_upper[:num_preds], color=&#x27;#d62728&#x27;, alpha=0.2, label=&#x27;95% CI&#x27;)
        # Plot Threshold
        plt.axhline(best_thresh, color=&#x27;gray&#x27;, linestyle=&#x27;--&#x27;, label=f&#x27;Threshold ({best_thresh:.2f})&#x27;)

        plt.title(f&#x27;Probabilistic Forecast ({model_name})&#x27;)
        plt.ylabel(&#x27;Bloom Probability&#x27;)
        plt.legend(loc=&#x27;upper right&#x27;, frameon=True)
        plt.grid(True, linestyle=&#x27;:&#x27;, alpha=0.5)
        plt.tight_layout()

        save_path = os.path.join(output_dir, f&quot;forecast_{model_name}.png&quot;)
        plt.savefig(save_path)
        print(f&quot;Saved Plot: {save_path}&quot;)
        plt.show()

    # Plot B: Calibration Curve
    try:
        prob_true, prob_pred = calibration_curve(y_test, y_mean, n_bins=10)
        plt.figure(figsize=(6, 6))
        plt.plot([0, 1], [0, 1], linestyle=&#x27;--&#x27;, color=&#x27;gray&#x27;, label=&#x27;Perfectly Calibrated&#x27;)
        plt.plot(prob_pred, prob_true, marker=&#x27;o&#x27;, color=&#x27;blue&#x27;, label=model_name)
        plt.title(&#x27;Calibration Curve (Reliability)&#x27;)
        plt.xlabel(&#x27;Mean Predicted Probability&#x27;)
        plt.ylabel(&#x27;Fraction of Positives (Actual)&#x27;)
        plt.legend()
        plt.grid(True, linestyle=&#x27;:&#x27;, alpha=0.5)
        plt.tight_layout()

        save_path = os.path.join(output_dir, f&quot;calibration_{model_name}.png&quot;)
        plt.savefig(save_path)
        print(f&quot;Saved Plot: {save_path}&quot;)
        plt.show()
    except Exception as e:
        print(f&quot;Skipping Calibration Plot: {e}&quot;)

    # Plot C: Entropy Distribution
    try:
        correct = (y_pred_class == y_test)
        plt.figure(figsize=(8, 5))
        sns.kdeplot(entropy[correct], fill=True, color=&#x27;green&#x27;, label=&#x27;Correct Predictions&#x27;)
        sns.kdeplot(entropy[~correct], fill=True, color=&#x27;red&#x27;, label=&#x27;Wrong Predictions&#x27;)
        plt.title(&#x27;Predictive Entropy (Uncertainty Distribution)&#x27;)
        plt.xlabel(&#x27;Entropy (Higher = More Uncertain)&#x27;)
        plt.legend()
        plt.tight_layout()

        save_path = os.path.join(output_dir, f&quot;entropy_{model_name}.png&quot;)
        plt.savefig(save_path)
        print(f&quot;Saved Plot: {save_path}&quot;)
        plt.show()
    except Exception as e:
        print(f&quot;Skipping Entropy Plot: {e}&quot;)

    # 6. Return structured data
    return {
        &#x27;metrics&#x27;: metrics,  # Clean dictionary for JSON
        &#x27;arrays&#x27;: {          # Arrays for CSV/Debug
            &#x27;y_mean&#x27;: y_mean,
            &#x27;y_std&#x27;: y_std,
            &#x27;entropy&#x27;: entropy,
            &#x27;y_actual&#x27;: y_test,
            &#x27;ci_lower&#x27;: ci_lower,
            &#x27;ci_upper&#x27;: ci_upper
        }
    }

# --- Execute ---
if __name__ == &quot;__main__&quot;:
    if &#x27;mc_preds&#x27; in locals() and mc_preds is not None:

        # 1. Run Evaluation (Try manual_threshold=0.3 if you want to fix lag)
        results = evaluate_mc_predictions(
            mc_preds=mc_preds,
            y_test=y_test_trimmed,
            test_df_unscaled=test_df,
            seq_len=SEQUENCE_LENGTH,
            forecast_horizon=FORECAST_HORIZON,
            model_name=&quot;EnKF_LSTM_MC&quot;,
            output_dir=&quot;output&quot;,
            manual_threshold=None  # &lt;--- Change to 0.3 to reduce lag!
        )

        # 2. Save Metrics to JSON
        if results:
            json_path = os.path.join(&quot;output&quot;, &quot;metrics_enkf_mc.json&quot;)
            try:
                with open(json_path, &#x27;w&#x27;) as f:
                    json.dump(results[&#x27;metrics&#x27;], f, indent=4)
                print(f&quot;✅ Saved Metrics to: {json_path}&quot;)
            except Exception as e:
                print(f&quot;Error saving JSON: {e}&quot;)

            # 3. Save Raw Data to CSV
            csv_path = os.path.join(&quot;output&quot;, &quot;predictions_enkf_mc.csv&quot;)
            try:
                df_results = pd.DataFrame(results[&#x27;arrays&#x27;])
                # Add timestamp index if possible
                start_idx = SEQUENCE_LENGTH + FORECAST_HORIZON - 1
                if start_idx + len(df_results) &lt;= len(test_df):
                    df_results.index = test_df.index[start_idx : start_idx + len(df_results)]

                df_results.to_csv(csv_path)
                print(f&quot;✅ Saved Raw Data to: {csv_path}&quot;)
            except Exception as e:
                print(f&quot;Error saving CSV: {e}&quot;)

    else:
        print(&quot;Error: &#x27;mc_preds&#x27; not found. Run Step 12 first.&quot;)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" id="bqhjyg-mmy-5-outputs" data-mdast-node-id="fofbs27-3eF5OvNHeJmzz" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>
=== Evaluating Uncertainty: EnKF_LSTM_MC ===
Optimal Threshold (Auto-F1): 0.8037 (Max F1: 0.8649)

Performance Metrics:
  Accuracy:    0.9130
  AUC-ROC:     0.9529
  Recall:      0.8421 (Caught 84.2% of blooms)
  Precision:   0.8889
  Brier Score: 0.1236
Saved Plot: output/forecast_EnKF_LSTM_MC.png
</span></code></pre></div></div><div data-name="safe-output-image"><img src="/build/ea44aa14eb6f9f5bc8e9257ee43bc189.png" alt="&lt;Figure size 3000x1800 with 1 Axes&gt;"/></div><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>Saved Plot: output/calibration_EnKF_LSTM_MC.png
</span></code></pre></div></div><div data-name="safe-output-image"><img src="/build/b775b9326344335f9f26b7411f2a23aa.png" alt="&lt;Figure size 1800x1800 with 1 Axes&gt;"/></div><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>Saved Plot: output/entropy_EnKF_LSTM_MC.png
</span></code></pre></div></div><div data-name="safe-output-image"><img src="/build/d016f356bf7188afa2fb7e4c31ac2c0c.png" alt="&lt;Figure size 2400x1500 with 1 Axes&gt;"/></div><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>✅ Saved Metrics to: output/metrics_enkf_mc.json
✅ Saved Raw Data to: output/predictions_enkf_mc.csv
</span></code></pre></div></div></div></div><div id="zix8y878hu4o" class="myst-jp-nb-block relative group/block"><h3 id="part-14-shap-analysis" class="relative group"><span class="heading-text">Part 14: SHAP Analysis</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#part-14-shap-analysis" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p>This section utilizes <strong>SHAP (SHapley Additive exPlanations)</strong> to interpret the LSTM model’s decision-making process. While deep learning models are often viewed as “black boxes,” SHAP values provide transparency by quantifying the contribution of each environmental feature to the final prediction.</p><h4 id="core-interpretability-components" class="relative group"><span class="heading-text">Core Interpretability Components</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#core-interpretability-components" title="Link to this Section" aria-label="Link to this Section">¶</a></h4><p>The analysis focuses on local and global feature importance to understand which drivers—such as nutrient concentrations, river discharge, or lagged bloom states—are most influential in triggering a red tide forecast.</p><ol start="1"><li><p><strong>DeepExplainer for LSTM:</strong>
We employ <code>shap.DeepExplainer</code> to handle the complex, 3-dimensional temporal data characteristic of LSTMs. This approach approximates SHAP values by comparing the model’s output for specific test sequences against a “background” dataset (typically a representative subset of the training data). This reveals how specific fluctuations in environmental conditions shift the probability of a bloom.</p></li><li><p><strong>Temporal Feature Contribution:</strong>
Since the LSTM processes sequences over time, SHAP analysis allows us to visualize which time steps within the input window are most critical. We can identify whether the model is reacting to immediate spikes in river discharge or if it is recognizing long-term patterns in nutrient accumulation across the entire sequence length.</p></li><li><p><strong>Global Feature Importance:</strong>
By aggregating the absolute SHAP values across the entire test set, we generate a global ranking of features. This is vital for ecological validation, as it allows us to verify if the model’s “reasoning” aligns with known biological drivers of <em>Karenia brevis</em>, such as specific salinity levels or nitrogen-to-phosphorus ratios.</p></li></ol><h4 id="visualization-and-insights" class="relative group"><span class="heading-text">Visualization and Insights</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#visualization-and-insights" title="Link to this Section" aria-label="Link to this Section">¶</a></h4><p>The section produces high-impact visualizations to communicate these findings:</p><ul><li><p><strong>SHAP Summary Plots:</strong> A comprehensive view showing the magnitude and direction of each feature’s impact. For instance, it might reveal that high values of a specific lagged nutrient feature consistently push the model toward a “bloom” prediction.</p></li><li><p><strong>Force Plots / Waterfall Plots:</strong> Detailed breakdowns of individual prediction samples, showing how different features “pushed” the model’s output from the base value to the final predicted probability.</p></li></ul><p>By integrating SHAP analysis, we transition from simply knowing <em>what</em> the model predicted to understanding <em>why</em> it made that prediction, providing essential context for environmental managers and researchers.</p></div><div id="id-94fzvl8sjexn" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div id="id-94fzvl8sjexn-code" class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">try:
    # 1. Check &amp; Fix Data Dimensions
    # ---------------------------------------------------------
    if &#x27;X_test&#x27; not in locals() or &#x27;trained_model&#x27; not in locals():
        raise NameError(&quot;Data or Model missing. Please restart kernel and run Steps 1-9.&quot;)

    true_n_steps = X_test.shape[1]    # e.g., 12
    true_n_features = X_test.shape[2] # e.g., 26

    print(f&quot;Data Shape: {true_n_steps} steps, {true_n_features} features&quot;)

    # Fix the feature name list if it doesn&#x27;t match the data
    current_names = final_feature_columns_used if &#x27;final_feature_columns_used&#x27; in locals() else []

    if len(current_names) != true_n_features:
        print(f&quot;Mismatch: {len(current_names)} names vs {true_n_features} data features.&quot;)
        diff = true_n_features - len(current_names)
        # We assume the extra columns are usually appended at the end
        plotting_feature_names = current_names + [f&quot;Extra_Feat_{i+1}&quot; for i in range(diff)]
    else:
        plotting_feature_names = current_names

    # 2. Define Wrapper (The Fix)
    # ---------------------------------------------------------
    def model_wrapper(flat_data):
        &quot;&quot;&quot;
        Converts flat SHAP input back to 3D and queries the model
        using Direct Call (not .predict) to avoid TF Graph errors.
        &quot;&quot;&quot;
        # Convert Numpy -&gt; Tensor
        # Reshape to (Samples, Time, Features)
        reshaped = flat_data.reshape(-1, true_n_steps, true_n_features)
        tensor_input = tf.convert_to_tensor(reshaped, dtype=tf.float32)

        # Direct call (Eager Mode) - Faster and safer for SHAP
        probs = trained_model(tensor_input, training=False)

        # Return as Numpy flattened array
        return probs.numpy().flatten()

    # 3. Optimize Background (K-Means)
    # ---------------------------------------------------------
    print(&quot;Summarizing background data...&quot;)
    # Flatten train data: (Samples, Time*Feats)
    X_train_flat = X_train.reshape(X_train.shape[0], -1)

    # Summarize training data into 10 weighted points (Centroids)
    background_summary = shap.kmeans(X_train_flat, 10)

    # 4. Initialize KernelExplainer
    # ---------------------------------------------------------
    print(&quot;Initializing KernelExplainer...&quot;)
    explainer = shap.KernelExplainer(model_wrapper, background_summary)

    # 5. Calculate SHAP Values
    # ---------------------------------------------------------
    explain_size = 20  # Keep small for speed
    X_test_flat = X_test.reshape(X_test.shape[0], -1)

    # Pick random samples to explain
    idxs = np.random.choice(X_test_flat.shape[0], explain_size, replace=False)
    X_test_sample = X_test_flat[idxs]

    print(f&quot;Calculating SHAP values for {explain_size} instances...&quot;)
    # nsamples=auto usually works best, or try 500 if slow
    shap_values = explainer.shap_values(X_test_sample, nsamples=500)

    print(f&quot;SHAP calculation complete.&quot;)

    # 6. Reshape for Visualization
    # ---------------------------------------------------------
    # (Samples, Flat_Features) -&gt; (Samples, Time, Features)
    shap_values_3d = np.array(shap_values).reshape(-1, true_n_steps, true_n_features)
    X_test_sample_3d = X_test_sample.reshape(-1, true_n_steps, true_n_features)

    # 7. PLOT A: Global Feature Importance
    # ---------------------------------------------------------
    # Sum SHAP impact across all time steps
    shap_sum_over_time = np.sum(shap_values_3d, axis=1) # (Samples, Features)
    features_mean = np.mean(X_test_sample_3d, axis=1)   # (Samples, Features)

    plt.figure(figsize=(10, 8), dpi=300)
    shap.summary_plot(shap_sum_over_time, features_mean,
                      feature_names=plotting_feature_names, show=False)
    plt.title(&quot;Global Feature Importance&quot;)
    plt.tight_layout()
    plt.savefig(&quot;output/shap_summary_global.png&quot;)
    plt.show()

    # 8. PLOT B: Temporal Heatmap
    # ---------------------------------------------------------
    # Average absolute impact per time step
    temporal_importance = np.mean(np.abs(shap_values_3d), axis=0) # (Time, Feats)

    plt.figure(figsize=(12, 6), dpi=300)
    sns.heatmap(temporal_importance.T, cmap=&#x27;viridis&#x27;,
                yticklabels=plotting_feature_names,
                xticklabels=range(true_n_steps))

    plt.title(&quot;Temporal Feature Importance\n(Brighter = Higher Impact at that Lag)&quot;)
    plt.xlabel(&quot;Time Lag (Steps into Sequence)&quot;)
    plt.ylabel(&quot;Feature&quot;)
    plt.tight_layout()
    plt.savefig(&quot;output/shap_temporal_heatmap.png&quot;)
    plt.show()

    print(&quot;✅ SHAP Analysis Completed Successfully.&quot;)

except Exception as e:
    print(f&quot;❌ SHAP Failed: {e}&quot;)
    import traceback
    traceback.print_exc()</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" id="94fzvl8sjexn-outputs" data-mdast-node-id="DNqvDicCjBc0uhiRvYqpu" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>Data Shape: 12 steps, 26 features
Mismatch: 25 names vs 26 data features.
Summarizing background data...
Initializing KernelExplainer...
Calculating SHAP values for 20 instances...
</span></code></pre></div></div><div><div class="p-2.5">Loading...</div></div><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>SHAP calculation complete.
</span></code></pre></div></div><div data-name="safe-output-image"><img src="/build/2f3397b85fc3a9579412e046c615d152.png" alt="&lt;Figure size 2400x2850 with 2 Axes&gt;"/></div><div data-name="safe-output-image"><img src="/build/06e8a9bca8f286989d47cee486aaa748.png" alt="&lt;Figure size 3600x1800 with 2 Axes&gt;"/></div><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>✅ SHAP Analysis Completed Successfully.
</span></code></pre></div></div></div></div><div id="zkmoj-onixny" class="myst-jp-nb-block relative group/block"><h3 id="part-15-time-lagged-cross-correlation-tlcc-analysis" class="relative group"><span class="heading-text">Part 15: Time-Lagged Cross-Correlation (TLCC) Analysis</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#part-15-time-lagged-cross-correlation-tlcc-analysis" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p>This section introduces <strong>Time-Lagged Cross-Correlation (TLCC)</strong> to investigate the dynamic relationships between environmental drivers and red tide concentrations. Unlike standard correlation, TLCC identifies leads and lags in the data, revealing how long it takes for a change in an environmental factor (like a nutrient spike) to manifest as a biological response (a bloom).</p><h4 id="core-analytical-components" class="relative group"><span class="heading-text">Core Analytical Components</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#core-analytical-components" title="Link to this Section" aria-label="Link to this Section">¶</a></h4><p>The analysis provides a temporal roadmap of the ecosystem’s behavior, helping to validate the choice of lookback windows used in the LSTM model.</p><ol start="1"><li><p><strong>Identifying Temporal Offsets:</strong>
TLCC calculates the correlation between two time series—such as river discharge and <em>Karenia brevis</em> abundance—at various time shifts. By finding the “peak” correlation, we can determine the specific latency of the system. For example, if the peak correlation with Phosphorus occurs at a lag of 14 days, it suggests a two-week window for biological uptake and population growth.</p></li><li><p><strong>Directionality of Influence:</strong>
By observing whether the peak correlation occurs at a positive or negative lag, we can confirm the causal direction. A positive lag (where the environmental variable precedes the bloom) confirms the variable as a leading indicator or “driver,” whereas a zero lag suggests a simultaneous response.</p></li><li><p><strong>Stability and Seasonality:</strong>
The analysis can be extended to <strong>Windowed TLCC</strong>, which evaluates how these correlations change over time. This is particularly relevant for Florida’s coastlines, where the relationship between rainfall and red tide may shift between the wet and dry seasons.</p></li></ol><h4 id="insights-and-model-refinement" class="relative group"><span class="heading-text">Insights and Model Refinement</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#insights-and-model-refinement" title="Link to this Section" aria-label="Link to this Section">¶</a></h4><p>The results from this section serve as a critical bridge between data science and marine biology:</p><ul><li><p><strong>Feature Engineering Validation:</strong> We use these findings to refine the <code>lag_days</code> parameters in Part 4, ensuring the model focuses on the most biologically relevant timeframes.</p></li><li><p><strong>System Memory:</strong> TLCC helps quantify the “memory” of the ecosystem, indicating how long an environmental disturbance continues to influence bloom dynamics.</p></li><li><p><strong>Predictive Confidence:</strong> Identifying features with strong, stable leads increases our confidence in the LSTM’s ability to provide early warnings before a bloom is visible in satellite or field data.</p></li></ul><p>By incorporating TLCC, we move beyond static snapshots to a fluid, temporal understanding of the drivers behind red tide events.</p></div><div id="lojzaajmldqx" class="myst-jp-nb-block relative group/block"><div class="myst-jp-nb-block-spinner flex sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:hidden"><div class="flex absolute top-0 right-0"></div></div><div class="myst-jp-nb-block sticky top-[115px] z-10 opacity-90 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute -top-[12px] right-0 flex flex-row rounded bg-white dark:bg-slate-800"></div></div><div id="lojzaajmldqx-code" class="relative myst-code group not-prose my-5 text-sm shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 border border-l-4 border-gray-200 border-l-blue-400 dark:border-l-blue-400 dark:border-gray-800"><pre class="block overflow-auto p-3 myst-code-body hljs" style="background-color:unset"><code class="language-python" style="white-space:pre">def analyze_onset_lag(y_true, y_pred_prob, threshold=0.3, tolerance=4):
    &quot;&quot;&quot;pynb i
    Calculates exactly how many weeks late (or early) the model predicts blooms.

    Args:
        y_true: Binary actuals (0/1)
        y_pred_prob: Continuous probabilities (0.0-1.0)
        threshold: Decision threshold
        tolerance: Max weeks to look for a matching predicted onset
    &quot;&quot;&quot;
    print(f&quot;\n--- Onset Latency Analysis (Threshold: {threshold}) ---&quot;)

    y_pred_class = (y_pred_prob &gt;= threshold).astype(int)

    # 1. Identify &quot;Start&quot; events (0 -&gt; 1 transitions)
    # We use diff() to find where value changes from 0 to 1
    actual_onsets = np.where(np.diff(y_true, prepend=0) == 1)[0]
    pred_onsets = np.where(np.diff(y_pred_class, prepend=0) == 1)[0]

    print(f&quot;Actual Bloom Starts found: {len(actual_onsets)}&quot;)
    print(f&quot;Predicted Bloom Starts found: {len(pred_onsets)}&quot;)

    if len(actual_onsets) == 0:
        print(&quot;No blooms in test set to analyze.&quot;)
        return

    # 2. Match Actual Starts to Nearest Predicted Start
    lags = []

    for t_actual in actual_onsets:
        # Find predicted onsets within &#x27;tolerance&#x27; weeks window
        # We look for the closest prediction around the actual event
        nearby_preds = pred_onsets[np.abs(pred_onsets - t_actual) &lt;= tolerance]

        if len(nearby_preds) &gt; 0:
            # Find the closest one
            closest_pred = nearby_preds[np.argmin(np.abs(nearby_preds - t_actual))]

            # Lag = Predicted Time - Actual Time
            # Positive = Late (Lag)
            # Negative = Early Warning (Lead)
            lag = closest_pred - t_actual
            lags.append(lag)

            status = &quot;LATE&quot; if lag &gt; 0 else &quot;EARLY&quot; if lag &lt; 0 else &quot;PERFECT&quot;
            print(f&quot;  Event at Week {t_actual}: Model is {status} by {abs(lag)} weeks.&quot;)
        else:
            print(f&quot;  Event at Week {t_actual}: MISSED COMPLETELY (No prediction within {tolerance} weeks)&quot;)
            lags.append(np.nan) # Missed event

    # 3. Summary Stats
    lags_clean = [l for l in lags if not np.isnan(l)]
    if lags_clean:
        avg_lag = np.mean(lags_clean)
        print(f&quot;\n&gt;&gt;&gt; AVERAGE LATENCY: {avg_lag:.2f} Weeks&quot;)
        if avg_lag &gt; 0:
            print(&quot;    (Positive = The model is LAGGING)&quot;)
        else:
            print(&quot;    (Negative = The model gives EARLY WARNING)&quot;)
    else:
        print(&quot;\n&gt;&gt;&gt; No matched events found.&quot;)

    # 4. Visualizing the Lag
    # Cross-Correlation Plot (Statistical Proof)
    # Shift predictions back/forward and see where correlation is highest
    shifts = range(-5, 6) # Shift -5 to +5 weeks
    correlations = []
    for s in shifts:
        # Shift predicted probabilities
        if s &lt; 0:
            p_shifted = y_pred_prob[-s:]
            y_shifted = y_true[:s]
        elif s &gt; 0:
            p_shifted = y_pred_prob[:-s]
            y_shifted = y_true[s:]
        else:
            p_shifted = y_pred_prob
            y_shifted = y_true

        # Calculate correlation
        if len(y_shifted) &gt; 0:
            corr = np.corrcoef(y_shifted, p_shifted)[0, 1]
            correlations.append(corr)
        else:
            correlations.append(0)

    plt.figure(figsize=(8, 5), dpi=300)
    plt.bar(shifts, correlations, color=&#x27;skyblue&#x27;, edgecolor=&#x27;black&#x27;)

    # Highlight the max
    max_idx = np.argmax(correlations)
    best_lag = shifts[max_idx]
    plt.bar(best_lag, correlations[max_idx], color=&#x27;red&#x27;, label=f&#x27;Peak Correlation (Lag={best_lag})&#x27;)

    plt.title(&quot;Time-Lagged Cross-Correlation (TLCC)&quot;)
    plt.xlabel(&quot;Lag (Weeks)\n&lt;-- Model Leads (Good) | Model Lags (Bad) --&gt;&quot;)
    plt.ylabel(&quot;Correlation with Actuals&quot;)
    plt.axvline(0, color=&#x27;black&#x27;, linestyle=&#x27;--&#x27;)
    plt.legend()
    plt.tight_layout()
    plt.savefig(&quot;output/lag_analysis.png&quot;)
    plt.show()

# --- Execute ---
if __name__ == &quot;__main__&quot;:
    if &#x27;mc_preds&#x27; in locals() and &#x27;y_test_trimmed&#x27; in locals():
        # Get mean probs
        probs = np.mean(mc_preds, axis=1)

        # Run with your chosen threshold (e.g., 0.3)
        analyze_onset_lag(y_test_trimmed.flatten(), probs.flatten(), threshold=0.3)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 myst-code-copy-icon top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"></path></svg></button></div><div data-name="outputs-container" id="lojzaajmldqx-outputs" data-mdast-node-id="hFod345M9-CRs2GBbLjhu" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div data-name="safe-output-stream"><div><pre class="myst-jp-stream-output text-sm font-thin font-system"><code><span>
--- Onset Latency Analysis (Threshold: 0.3) ---
Actual Bloom Starts found: 10
Predicted Bloom Starts found: 4
  Event at Week 8: Model is PERFECT by 0 weeks.
  Event at Week 70: MISSED COMPLETELY (No prediction within 4 weeks)
  Event at Week 98: MISSED COMPLETELY (No prediction within 4 weeks)
  Event at Week 103: MISSED COMPLETELY (No prediction within 4 weeks)
  Event at Week 112: MISSED COMPLETELY (No prediction within 4 weeks)
  Event at Week 168: Model is LATE by 1 weeks.
  Event at Week 179: MISSED COMPLETELY (No prediction within 4 weeks)
  Event at Week 182: MISSED COMPLETELY (No prediction within 4 weeks)
  Event at Week 190: MISSED COMPLETELY (No prediction within 4 weeks)
  Event at Week 196: MISSED COMPLETELY (No prediction within 4 weeks)

&gt;&gt;&gt; AVERAGE LATENCY: 0.50 Weeks
    (Positive = The model is LAGGING)
</span></code></pre></div></div><div data-name="safe-output-image"><img src="/build/03025f3dd8a708372714dfae593a7247.png" alt="&lt;Figure size 2400x1500 with 1 Axes&gt;"/></div></div></div><div class="myst-backmatter-parts"></div></article></main><script>((a,l)=>{if(!window.history.state||!window.history.state.key){let u=Math.random().toString(32).slice(2);window.history.replaceState({key:u},"")}try{let d=JSON.parse(sessionStorage.getItem(a)||"{}")[l||window.history.state.key];typeof d=="number"&&window.scrollTo(0,d)}catch(u){console.error(u),sessionStorage.removeItem(a)}})("positions", null)</script><link rel="modulepreload" href="/build/entry.client-PCJPW7TK.js"/><link rel="modulepreload" href="/build/_shared/chunk-AQ2CODAG.js"/><link rel="modulepreload" href="/build/_shared/chunk-JJXTQVMA.js"/><link rel="modulepreload" href="/build/_shared/chunk-OZE3FFNP.js"/><link rel="modulepreload" href="/build/_shared/chunk-CH4FVTDV.js"/><link rel="modulepreload" href="/build/_shared/chunk-C4DFGG5C.js"/><link rel="modulepreload" href="/build/_shared/chunk-J7TUH54J.js"/><link rel="modulepreload" href="/build/_shared/chunk-FZ2S7OYD.js"/><link rel="modulepreload" href="/build/_shared/chunk-JEM6JXYA.js"/><link rel="modulepreload" href="/build/_shared/chunk-34XIY2DH.js"/><link rel="modulepreload" href="/build/_shared/chunk-KQM5FBHR.js"/><link rel="modulepreload" href="/build/_shared/chunk-OCWQY3HK.js"/><link rel="modulepreload" href="/build/_shared/chunk-7HNKBP4B.js"/><link rel="modulepreload" href="/build/_shared/chunk-CUKUDK3R.js"/><link rel="modulepreload" href="/build/_shared/chunk-3EBOCCHJ.js"/><link rel="modulepreload" href="/build/_shared/chunk-O4VQNZ62.js"/><link rel="modulepreload" href="/build/_shared/chunk-4OEDG4JQ.js"/><link rel="modulepreload" href="/build/_shared/chunk-GUCIBHGO.js"/><link rel="modulepreload" href="/build/root-SIO6LUTY.js"/><link rel="modulepreload" href="/build/_shared/chunk-FAHZZXAC.js"/><link rel="modulepreload" href="/build/routes/_index-QUJ6PDLY.js"/><script>window.__remixContext = {"url":"/","state":{"loaderData":{"root":{"config":{"version":3,"myst":"1.8.0","options":{"favicon":"/build/favicon-d02b964963c07538187932185c6245df.ico"},"nav":[],"actions":[{"title":"Download Dataset","url":"/build/data_weekly_interpol-99ca3242edec19edf7b1cbdf70a6aed8.csv","filename":"data_weekly_interpolated.csv","static":true}],"projects":[{"title":"Mitigating prediction drift in harmful algal bloom forecasting: A hybrid deep learning and ensemble Kalman filter framework","authors":[{"nameParsed":{"literal":"Victor M. Blanco","given":"Victor M.","family":"Blanco"},"name":"Victor M. Blanco","id":"contributors-Users\\victo\\PyCharmMiscProject\\myst-generated-uid-0"},{"nameParsed":{"literal":"Daniel Krutky","given":"Daniel","family":"Krutky"},"name":"Daniel Krutky","id":"contributors-Users\\victo\\PyCharmMiscProject\\myst-generated-uid-1"},{"nameParsed":{"literal":"Peter Nguyen","given":"Peter","family":"Nguyen"},"name":"Peter Nguyen","id":"contributors-Users\\victo\\PyCharmMiscProject\\myst-generated-uid-2"},{"nameParsed":{"literal":"Ahmed S. Elshall","given":"Ahmed S.","family":"Elshall"},"name":"Ahmed S. Elshall","id":"contributors-Users\\victo\\PyCharmMiscProject\\myst-generated-uid-3"},{"nameParsed":{"literal":"Ming Ye","given":"Ming","family":"Ye"},"name":"Ming Ye","id":"contributors-Users\\victo\\PyCharmMiscProject\\myst-generated-uid-4"},{"nameParsed":{"literal":"Michael L. Parsons","given":"Michael L.","family":"Parsons"},"name":"Michael L. Parsons","id":"contributors-Users\\victo\\PyCharmMiscProject\\myst-generated-uid-5"}],"keywords":["harmful algae blooms","red tides","machine learning","deep learning","ensemble Kalman filter","LSTM","uncertainty analysis","data assimilation","Florida"],"id":"98e22f5a-97c3-43e5-9695-946bb291d410","exports":[],"bibliography":[],"index":"lstm-enkf","pages":[]}]},"CONTENT_CDN_PORT":"3100","MODE":"static"},"routes/_index":{"config":{"version":3,"myst":"1.8.0","options":{"favicon":"/build/favicon-d02b964963c07538187932185c6245df.ico"},"nav":[],"actions":[{"title":"Download Dataset","url":"/build/data_weekly_interpol-99ca3242edec19edf7b1cbdf70a6aed8.csv","filename":"data_weekly_interpolated.csv","static":true}],"projects":[{"title":"Mitigating prediction drift in harmful algal bloom forecasting: A hybrid deep learning and ensemble Kalman filter framework","authors":[{"nameParsed":{"literal":"Victor M. Blanco","given":"Victor M.","family":"Blanco"},"name":"Victor M. Blanco","id":"contributors-Users\\victo\\PyCharmMiscProject\\myst-generated-uid-0"},{"nameParsed":{"literal":"Daniel Krutky","given":"Daniel","family":"Krutky"},"name":"Daniel Krutky","id":"contributors-Users\\victo\\PyCharmMiscProject\\myst-generated-uid-1"},{"nameParsed":{"literal":"Peter Nguyen","given":"Peter","family":"Nguyen"},"name":"Peter Nguyen","id":"contributors-Users\\victo\\PyCharmMiscProject\\myst-generated-uid-2"},{"nameParsed":{"literal":"Ahmed S. Elshall","given":"Ahmed S.","family":"Elshall"},"name":"Ahmed S. Elshall","id":"contributors-Users\\victo\\PyCharmMiscProject\\myst-generated-uid-3"},{"nameParsed":{"literal":"Ming Ye","given":"Ming","family":"Ye"},"name":"Ming Ye","id":"contributors-Users\\victo\\PyCharmMiscProject\\myst-generated-uid-4"},{"nameParsed":{"literal":"Michael L. Parsons","given":"Michael L.","family":"Parsons"},"name":"Michael L. Parsons","id":"contributors-Users\\victo\\PyCharmMiscProject\\myst-generated-uid-5"}],"keywords":["harmful algae blooms","red tides","machine learning","deep learning","ensemble Kalman filter","LSTM","uncertainty analysis","data assimilation","Florida"],"id":"98e22f5a-97c3-43e5-9695-946bb291d410","exports":[],"bibliography":[],"index":"lstm-enkf","pages":[]}]},"page":{"version":3,"kind":"Notebook","sha256":"76c286d5dc253bae14b24f9694bdd23e1c3ee36934db17e5846451d04204fb24","slug":"lstm-enkf","location":"/LSTM+EnKF.ipynb","dependencies":[],"frontmatter":{"title":"Outline","content_includes_title":false,"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"authors":[{"nameParsed":{"literal":"Victor M. Blanco","given":"Victor M.","family":"Blanco"},"name":"Victor M. Blanco","id":"contributors-Users\\victo\\PyCharmMiscProject\\myst-generated-uid-0"},{"nameParsed":{"literal":"Daniel Krutky","given":"Daniel","family":"Krutky"},"name":"Daniel Krutky","id":"contributors-Users\\victo\\PyCharmMiscProject\\myst-generated-uid-1"},{"nameParsed":{"literal":"Peter Nguyen","given":"Peter","family":"Nguyen"},"name":"Peter Nguyen","id":"contributors-Users\\victo\\PyCharmMiscProject\\myst-generated-uid-2"},{"nameParsed":{"literal":"Ahmed S. Elshall","given":"Ahmed S.","family":"Elshall"},"name":"Ahmed S. Elshall","id":"contributors-Users\\victo\\PyCharmMiscProject\\myst-generated-uid-3"},{"nameParsed":{"literal":"Ming Ye","given":"Ming","family":"Ye"},"name":"Ming Ye","id":"contributors-Users\\victo\\PyCharmMiscProject\\myst-generated-uid-4"},{"nameParsed":{"literal":"Michael L. Parsons","given":"Michael L.","family":"Parsons"},"name":"Michael L. Parsons","id":"contributors-Users\\victo\\PyCharmMiscProject\\myst-generated-uid-5"}],"keywords":["harmful algae blooms","red tides","machine learning","deep learning","ensemble Kalman filter","LSTM","uncertainty analysis","data assimilation","Florida"],"exports":[{"format":"ipynb","filename":"LSTM+EnKF.ipynb","url":"/build/LSTM+EnKF-f375f434b088e097355cdcd5881a5701.ipynb"}]},"widgets":{"application/vnd.jupyter.widget-state+json":{"0eef66dc7b3b45f581fc334c6d33db34":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_03534a03d35d49f983c66f8bc1572b9c","IPY_MODEL_eff0a5292d484968ba7e7bf3cef090c3","IPY_MODEL_eecb5d87e890498487275b62e90adcd4"],"layout":"IPY_MODEL_8b1224ba29d34b84b1981718c4b70ab0"}},"03534a03d35d49f983c66f8bc1572b9c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9dff54d4d6f4fde817049a8bc267cd1","placeholder":"​","style":"IPY_MODEL_f7a5380eec4d426ab054cfbfe967b776","value":"100%"}},"eff0a5292d484968ba7e7bf3cef090c3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b27cac4bb2d64bd090640e6a725c0057","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_486fa3ac2e2e4b2681e63ff0ba064f8e","value":20}},"eecb5d87e890498487275b62e90adcd4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac29d24173054a929fc194828a9f1e60","placeholder":"​","style":"IPY_MODEL_e5f7e2ea050041ed9cc2891b4d3c3eea","value":" 20/20 [00:02\u0026lt;00:00,  9.39it/s]"}},"8b1224ba29d34b84b1981718c4b70ab0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9dff54d4d6f4fde817049a8bc267cd1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7a5380eec4d426ab054cfbfe967b776":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b27cac4bb2d64bd090640e6a725c0057":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"486fa3ac2e2e4b2681e63ff0ba064f8e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ac29d24173054a929fc194828a9f1e60":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e5f7e2ea050041ed9cc2891b4d3c3eea":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"mdast":{"type":"root","children":[{"type":"block","kind":"notebook-content","data":{"id":"dc526dc0"},"children":[{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"This project presents a novel integration of Deep Learning and Sequential Data Assimilation to forecast Karenia brevis (red tide) blooms. While LSTMs are powerful at capturing non-linear temporal dependencies, they often suffer from “drift” when applied to long-term forecasting without mid-course corrections. Our framework solves this by treating the LSTM as the “transition model” within an Ensemble Kalman Filter (EnKF) loop.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Ayzj7FuYGQ"}],"key":"qgA5IPLMUk"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"Research Objectives\nThe primary goal of this research is to develop a robust, self-correcting forecasting system for harmful algal blooms along the Florida coast.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"AuscEExT3F"}],"key":"BxoOXuSs86"},{"type":"paragraph","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"Architecture Design: Construct an LSTM neural network optimized for high-dimensional environmental time-series and extreme class imbalance.","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"PkqAc6IGnD"}],"key":"Guu88qiaqW"},{"type":"paragraph","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Hybrid Integration: Develop a mathematical bridge that allows LSTM state outputs to be assimilated and corrected by an Ensemble Kalman Filter (EnKF) in real-time.","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"fwWWWGTJPz"}],"key":"EO7n769r82"},{"type":"paragraph","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"Uncertainty Quantification: Implement Monte Carlo (MC) Dropout to shift from deterministic “point-forecasts” to probabilistic risk assessments.","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"NB0lADmKZ0"}],"key":"zdI5GwjxVE"},{"type":"paragraph","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"Ecological Interpretability: Quantify the sensitivity of the model to specific environmental drivers like nutrient loading and river discharge using SHAP values.","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"BeZB7ON7G8"}],"key":"mNpg7uUKYv"},{"type":"paragraph","position":{"start":{"line":16,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"text","value":"Research Questions\nTo evaluate the effectiveness of this hybrid framework, this project seeks to answer:","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"rZtrNsmHyd"}],"key":"UIruzjYnxS"},{"type":"paragraph","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"text","value":"Can data assimilation mitigate LSTM drift? To what extent does the periodic injection of physical observations via EnKF improve the long-term stability of K. brevis forecasts?","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"caxr0qTlLU"}],"key":"WYMybVNIJ8"},{"type":"paragraph","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"How does ecological memory impact accuracy? Does the inclusion of long-term rolling aggregates and multi-day lags significantly outperform models relying only on immediate environmental snapshots?","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"s5GN0WVDtZ"}],"key":"zCgxfsOEw0"},{"type":"paragraph","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"What is the “Reliability-Resolution” trade-off? How well does the MC Dropout ensemble capture the actual variance of bloom occurrences?","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"o2xkfMB4LZ"}],"key":"zb6zvH6dLe"},{"type":"paragraph","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"text","value":"Which environmental drivers dominate the model’s decision-making? Does the model prioritize nutrient concentrations, physical transport (discharge), or biological persistence when predicting a bloom?","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"zpPdWbboYG"}],"key":"aPnOfe1dJW"},{"type":"paragraph","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"text","value":"Technical Roadmap","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"MUznlBz1yj"}],"key":"YbQGNYYjhG"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":28,"column":1},"end":{"line":30,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":28,"column":1},"end":{"line":30,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Data Foundation \u0026 Enhanced Engineering\nThe predictive power of this framework rests on capturing the “ecological memory” of the Florida coast.","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"key":"L7K9NXGWCs"}],"key":"jtdgLeDDtq"}],"key":"pd95oz9WXB"}],"key":"xE04xe6Sii"},{"type":"paragraph","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"children":[{"type":"text","value":"Feature Engineering (Parts 1-4): We move beyond raw concentrations to create Rolling Aggregates (7, 14, and 30-day windows) and Time-Lagged Variables. This allows the model to “see” nutrient accumulation trends and delayed river discharge impacts.","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"kPJ8pWnJ2s"}],"key":"clZi0DiFk2"},{"type":"paragraph","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"children":[{"type":"text","value":"Robust Preprocessing: Using RobustScaler, we ensure that extreme outliers—common in nutrient data during hurricane or high-flow events—do not skew the model’s weight distribution.","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"key":"pKooehBvLG"}],"key":"cB8Hb8Ra9N"},{"type":"list","ordered":true,"start":2,"spread":false,"position":{"start":{"line":35,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":35,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"The LSTM Predictive Engine\nArchitecture \u0026 Tuning (Parts 5-9): We implement a stacked LSTM architecture designed to process 3D temporal sequences. To combat extreme class imbalance, we utilize Custom Class Weighting in the loss function to penalize the misclassification of rare bloom events more heavily.","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"key":"fr3K63HEip"}],"key":"epCAMoofxd"}],"key":"jsEkC3RCN4"}],"key":"ajeMRW0vhO"},{"type":"paragraph","position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"children":[{"type":"text","value":"Hyperparameter Optimization: We utilize Bayesian optimization to determine the ideal hidden layer depth, dropout rates, and optimal sequence length (lookback).","position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"key":"cLSK1LpU19"}],"key":"bqaylEDRfk"},{"type":"list","ordered":true,"start":3,"spread":false,"position":{"start":{"line":40,"column":1},"end":{"line":42,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":40,"column":1},"end":{"line":42,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"The Hybrid Loop: Data Assimilation (EnKF)\nThis is the core innovation: anchoring deep learning predictions to physical reality.","position":{"start":{"line":40,"column":1},"end":{"line":40,"column":1}},"key":"b37qLVXKkd"}],"key":"Qe49GjfHtE"}],"key":"j6fzerzfGG"}],"key":"z3BSVaeQz8"},{"type":"paragraph","position":{"start":{"line":43,"column":1},"end":{"line":43,"column":1}},"children":[{"type":"text","value":"The Forecast Step: The LSTM generates a state estimate for the next time step.","position":{"start":{"line":43,"column":1},"end":{"line":43,"column":1}},"key":"OF52BFq0Rf"}],"key":"fQeL6lazMe"},{"type":"paragraph","position":{"start":{"line":45,"column":1},"end":{"line":45,"column":1}},"children":[{"type":"text","value":"The Assimilation Step (Parts 10-12): When a new physical observation (e.g., a nitrogen measurement) becomes available, the EnKF calculates the Kalman Gain. This corrects the model’s internal state variables before the next recursive forecast, effectively resetting the “drift.”","position":{"start":{"line":45,"column":1},"end":{"line":45,"column":1}},"key":"q7xMgEONTz"}],"key":"yZhUs0eMJL"},{"type":"paragraph","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"children":[{"type":"text","value":"MC Dropout: By keeping dropout active during inference, we generate an ensemble of predictions to quantify Epistemic Uncertainty.","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"wBu5MMFH7l"}],"key":"q2BJo5EzLL"},{"type":"list","ordered":true,"start":4,"spread":false,"position":{"start":{"line":49,"column":1},"end":{"line":51,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":49,"column":1},"end":{"line":51,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Advanced Evaluation \u0026 Explainability\nProbabilistic Evaluation (Part 13): We evaluate using Brier Skill Scores to measure the calibration of our probability forecasts and Precision-Recall Curves to assess bloom detection reliability.","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"key":"Pu33J0qTOL"}],"key":"YMEzAdQvbe"}],"key":"qaqtPTh9rZ"}],"key":"nbuV3rCxJQ"},{"type":"paragraph","position":{"start":{"line":52,"column":1},"end":{"line":52,"column":1}},"children":[{"type":"text","value":"Opening the Black Box (Parts 14-15): * SHAP Analysis: Identifying which specific nutrients are driving “high probability” forecasts.","position":{"start":{"line":52,"column":1},"end":{"line":52,"column":1}},"key":"xEyIac8Dm0"}],"key":"wQBwr6c7LM"},{"type":"paragraph","position":{"start":{"line":54,"column":1},"end":{"line":54,"column":1}},"children":[{"type":"text","value":"TLCC Analysis: Validating the lead-time of features to ensure the LSTM is learning biologically plausible causal relationships.","position":{"start":{"line":54,"column":1},"end":{"line":54,"column":1}},"key":"MrUjMvPcCo"}],"key":"Hucw1U4OLJ"}],"identifier":"dc526dc0","label":"dc526dc0","html_id":"dc526dc0","key":"N7VHcIrjHB"},{"type":"block","kind":"notebook-content","data":{"id":"vtGtg_lX_K5T"},"children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Library installation","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hgUMZTrNuF"}],"identifier":"library-installation","label":"Library installation","html_id":"library-installation","implicit":true,"key":"A3YeEo2Fxe"}],"identifier":"vtgtg_lx_k5t","label":"vtGtg_lX_K5T","html_id":"vtgtg-lx-k5t","key":"sCQmTA3621"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iuwFYPopie1_","outputId":"1fec5d1b-1541-445e-9c4b-3849ead10d6d","jupyter":{"is_executing":true}},"children":[{"type":"code","lang":"python","executable":true,"value":"!pip install tensorflow.keras\n!pip install keras_tuner\nimport json\nimport os\n\nimport joblib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport shap\nimport tensorflow as tf\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score,\n                             brier_score_loss, accuracy_score, precision_score,\n                             recall_score, f1_score)\nfrom sklearn.metrics import (precision_recall_curve)\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, Input\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.metrics import BinaryAccuracy, AUC, Recall, Precision\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\ntry:\n    from tqdm.notebook import tqdm\n    print(\"Using tqdm.notebook for progress bars.\")\nexcept ImportError:\n    try:\n        from tqdm import tqdm\n        print(\"Using standard tqdm for progress bars.\")\n    except ImportError:\n        print(\"Warning: tqdm not installed. Progress bars will not be shown.\")\n        # Define a dummy tqdm function if not installed\n        def tqdm(iterable=None, *args, **kwargs):\n            if iterable is None:\n                # Handle case where tqdm is called without an iterable\n                class DummyTqdm:\n                    def __enter__(self): return self\n                    def __exit__(self, *args): pass\n                    def update(self, n=1): pass\n                    def close(self): pass\n                    def set_description(self, desc): pass\n                return DummyTqdm()\n            else:\n                return iterable\n\nprint(\"Imported libraries.\")","identifier":"iuwfypopie1_-code","enumerator":"1","html_id":"iuwfypopie1-code","key":"gprOA9iIki"},{"type":"outputs","id":"y94gKTDEspEHt5sU5JZpX","children":[],"identifier":"iuwfypopie1_-outputs","html_id":"iuwfypopie1-outputs","key":"hsGQhjvDRZ"}],"identifier":"iuwfypopie1_","label":"iuwFYPopie1_","html_id":"iuwfypopie1","key":"cKcb3rigiq"},{"type":"block","kind":"notebook-content","data":{"id":"2ngbXkSqi50I"},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Part 1: Configuration \u0026 Setup","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"d21FkQgDge"}],"identifier":"part-1-configuration-setup","label":"Part 1: Configuration \u0026 Setup","html_id":"part-1-configuration-setup","implicit":true,"key":"EtTJkX7fgY"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Define all file paths (input data, saved components like scaler/model).","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"ZPi4euY4b4"}],"key":"cIengZFpph"}],"key":"pSfn63MnCT"},{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Set core parameters: ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"bycrC4uOZd"},{"type":"inlineCode","value":"SEQUENCE_LENGTH","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"YkT2FVwuBr"},{"type":"text","value":", ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"vFkyAj7aRw"},{"type":"inlineCode","value":"FORECAST_HORIZON","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"JL0j6Gdxw6"},{"type":"text","value":", ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"xkkM6GbjTh"},{"type":"inlineCode","value":"BLOOM_THRESHOLD","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Kkij3jv5nv"},{"type":"text","value":".","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"YF7P2B5u2r"}],"key":"PikgbpiPLz"}],"key":"THcRZ1Gozv"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Define feature lists: ","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"p9w2UiSYx1"},{"type":"inlineCode","value":"BASE_FEATURES","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"HYG73Jc64X"},{"type":"text","value":", ","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"wQIeRaqXjM"},{"type":"inlineCode","value":"ENKF_STATE_VARS","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"E5UqEvNScZ"},{"type":"text","value":", ","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"BFmsxkSCti"},{"type":"inlineCode","value":"ENHANCED_KB_LAGS","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"pkxjHFIwwS"},{"type":"text","value":" (optional).","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"svXnin8Ubl"}],"key":"bIsu5bbe8x"}],"key":"xs8Lu3oZpR"},{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Control Flags/Options:","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"gWrRZkIGz6"}],"key":"tWpPtYMvmU"}],"key":"QJ3TVGEt8z"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":6,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"paragraph","children":[{"type":"inlineCode","value":"USE_ENHANCED_FEATURES","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"ivbfCy4sLm"},{"type":"text","value":": Boolean (True/False) to switch between basic and enhanced feature engineering.","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"oCpjAf7GFh"}],"key":"uimaANDNl2"}],"key":"hDY0fcCF2i"},{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"paragraph","children":[{"type":"inlineCode","value":"SCALER_TYPE","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"sgAXwxTIk3"},{"type":"text","value":": String (‘Standard’ or ‘Robust’) to choose the scaler.","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"neVZxisAn0"}],"key":"jd6dzWmV4n"}],"key":"OYLEcqSjZ2"},{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"inlineCode","value":"USE_CLASS_WEIGHT","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"u1yFQVTXJG"},{"type":"text","value":": Boolean (True/False) to enable/disable class weighting during training.","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"ns6qCe9oq4"}],"key":"aBAMxKG6SC"}],"key":"nHB5x3y6zV"},{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"paragraph","children":[{"type":"inlineCode","value":"PERFORM_TUNING","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"Md90jpxYpw"},{"type":"text","value":": Boolean (True/False) to run hyperparameter tuning or use defaults.","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"ZXgHQ0zRsT"}],"key":"dAiYbnMdhv"}],"key":"Oe1Er90S2P"}],"key":"vrTy1wqJE7"}],"key":"T90SObwacG"},{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Set tuning parameters: ","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"JjapzgLiol"},{"type":"inlineCode","value":"MAX_TRIALS","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"Egy8zPmbYE"},{"type":"text","value":", ","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"QlyxPAXrIl"},{"type":"inlineCode","value":"TUNER_EPOCHS","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"SFsayCL8nG"},{"type":"text","value":".","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"jlmNj1y2J7"}],"key":"lOTCLk1UnO"}],"key":"gOxtsKCPcZ"},{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Set training parameters: ","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"tWnJ6vCKGz"},{"type":"inlineCode","value":"EPOCHS","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"Kx8kNJSs9q"},{"type":"text","value":", ","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"w6SrPTPgyr"},{"type":"inlineCode","value":"BATCH_SIZE","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"tNlw9rWtRY"},{"type":"text","value":", ","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"espEVVw8HR"},{"type":"inlineCode","value":"PATIENCE","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"K6VQFcPqrD"},{"type":"text","value":".","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"Umw9cDNLn4"}],"key":"GqHF3WkS1U"}],"key":"bpfKmeFDj3"},{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Set EnKF parameters: ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"CcYaLJu5An"},{"type":"inlineCode","value":"N_ENKF","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"OV2sSPEq3M"},{"type":"text","value":" (ensemble size), define noise matrix estimation approach (e.g., ‘basic_stats’ or ‘manual’).","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"XVmhRh9lZg"}],"key":"FykBcyxfaF"}],"key":"chpXi4p9KM"},{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Set MC Dropout parameters: ","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"o1bZxSSOBZ"},{"type":"inlineCode","value":"N_MC_SAMPLES","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"oFL3xzQhjK"},{"type":"text","value":".","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"STnDfahr52"}],"key":"gt2wZzOYgF"}],"key":"H14MJFKFOh"}],"key":"x1allual8g"}],"identifier":"2ngbxksqi50i","label":"2ngbXkSqi50I","html_id":"id-2ngbxksqi50i","key":"NmFq9YQMvt"},{"type":"block","kind":"notebook-code","data":{"id":"vP1-KGorslgp","ExecuteTime":{"end_time":"2026-01-07T03:44:43.360958Z","start_time":"2026-01-07T03:37:16.343344Z"}},"children":[{"type":"code","lang":"python","executable":true,"value":"# Set these flags to control the workflow execution\n# === Feature Engineering ===\n# Set to True to use detailed lags (like RF paper), False: Use basic lags\nUSE_ENHANCED_FEATURES = True\n\n# === Preprocessing ===\n# Options: 'Robust' or 'Standard'\nSCALER_TYPE = 'Robust'\n\n# === Training ===\n# Set to True to apply class weighting during LSTM training\nUSE_CLASS_WEIGHT = True\n# Set to True to run KerasTuner hyperparameter search, False to use default HPs\nPERFORM_TUNING = True\n\n# === Advanced Steps ===\n# Set to True to run EnKF data assimilation during testing/forecasting\nPERFORM_ENKF = True","identifier":"vp1-kgorslgp-code","enumerator":"2","html_id":"vp1-kgorslgp-code","key":"W37X4U5X7f"},{"type":"outputs","id":"_zS-G4kyLNSzrzojOa8ms","children":[],"identifier":"vp1-kgorslgp-outputs","html_id":"vp1-kgorslgp-outputs","key":"H48D52Umqw"}],"identifier":"vp1-kgorslgp","label":"vP1-KGorslgp","html_id":"vp1-kgorslgp","key":"xX69RmGZ6F"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b5eUaHJksrFS","outputId":"a49d3d56-ded0-45e3-941b-e581a2233584","ExecuteTime":{"end_time":"2026-01-07T03:44:43.360958Z","start_time":"2026-01-07T03:34:10.647980Z"}},"children":[{"type":"code","lang":"python","executable":true,"value":"# --- File Paths ---\nINPUT_DATA_PATH = 'data_weekly_interpolated.csv'\n# Directory for optional external files (e.g., Caloosahatchee) - Ensure this exists if used\nEXTERNAL_DATA_DIR = 'external_data/'\n# Directory to save results, models, scalers (use a distinct name)\nOUTPUT_DIR = 'output_refactored/'\n# Construct filenames dynamically based on config where appropriate\nscaler_suffix = SCALER_TYPE.lower()\nfeature_suffix = 'enhanced' if USE_ENHANCED_FEATURES else 'basic'\nSCALER_FILENAME = os.path.join(OUTPUT_DIR, f'red_tide_scaler_{scaler_suffix}.joblib')\nFEATURE_LIST_FILENAME = os.path.join(OUTPUT_DIR, f'red_tide_feature_list_{feature_suffix}.joblib')\n# Template for sequence files: {horizon}{feature_suffix}\nSEQUENCES_FILENAME_TEMPLATE = os.path.join(OUTPUT_DIR, 'sequences_horizon{}wk_{}.npz')\n# Template for model checkpoint files: {model_type} e.g., baseline_weighted\n# Using .keras extension for saving the full model (architecture + weights + optimizer state)\nMODEL_CHECKPOINT_TEMPLATE = os.path.join(OUTPUT_DIR, 'best_lstm_model_{}.keras')\n# Use .weights.h5 if saving only weights (e.g., for subclassed models like physics-informed)\nMODEL_WEIGHTS_CHECKPOINT_TEMPLATE = os.path.join(OUTPUT_DIR, 'best_lstm_model_{}.weights.h5')\n\nTUNER_PROJECT_DIR = 'keras_tuner_dir_refactored' # Use a new name\nTUNER_PROJECT_NAME = 'red_tide_lstm_tuning'\n\n# Create output directory if it doesn't exist\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nprint(f\"Output directory: {OUTPUT_DIR}\")","identifier":"b5euahjksrfs-code","enumerator":"3","html_id":"b5euahjksrfs-code","key":"pTqYIVw5Yi"},{"type":"outputs","id":"r_D-DzDmGcWAkXWbvy65F","children":[{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"Output directory: output_refactored/\n"},"children":[],"identifier":"b5euahjksrfs-outputs-0","html_id":"b5euahjksrfs-outputs-0","key":"mcxE0nw6Wk"}],"identifier":"b5euahjksrfs-outputs","html_id":"b5euahjksrfs-outputs","key":"Ntl5xCF9sK"}],"identifier":"b5euahjksrfs","label":"b5eUaHJksrFS","html_id":"b5euahjksrfs","key":"VpDzgJ4DSn"},{"type":"block","kind":"notebook-code","data":{"id":"6wGE-LHas0R_","ExecuteTime":{"end_time":"2026-01-07T03:44:43.368602700Z","start_time":"2026-01-07T03:34:10.669998Z"}},"children":[{"type":"code","lang":"python","executable":true,"value":"# --- Core Parameters ---\nDATETIME_COL = 'time'\nTARGET_COL = 'kb' # Original cell count column\nTARGET_BINARY_COL = 'bloom_target' # Binary target column name\nBLOOM_THRESHOLD = 1e5 # Cells/L\nSEQUENCE_LENGTH = 12  # Default sequence length (can be tuned)\nFORECAST_HORIZON = 1   # Default forecast horizon (1-week or 4-week)","identifier":"6wge-lhas0r_-code","enumerator":"4","html_id":"id-6wge-lhas0r-code","key":"Vy42H9dHSl"},{"type":"outputs","id":"pJcpAjoInJKYEpZqCh3Sq","children":[],"identifier":"6wge-lhas0r_-outputs","html_id":"id-6wge-lhas0r-outputs","key":"hCaSSUCSGH"}],"identifier":"6wge-lhas0r_","label":"6wGE-LHas0R_","html_id":"id-6wge-lhas0r","key":"Wfzqzd24wx"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BqoFhfOQiRYw","outputId":"30f18e42-46d3-4574-e0e2-aa90d4332569","ExecuteTime":{"end_time":"2026-01-07T03:44:43.372952600Z","start_time":"2026-01-07T03:35:26.298847Z"}},"children":[{"type":"code","lang":"python","executable":true,"value":"# --- Feature Engineering Configuration ---\nBASE_FEATURES = [ # Features from the core dataset to consider initially\n    'zos', 'water_temp',\n    'peace_discharge', 'peace_TN', 'peace_TP',\n    'wind_u', 'wind_v'\n]\n# Define basic lag configuration\nBASIC_LAG_CONFIG = {\n    'kb_lags': [1, 2],\n    'env_lags': list(range(1, 7)) # e.g., 1 to 6 weeks\n}\n# Define enhanced lag configuration (matching RF paper more closely)\nENHANCED_LAG_CONFIG = {\n    # 'kb_lags': [1, 2],\n    'kb_rolling_windows': [(1, 4), (5, 8), (9, 12)], # (start_lag, end_lag) for mean \u0026 prop\n    'discharge_rolling_window': 4, # e.g., 4-week avg discharge lag 1\n    'env_lags': [1] # Lags for other env vars in enhanced mode\n}\n\n# --- Data Splitting ---\nTRAIN_SPLIT_RATIO = 0.70\nVALIDATION_SPLIT_RATIO = 0.15\n# Test split is the remainder\n\n# --- LSTM Model Default Hyperparameters (Used if PERFORM_TUNING is False) ---\nDEFAULT_LSTM_UNITS = 64\nDEFAULT_DROPOUT_RATE = 0.3\nDEFAULT_LEARNING_RATE = 0.001\n\n# --- KerasTuner Configuration (Used if PERFORM_TUNING is True) ---\nTUNER_MAX_TRIALS = 10\nTUNER_EPOCHS = 30\nTUNER_BATCH_SIZE = 32\n\n# --- Training Parameters ---\nTRAIN_EPOCHS = 50\nTRAIN_BATCH_SIZE = 32\nTRAIN_PATIENCE = 10 # For EarlyStopping\n\n# --- EnKF Configuration ---\nENKF_STATE_VARS = [\n    'peace_discharge', 'peace_TN', 'peace_TP', 'kb','wind_u', 'wind_v'\n]\n# Variables to assimilate\nN_ENKF = 50 # Ensemble size\n# Noise Estimation Approach ('basic_stats' uses train set stats, 'manual' requires defining R_diag, Q_diag below)\nENKF_NOISE_ESTIMATION = 'manual'\n# Manual noise variances (used only if ENKF_NOISE_ESTIMATION = 'manual') - Define placeholder values\nMANUAL_R_DIAG = [\n    (100.0 * 0.01)**2, # Discharge (Trust the gauge)\n    (0.2 * 0.01)**2,   # TN (Trust the sample)\n    (0.02 * 0.01)**2,  # TP\n    (5.0 * 0.05)**2,   # Wind U (Wind is noisy, maybe 5% error)\n    (5.0 * 0.05)**2,   # Wind V\n    (0.1 * 0.01)**2    # ZOS (Trust the satellite) # Example observation noise variances\n    ]\nMANUAL_Q_DIAG = [\n    (150.0 * 2.0)**2,  # Discharge can spike massively\n    (0.3 * 2.0)**2,    # TN spikes\n    (0.03 * 2.0)**2,   # TP spikes\n    (10.0 * 1.5)**2,   # Wind changes direction rapidly\n    (10.0 * 1.5)**2,   # Wind V\n    (0.1 * 1.0)**2     # ZOS changes\n]\n\n# --- MC Dropout Configuration ---\nN_MC_SAMPLES = 50\n\n\n# --- Print Setup Summary ---\nprint(\"\\n--- Workflow Configuration Summary ---\")\nprint(f\"Enhanced Features Enabled: {USE_ENHANCED_FEATURES}\")\nprint(f\"Scaler Type Selected: {SCALER_TYPE}\")\nprint(f\"Class Weighting Enabled: {USE_CLASS_WEIGHT}\")\nprint(f\"Hyperparameter Tuning Enabled: {PERFORM_TUNING}\")\nprint(f\"EnKF Enabled: {PERFORM_ENKF}\")\nprint(f\"Sequence Length: {SEQUENCE_LENGTH}\")\nprint(f\"Forecast Horizon: {FORECAST_HORIZON} week(s)\")\nif PERFORM_TUNING:\n    print(f\"Tuner Max Trials: {TUNER_MAX_TRIALS}, Epochs per Trial: {TUNER_EPOCHS}\")\nelse:\n    print(f\"Using Default LSTM HPs: Units={DEFAULT_LSTM_UNITS}, Dropout={DEFAULT_DROPOUT_RATE}, LR={DEFAULT_LEARNING_RATE}\")\nif PERFORM_ENKF:\n    print(f\"EnKF Ensemble Size: {N_ENKF}, Noise Estimation: {ENKF_NOISE_ESTIMATION}\")\nprint(\"------------------------------------\")\n\n# Check if optional modules were imported if flags are set\nif PERFORM_TUNING and kt is None:\n    print(\"\\nWarning: KerasTuner (kt) not imported/installed, but PERFORM_TUNING is True. Tuning will be skipped.\")\n    PERFORM_TUNING = False # Disable tuning if library not available\nif PERFORM_ENKF and 'EnsembleKalmanFilter' not in locals():\n     # We will define EnKF class later, but good to note dependency\n     print(\"\\nNote: EnKF flag is True, ensure EnsembleKalmanFilter class is defined/imported later.\")","identifier":"bqofhfoqiryw-code","enumerator":"5","html_id":"bqofhfoqiryw-code","key":"U429jJ6ozl"},{"type":"outputs","id":"AcWurW_a4vlmGCr2EVOO-","children":[{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"\n--- Workflow Configuration Summary ---\nEnhanced Features Enabled: True\nScaler Type Selected: Robust\nClass Weighting Enabled: True\nHyperparameter Tuning Enabled: True\nEnKF Enabled: True\nSequence Length: 12\nForecast Horizon: 1 week(s)\nTuner Max Trials: 10, Epochs per Trial: 30\nEnKF Ensemble Size: 50, Noise Estimation: manual\n------------------------------------\n\nNote: EnKF flag is True, ensure EnsembleKalmanFilter class is defined/imported later.\n"},"children":[],"identifier":"bqofhfoqiryw-outputs-0","html_id":"bqofhfoqiryw-outputs-0","key":"E6T7RMdxAb"}],"identifier":"bqofhfoqiryw-outputs","html_id":"bqofhfoqiryw-outputs","key":"YDvpHOvgl7"}],"identifier":"bqofhfoqiryw","label":"BqoFhfOQiRYw","html_id":"bqofhfoqiryw","key":"z2hm7nFrsF"},{"type":"block","kind":"notebook-content","data":{"id":"HKujscl3jRj2"},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Part 2: Data Loading \u0026 Initial Processing","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZdFX7noU9Y"}],"identifier":"part-2-data-loading-initial-processing","label":"Part 2: Data Loading \u0026 Initial Processing","html_id":"part-2-data-loading-initial-processing","implicit":true,"key":"VjadFqJ92P"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"This cell defines and executes a function to load the raw weekly data, handle the datetime index, calculate U/V wind components, and create the binary target variable based on the bloom threshold. It uses the configuration parameters defined in Part 1.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"ts3uGq2eRg"}],"key":"YOfjSoUU3A"},{"type":"paragraph","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"Explanation:","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"dOWvEyer2c"}],"key":"UIPkW7E6lR"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":8,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"calculate_wind_components(df): Takes a DataFrame, calculates U and V wind components if ‘wind_direction’ and ‘wind_speed’ exist, drops the original columns, and returns the modified DataFrame and a list of the newly added column names. Includes basic NaN handling for the calculation.","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"FBzaXpovzf"}],"key":"ac5QZkX76v"}],"key":"TObNbvgWK9"},{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"create_target(...): Takes the DataFrame and target configuration, ensures the original target column is numeric, drops rows where the target is NaN (important for supervised learning), creates the binary bloom_target column, prints the class distribution, and returns the DataFrame.","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"PCsRmhtlaU"}],"key":"j3V8VDBlji"}],"key":"i8blAjh4mx"},{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"load_and_prepare_data(...): Orchestrates the initial steps: loads the CSV, sets the datetime index, calls calculate_wind_components, calls create_target, and returns the resulting DataFrame (df_initial). Includes error handling for file not found or missing columns.","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"SZ8eGpwY3n"}],"key":"PdcLIL41JE"}],"key":"uq1ezsX6d5"},{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"merge_external_data(...) (Optional Placeholder): Provides a structure for loading and merging additional datasets based on a dictionary of file paths. It performs a left merge to keep all original data points. Note: This function needs actual file paths and assumes external CSVs have a compatible datetime index.","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"XWyj0ug1cD"}],"key":"xF7L0GPPFT"}],"key":"VaeQ4qG0sS"},{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Main Execution Block (if ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"RCrw30QLB8"},{"type":"strong","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"name","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"HAEi9HdYqQ"}],"key":"oSxNuPfm0v"},{"type":"text","value":" == “","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"XcfsflN0Dr"},{"type":"strong","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"main","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"wRVyRd6umy"}],"key":"IQIz8XX1pn"},{"type":"text","value":"”:): Calls load_and_prepare_data using the configuration variables. Includes a commented-out section showing how merge_external_data would be called if needed. Prints final info about the df_initial DataFrame. Includes basic NameError handling in case Part 1 wasn’t run.","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"uBTz2PBTft"}],"key":"oAYCmK0rH4"}],"key":"eiuiTGhgwn"}],"key":"PH0zKIiEKs"},{"type":"paragraph","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"After running this cell, the df_initial DataFrame should contain the core data with wind components calculated and the binary bloom target created, ready for the feature engineering steps in Part 3.","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"nNwS59a5WG"}],"key":"wix7BzBu5n"}],"identifier":"hkujscl3jrj2","label":"HKujscl3jRj2","html_id":"hkujscl3jrj2","key":"gjeOKTQF7G"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jaMZelqJjS5m","outputId":"223d2c0c-8aa0-4249-f7d2-3bf492b3cff4","ExecuteTime":{"end_time":"2026-01-07T03:44:43.373949800Z","start_time":"2026-01-07T03:35:26.321856Z"}},"children":[{"type":"code","lang":"python","executable":true,"value":"# Ensure configuration variables from Part 1 are accessible in the environment\n# Example: INPUT_DATA_PATH, DATETIME_COL, TARGET_COL, BLOOM_THRESHOLD, TARGET_BINARY_COL\n\ndef calculate_wind_components(df):\n    \"\"\"Calculates U and V wind components if columns exist.\"\"\"\n    if 'wind_direction' in df.columns and 'wind_speed' in df.columns:\n        print(\"Calculating wind U/V components...\")\n        # Ensure numeric types, handle potential NaNs before calculation\n        df['wind_direction'] = pd.to_numeric(df['wind_direction'], errors='coerce')\n        df['wind_speed'] = pd.to_numeric(df['wind_speed'], errors='coerce')\n\n        # Temporarily fill NaNs with 0 for calculation if they exist\n        wind_cols_to_check = ['wind_direction', 'wind_speed']\n        if df[wind_cols_to_check].isnull().any().any():\n            print(\"Warning: NaNs found in wind direction/speed. Temporarily filling with 0 for component calculation.\")\n            df[wind_cols_to_check] = df[wind_cols_to_check].fillna(0)\n\n        wind_dir_rad = np.deg2rad(df['wind_direction'])\n        wind_speed = df['wind_speed']\n        # Meteorological convention: wind direction 'coming from'\n        df['wind_u'] = -wind_speed * np.sin(wind_dir_rad)\n        df['wind_v'] = -wind_speed * np.cos(wind_dir_rad)\n        # Drop original wind columns\n        df = df.drop(columns=['wind_direction', 'wind_speed'])\n        print(\"Calculated wind U/V components and dropped original columns.\")\n        added_cols = ['wind_u', 'wind_v']\n    else:\n        print(\"Warning: 'wind_direction' or 'wind_speed' not found. Skipping component calculation.\")\n        added_cols = []\n    return df, added_cols\n\ndef create_target(df, target_col, bloom_threshold, target_binary_col):\n    \"\"\"Creates the binary target variable.\"\"\"\n    if target_col not in df.columns:\n        print(f\"Error: Target column '{target_col}' not found.\")\n        return None # Return None if target column is missing\n\n    print(f\"Creating binary target '{target_binary_col}' using threshold {bloom_threshold:.0f} cells/L...\")\n    df[target_col] = pd.to_numeric(df[target_col], errors='coerce')\n\n    # Handle potential NaNs in target before comparison by dropping rows\n    initial_rows = len(df)\n    if df[target_col].isnull().any():\n        print(f\"Warning: NaNs found in target column '{target_col}'. Dropping rows with NaN target.\")\n        df.dropna(subset=[target_col], inplace=True)\n        print(f\"Dropped {initial_rows - len(df)} rows with NaN in '{target_col}'. New shape: {df.shape}\")\n\n    df[target_binary_col] = (df[target_col] \u003e= bloom_threshold).astype(int)\n    print(f\"Target distribution (%):\\n{df[target_binary_col].value_counts(normalize=True) * 100}\")\n    return df\n\ndef load_and_prepare_data(filepath, datetime_col, target_col, bloom_threshold, target_binary_col):\n    \"\"\"Loads data, handles datetime, calculates wind components, creates target.\"\"\"\n    print(f\"Loading data from: {filepath}\")\n    try:\n        df = pd.read_csv(filepath)\n        # Handle datetime index\n        if datetime_col not in df.columns:\n            raise ValueError(f\"Datetime column '{datetime_col}' not found.\")\n        df[datetime_col] = pd.to_datetime(df[datetime_col])\n        df = df.sort_values(datetime_col).set_index(datetime_col)\n        print(f\"Data loaded successfully. Shape: {df.shape}, Time range: {df.index.min()} to {df.index.max()}\")\n        print(f\"Initial NaN counts:\\n{df.isnull().sum()}\")\n\n        # Calculate wind components\n        df, wind_cols = calculate_wind_components(df)\n\n        # Create binary target\n        df = create_target(df, target_col, bloom_threshold, target_binary_col)\n\n        if df is not None:\n            print(\"\\n--- Initial Data Preparation Complete ---\")\n            print(f\"DataFrame shape after initial processing: {df.shape}\")\n            print(\"Columns:\", df.columns.tolist())\n            print(\"\\nFirst 5 rows:\")\n            print(df.head())\n        return df\n\n    except FileNotFoundError:\n        print(f\"Error: Input data file not found at {filepath}\")\n        return None\n    except ValueError as ve:\n        print(f\"ValueError during data preparation: {ve}\")\n        return None\n    except Exception as e:\n        print(f\"An unexpected error occurred during data loading/preparation: {e}\")\n        return None\n\n# --- Optional: Function Placeholder for Merging External Data ---\ndef merge_external_data(df, external_files_dict):\n    \"\"\"\n    Loads and merges external data (e.g., Caloosahatchee Q/NOx) into the main dataframe.\n    Args:\n        df (pd.DataFrame): The main dataframe with datetime index.\n        external_files_dict (dict): Dictionary where keys are column names (e.g., 'caloos_Q')\n                                     and values are file paths to the external CSV data.\n                                     External CSVs must have a compatible datetime index.\n    Returns:\n        pd.DataFrame: DataFrame with external data merged (left join).\n    \"\"\"\n    print(\"\\n--- Merging External Data (Placeholder) ---\")\n    if not external_files_dict:\n        print(\"No external data files specified.\")\n        return df\n\n    df_merged = df.copy()\n    for col_name, file_path in external_files_dict.items():\n        try:\n            print(f\"Loading external data for '{col_name}' from '{file_path}'...\")\n            # Assuming external CSV has datetime index named same as DATETIME_COL or is the index\n            df_ext = pd.read_csv(file_path, index_col=DATETIME_COL, parse_dates=True) # Adjust index_col if needed\n            df_ext = df_ext[[col_name]] # Keep only the specified column\n            # Perform left merge\n            df_merged = df_merged.merge(df_ext, left_index=True, right_index=True, how='left')\n            print(f\"Merged '{col_name}'. NaN count: {df_merged[col_name].isnull().sum()}\")\n        except FileNotFoundError:\n            print(f\"Warning: External data file not found: {file_path}. Skipping '{col_name}'.\")\n        except KeyError:\n             print(f\"Warning: Column '{col_name}' not found in file {file_path}. Skipping.\")\n        except Exception as e:\n            print(f\"Error merging external file {file_path}: {e}\")\n\n    print(\"External data merging complete.\")\n    return df_merged\n\n\n# --- Execute Data Loading and Preparation ---\n# Ensure config variables from Part 1 are defined before running this\nif __name__ == \"__main__\":\n    try:\n        df_initial = load_and_prepare_data(\n            INPUT_DATA_PATH,\n            DATETIME_COL,\n            TARGET_COL,\n            BLOOM_THRESHOLD,\n            TARGET_BINARY_COL\n        )\n\n        if df_initial is not None:\n            # Display basic info about the prepared dataframe\n            print(\"\\n--- Dataframe after initial processing (df_initial) ---\")\n            df_initial.info()\n        else:\n            print(\"\\nData loading/preparation failed.\")\n\n    except NameError as ne:\n         print(f\"NameError: A configuration variable might be missing. Did Part 1 run correctly? Error: {ne}\")\n    except Exception as e:\n         print(f\"An unexpected error occurred in main execution block: {e}\")\n\n","identifier":"jamzelqjjs5m-code","enumerator":"6","html_id":"jamzelqjjs5m-code","key":"LIS3refIMa"},{"type":"outputs","id":"EDaiOi_gW4ZDB4TG937lW","children":[{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"Loading data from: data_weekly_intepolated.csv\nData loaded successfully. Shape: (1617, 9), Time range: 1993-01-04 00:00:00 to 2023-12-25 00:00:00\nInitial NaN counts:\nkb                 0\nzos                0\nsalinity           0\nwater_temp         0\nwind_direction     0\nwind_speed         0\npeace_discharge    0\npeace_TN           0\npeace_TP           0\ndtype: int64\nCalculating wind U/V components...\nCalculated wind U/V components and dropped original columns.\nCreating binary target 'bloom_target' using threshold 100000 cells/L...\nTarget distribution (%):\nbloom_target\n0    73.09833\n1    26.90167\nName: proportion, dtype: float64\n\n--- Initial Data Preparation Complete ---\nDataFrame shape after initial processing: (1617, 10)\nColumns: ['kb', 'zos', 'salinity', 'water_temp', 'peace_discharge', 'peace_TN', 'peace_TP', 'wind_u', 'wind_v', 'bloom_target']\n\nFirst 5 rows:\n               kb       zos   salinity  water_temp  peace_discharge  peace_TN  \\\ntime                                                                            \n1993-01-04  333.0  0.012906  33.043478        26.8            202.0    8.2000   \n1993-01-11  667.0  0.015614  33.065217        27.0            423.0   10.1000   \n1993-01-18  667.0  0.021702  33.086957        27.1           1470.0   12.0000   \n1993-01-25    0.0  0.015950  33.108696        26.8           1450.0   10.0475   \n1993-02-01    0.0  0.008977  33.130435        26.5           1490.0    8.0950   \n\n            peace_TP     wind_u     wind_v  bloom_target  \ntime                                                      \n1993-01-04  1.999091  -8.170215 -11.245336             0  \n1993-01-11  1.934545 -14.303751   7.605439             0  \n1993-01-18  1.870000 -15.407116   5.006075             0  \n1993-01-25  1.870500 -11.840127   4.309454             0  \n1993-02-01  1.871000  -4.233633 -16.980175             0  \n\n--- Dataframe after initial processing (df_initial) ---\n\u003cclass 'pandas.core.frame.DataFrame'\u003e\nDatetimeIndex: 1617 entries, 1993-01-04 to 2023-12-25\nData columns (total 10 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   kb               1617 non-null   float64\n 1   zos              1617 non-null   float64\n 2   salinity         1617 non-null   float64\n 3   water_temp       1617 non-null   float64\n 4   peace_discharge  1617 non-null   float64\n 5   peace_TN         1617 non-null   float64\n 6   peace_TP         1617 non-null   float64\n 7   wind_u           1617 non-null   float64\n 8   wind_v           1617 non-null   float64\n 9   bloom_target     1617 non-null   int64  \ndtypes: float64(9), int64(1)\nmemory usage: 139.0 KB\n"},"children":[],"identifier":"jamzelqjjs5m-outputs-0","html_id":"jamzelqjjs5m-outputs-0","key":"I4g58TkBcI"}],"identifier":"jamzelqjjs5m-outputs","html_id":"jamzelqjjs5m-outputs","key":"saQyOX4Kns"}],"identifier":"jamzelqjjs5m","label":"jaMZelqJjS5m","html_id":"jamzelqjjs5m","key":"LfNRbEvlNu"},{"type":"block","kind":"notebook-content","data":{"id":"Fdv8RABhjnY3"},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Part 3: Feature Engineering","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nKD6p8BNXo"}],"identifier":"part-3-feature-engineering","label":"Part 3: Feature Engineering","html_id":"part-3-feature-engineering","implicit":true,"key":"wc3vRchIqQ"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"This part defines and executes a function create_features that adds lagged features to the initially processed DataFrame (df_initial from Part 2). It uses the configuration flags set in Part 1 (USE_ENHANCED_FEATURES) to determine whether to create basic lags or the more detailed lags inspired by the Random Forest paper.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"OwxxT5nCz3"}],"key":"VhjHznVa00"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"This cell defines the feature engineering function and applies it to the df_initial DataFrame. It creates lagged versions of environmental variables and, optionally, more detailed lagged and rolling aggregate features for K. brevis counts.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"lt1ZNDLkha"}],"key":"H1qsAiIlfC"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Explanation:","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"uYcHTHqc6d"}],"key":"zfooZmL6Up"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":9,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"Configuration: It first defines or ensures access to the necessary configuration variables from Part 1 (like USE_ENHANCED_FEATURES, lag definitions, feature lists).","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"T0sEK9rMAP"}],"key":"QPrVpgVlLm"}],"key":"hDzZtpbY0V"},{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"create_features(...) Function:","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"FXj26S5pMy"}],"key":"YHwxCLkGqr"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":11,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Takes the initial DataFrame (df_initial) and configuration details as input.","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"uAha8EqNEQ"}],"key":"TNDgqITlUC"}],"key":"jlITcSJsg0"},{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Selects either BASIC_LAG_CONFIG or ENHANCED_LAG_CONFIG based on the USE_ENHANCED_FEATURES flag.","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"S0penYOZre"}],"key":"vHtzDNujHo"}],"key":"wTCovipVV8"},{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"KB Lags: Creates simple weekly lags (_L1, _L2) for the raw kb column.","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"P4uku8z7KB"}],"key":"XRC3EXl6tZ"}],"key":"V4lc62Iffc"},{"type":"listItem","spread":true,"position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"KB Rolling Aggregates (Enhanced Only): Calculates rolling mean (_M1_mean, _M2_mean, etc.) and rolling proportion of bloom weeks (_M1_prop, _M2_prop, etc.) over specified windows (e.g., 4, 8, 12 weeks), shifted back appropriately.","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"AZGwDknEmf"}],"key":"XuoJeT7PBO"}],"key":"VaTelzwEi1"},{"type":"listItem","spread":true,"position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Discharge Rolling Average (Enhanced Only): Calculates a rolling average (e.g., 4-week) for the primary discharge column, lagged by 1 week.","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"yudK4zHpkD"}],"key":"SeIElCniWn"}],"key":"XdUN7XVN1f"},{"type":"listItem","spread":true,"position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Environmental Lags: Creates simple weekly lags for all other specified environmental/hydrological variables (BASE_FEATURES). The number of lags depends on whether basic or enhanced mode is selected.","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"oFUnXE9hpo"}],"key":"Y6oMcCg0O2"}],"key":"WxQsquhuM9"},{"type":"listItem","spread":true,"position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"NaN Handling: Tracks the maximum lag introduced by any operation and drops the corresponding number of initial rows from the DataFrame to ensure sequences are complete.","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"xMEO9EqUMI"}],"key":"i2B6TXlngF"}],"key":"fkFysruacQ"},{"type":"listItem","spread":true,"position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"All-NaN Column Check: Includes a check to identify and optionally drop columns that might become entirely NaN after lagging (important for sparse data).","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"h8Di4l7qW6"}],"key":"x6fnMQULYI"}],"key":"oTvHvHF9nD"},{"type":"listItem","spread":true,"position":{"start":{"line":19,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Returns the final DataFrame (df_processed) with all engineered features.","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"dWjcP2i3Sv"}],"key":"OZvqt1qVC2"}],"key":"M4ExwgaP5g"}],"key":"apB6Ii7L8T"}],"key":"OvZOWTrGZx"},{"type":"listItem","spread":true,"position":{"start":{"line":21,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"Main Execution: Calls the create_features function with the appropriate arguments based on the configuration flags. Prints the head and tail of the resulting df_processed DataFrame.","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"LxCf7sM3PL"}],"key":"Mr8srXBJYT"}],"key":"IP45Ojleay"}],"key":"g6tpsJljKJ"},{"type":"paragraph","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"After running this cell, df_processed will contain the data ready for splitting (Part 4) and subsequent preprocessing/modeling steps. The number of columns will vary depending on whether basic or enhanced features were generated.","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"EpEPcAozo8"}],"key":"X2mi6xdRGZ"}],"identifier":"fdv8rabhjny3","label":"Fdv8RABhjnY3","html_id":"fdv8rabhjny3","key":"fFRljNfhZx"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"DswtKiJRjo8j","outputId":"472733a1-fd16-4a85-c5ff-3283a3109454","ExecuteTime":{"end_time":"2026-01-07T03:44:43.373949800Z","start_time":"2026-01-07T03:37:30.134123Z"}},"children":[{"type":"code","lang":"python","executable":true,"value":"# --- Configuration (Ensure these are defined from Part 1 or redefined here) ---\nTARGET_COL = 'kb'\nTARGET_BINARY_COL = 'bloom_target'\nUSE_ENHANCED_FEATURES = True # Set based on Part 1 config\n\n# Define lag configurations (can be pulled from Part 1 config)\nBASIC_LAG_CONFIG = {\n    'kb_lags': [1, 2],\n    'env_lags': list(range(1, 7))\n}\nENHANCED_LAG_CONFIG = {\n    'kb_lags': [1, 2],\n    'kb_rolling_windows': [(1, 4), (5, 8), (9, 12)], # (start_lag, end_lag)\n    'discharge_rolling_window': 4, # Window size for rolling avg discharge\n    'env_lags': [1] # Lags for other env vars in enhanced mode\n}\n# Features to lag (environmental/hydrological) - should include wind components if created\nBASE_FEATURES = [\n    'zos', 'water_temp',\n    'peace_discharge', 'peace_TN', 'peace_TP',\n    'wind_u', 'wind_v'\n]\n\n# --- Function Definition ---\ndef create_features(df, target_col, target_binary_col, base_features, use_enhanced=False, basic_cfg=None, enhanced_cfg=None):\n    \"\"\"Creates lagged and rolling features based on configuration.\"\"\"\n    if df is None:\n        print(\"Error in create_features: Input DataFrame is None.\")\n        return None\n\n    print(f\"\\n--- Creating Features (Enhanced Mode: {use_enhanced}) ---\")\n    df_featured = df.copy()\n    all_created_feature_cols = list(df.columns) # Start with existing columns\n\n    # Select config based on flag\n    if use_enhanced:\n        cfg = enhanced_cfg if enhanced_cfg else ENHANCED_LAG_CONFIG\n        env_lags = cfg.get('env_lags', [1])\n        kb_lags = cfg.get('kb_lags', [])\n        kb_rolling_windows = cfg.get('kb_rolling_windows', [])\n        discharge_rolling_window = cfg.get('discharge_rolling_window', None)\n        print(\"Using ENHANCED feature configuration.\")\n    else:\n        cfg = basic_cfg if basic_cfg else BASIC_LAG_CONFIG\n        env_lags = cfg.get('env_lags', [])\n        kb_lags = cfg.get('kb_lags', [])\n        kb_rolling_windows = [] # No rolling features in basic mode\n        discharge_rolling_window = None\n        print(\"Using BASIC feature configuration.\")\n\n    max_lag_needed = 0 # Track the maximum lag introduced\n\n    # --- Lagged K. brevis Features (Raw Counts) ---\n    if target_col in df.columns and kb_lags:\n        print(f\"Creating lagged features for target: {target_col}...\")\n        df[target_col] = pd.to_numeric(df[target_col], errors='coerce')\n        for lag in kb_lags:\n            col_name = f'{target_col}_L{lag}'\n            df_featured[col_name] = df[target_col].shift(lag)\n            all_created_feature_cols.append(col_name)\n            max_lag_needed = max(max_lag_needed, lag)\n        print(f\"  Created KB weekly lags: L{', L'.join(map(str, kb_lags))}\")\n    elif kb_lags:\n        print(f\"Warning: Target column '{target_col}' not found for lagging.\")\n\n    # --- Rolling K. brevis Features (Enhanced Mode Only) ---\n    if use_enhanced and kb_rolling_windows and target_col in df.columns and target_binary_col in df.columns:\n        print(\"Creating rolling aggregate features for KB...\")\n        target_binary_1e5 = df[target_binary_col]\n        monthly_lags_created = []\n        max_roll_lag = 0\n        for i, (start_lag, end_lag) in enumerate(kb_rolling_windows):\n            window_size = end_lag - start_lag + 1\n            month_lag_id = f'M{i+1}'\n            monthly_lags_created.append(month_lag_id)\n            max_roll_lag = max(max_roll_lag, end_lag) # Max lag needed for rolling window\n\n            # Mean of raw kb over the window, shifted back\n            col_name_mean = f'{target_col}_{month_lag_id}_mean'\n            df_featured[col_name_mean] = df[target_col].rolling(window=window_size, min_periods=1).mean().shift(start_lag)\n            all_created_feature_cols.append(col_name_mean)\n\n            # Proportion of bloom weeks (target=1) over the window, shifted back\n            col_name_prop = f'{target_binary_col}_{month_lag_id}_prop'\n            df_featured[col_name_prop] = target_binary_1e5.rolling(window=window_size, min_periods=1).mean().shift(start_lag)\n            all_created_feature_cols.append(col_name_prop)\n        print(f\"  Created approx KB monthly lags (mean, prop_bloom): {', '.join(monthly_lags_created)}\")\n        max_lag_needed = max(max_lag_needed, max_roll_lag) # Rolling needs lookback up to end_lag\n\n    # --- Rolling Discharge Feature (Enhanced Mode Only) ---\n    discharge_col = 'peace_discharge' # Or potentially 'caloos_Q' if prioritized/available\n    if use_enhanced and discharge_rolling_window and discharge_col in df.columns:\n         print(f\"Creating rolling average feature for {discharge_col}...\")\n         window_size = discharge_rolling_window\n         # Typically want average over past month, lagged by 1 week\n         start_lag = 1\n         col_name_discharge_roll = f'{discharge_col}_{window_size}w_avg_L{start_lag}'\n         df_featured[col_name_discharge_roll] = df[discharge_col].rolling(window=window_size, min_periods=1).mean().shift(start_lag)\n         all_created_feature_cols.append(col_name_discharge_roll)\n         max_lag_needed = max(max_lag_needed, window_size + start_lag -1) # Max lookback needed\n         print(f\"  Created {col_name_discharge_roll}\")\n\n\n    # --- Lagged Environmental/Hydrological Features ---\n    if env_lags:\n        print(\"\\nCreating lagged environmental/hydrological features...\")\n        lagged_env_cols_added_names = []\n        max_env_lag = 0\n        for feature in base_features:\n             if feature in df.columns:\n                df[feature] = pd.to_numeric(df[feature], errors='coerce') # Ensure numeric\n                for lag in env_lags:\n                    col_name = f'{feature}_L{lag}'\n                    df_featured[col_name] = df[feature].shift(lag)\n                    all_created_feature_cols.append(col_name)\n                    max_env_lag = max(max_env_lag, lag)\n                lagged_env_cols_added_names.append(feature)\n             else:\n                print(f\"  Warning: Feature '{feature}' not found for lagging.\")\n        if lagged_env_cols_added_names:\n            print(f\"  Lagged features created for: {', '.join(lagged_env_cols_added_names)} using lags L{', L'.join(map(str, env_lags))}\")\n        max_lag_needed = max(max_lag_needed, max_env_lag)\n\n    # --- Drop Rows with NaNs from Lagging/Rolling ---\n    print(f\"\\nMaximum lag/window introduced: {max_lag_needed} weeks.\")\n    initial_rows = len(df_featured)\n    if max_lag_needed \u003e 0:\n        df_processed = df_featured.iloc[max_lag_needed:].copy()\n        print(f\"Dropped first {max_lag_needed} rows due to NaNs generated by feature engineering.\")\n        print(f\"Shape after dropping initial NaNs: {df_processed.shape}\")\n    else:\n        df_processed = df_featured.copy()\n        print(\"No lagging applied or max lag was 0, no initial rows dropped.\")\n\n    # Final check for all-NaN columns (can happen with rolling/shifting if data is sparse)\n    all_nan_cols = df_processed.columns[df_processed.isnull().all()].tolist()\n    if all_nan_cols:\n        print(f\"\\nWarning: The following columns consist entirely of NaNs and will be dropped: {all_nan_cols}\")\n        df_processed = df_processed.dropna(axis=1, how='all')\n        print(f\"Shape after dropping all-NaN columns: {df_processed.shape}\")\n\n    # Ensure no duplicate columns (though unlikely with this structure)\n    df_processed = df_processed.loc[:,~df_processed.columns.duplicated()]\n\n    print(\"\\n--- Feature Engineering Complete ---\")\n    print(f\"Final number of columns (features + targets): {len(df_processed.columns)}\")\n    return df_processed\n\n# --- Execute Feature Engineering ---\n# Ensure df_initial exists from Part 2 and config variables from Part 1\nif __name__ == \"__main__\":\n    try:\n        if 'df_initial' in locals() and df_initial is not None:\n            # Determine which config to use based on the flag from Part 1\n            lag_config_to_use = ENHANCED_LAG_CONFIG if USE_ENHANCED_FEATURES else BASIC_LAG_CONFIG\n            features_to_lag_list = BASE_FEATURES # Modify if external data was added\n\n            df_processed = create_features(\n                df_initial,\n                TARGET_COL,\n                TARGET_BINARY_COL,\n                features_to_lag_list,\n                use_enhanced=USE_ENHANCED_FEATURES,\n                basic_cfg=BASIC_LAG_CONFIG,\n                enhanced_cfg=ENHANCED_LAG_CONFIG\n            )\n\n            if df_processed is not None:\n                print(\"\\nFirst 5 rows of processed data (df_processed):\")\n                display(df_processed.head())\n                print(\"\\nLast 5 rows:\")\n                display(df_processed.tail())\n                print(f\"\\nFinal shape of df_processed: {df_processed.shape}\")\n            else:\n                print(\"\\nFeature engineering failed.\")\n        else:\n            print(\"\\nError: df_initial not found or is None. Please run Part 2 first.\")\n\n    except NameError as ne:\n         print(f\"NameError: A configuration variable might be missing. Did Part 1 run correctly? Error: {ne}\")\n    except Exception as e:\n         print(f\"An unexpected error occurred in main execution block: {e}\")\n\n","identifier":"dswtkijrjo8j-code","enumerator":"7","html_id":"dswtkijrjo8j-code","key":"B3anASuEqo"},{"type":"outputs","id":"VPQczK_2lMEKFyeTT7tM9","children":[{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"\n--- Creating Features (Enhanced Mode: True) ---\nUsing ENHANCED feature configuration.\nCreating lagged features for target: kb...\n  Created KB weekly lags: L1, L2\nCreating rolling aggregate features for KB...\n  Created approx KB monthly lags (mean, prop_bloom): M1, M2, M3\nCreating rolling average feature for peace_discharge...\n  Created peace_discharge_4w_avg_L1\n\nCreating lagged environmental/hydrological features...\n  Lagged features created for: zos, water_temp, peace_discharge, peace_TN, peace_TP, wind_u, wind_v using lags L1\n\nMaximum lag/window introduced: 12 weeks.\nDropped first 12 rows due to NaNs generated by feature engineering.\nShape after dropping initial NaNs: (1605, 26)\n\n--- Feature Engineering Complete ---\nFinal number of columns (features + targets): 26\n\nFirst 5 rows of processed data (df_processed):\n"},"children":[],"identifier":"dswtkijrjo8j-outputs-0","html_id":"dswtkijrjo8j-outputs-0","key":"o0MBmOwEF5"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"             kb       zos   salinity  water_temp  peace_discharge  peace_TN  \\\ntime                                                                          \n1993-03-29  0.0 -0.060247  33.304348        26.2           2000.0      2.02   \n1993-04-05  0.0 -0.037195  33.326087        26.4           3290.0      5.68   \n1993-04-12  0.0 -0.008214  33.347826        26.3           3140.0      9.34   \n1993-04-19  0.0 -0.012612  33.369565        26.5           2040.0     13.00   \n1993-04-26  0.0 -0.034968  33.391304        26.9           1800.0      9.30   \n\n            peace_TP     wind_u     wind_v  bloom_target  ...  kb_M3_mean  \\\ntime                                                      ...               \n1993-03-29  1.504000 -11.346331   9.188078             0  ...      416.75   \n1993-04-05  1.442667 -17.383315  -2.753247             0  ...      333.50   \n1993-04-12  1.381333 -12.056565   9.419621             0  ...      166.75   \n1993-04-19  1.320000  -9.191863 -10.954436             0  ...        0.00   \n1993-04-26  1.440000 -12.943406   3.468175             0  ...        0.00   \n\n            bloom_target_M3_prop  peace_discharge_4w_avg_L1    zos_L1  \\\ntime                                                                    \n1993-03-29                   0.0                     1822.5 -0.061417   \n1993-04-05                   0.0                     2050.0 -0.060247   \n1993-04-12                   0.0                     2600.0 -0.037195   \n1993-04-19                   0.0                     2745.0 -0.008214   \n1993-04-26                   0.0                     2617.5 -0.012612   \n\n            water_temp_L1  peace_discharge_L1  peace_TN_L1  peace_TP_L1  \\\ntime                                                                      \n1993-03-29           25.5              2550.0        1.948     1.565333   \n1993-04-05           26.2              2000.0        2.020     1.504000   \n1993-04-12           26.4              3290.0        5.680     1.442667   \n1993-04-19           26.3              3140.0        9.340     1.381333   \n1993-04-26           26.5              2040.0       13.000     1.320000   \n\n            wind_u_L1  wind_v_L1  \ntime                              \n1993-03-29 -17.499440   5.685913  \n1993-04-05 -11.346331   9.188078  \n1993-04-12 -17.383315  -2.753247  \n1993-04-19 -12.056565   9.419621  \n1993-04-26  -9.191863 -10.954436  \n\n[5 rows x 26 columns]","content_type":"text/plain"},"text/html":{"content":"\n  \u003cdiv id=\"df-3a899256-df90-46c0-87fb-836f1b7e7467\" class=\"colab-df-container\"\u003e\n    \u003cdiv\u003e\n\u003cstyle scoped\u003e\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\u003c/style\u003e\n\u003ctable border=\"1\" class=\"dataframe\"\u003e\n  \u003cthead\u003e\n    \u003ctr style=\"text-align: right;\"\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003ekb\u003c/th\u003e\n      \u003cth\u003ezos\u003c/th\u003e\n      \u003cth\u003esalinity\u003c/th\u003e\n      \u003cth\u003ewater_temp\u003c/th\u003e\n      \u003cth\u003epeace_discharge\u003c/th\u003e\n      \u003cth\u003epeace_TN\u003c/th\u003e\n      \u003cth\u003epeace_TP\u003c/th\u003e\n      \u003cth\u003ewind_u\u003c/th\u003e\n      \u003cth\u003ewind_v\u003c/th\u003e\n      \u003cth\u003ebloom_target\u003c/th\u003e\n      \u003cth\u003e...\u003c/th\u003e\n      \u003cth\u003ekb_M3_mean\u003c/th\u003e\n      \u003cth\u003ebloom_target_M3_prop\u003c/th\u003e\n      \u003cth\u003epeace_discharge_4w_avg_L1\u003c/th\u003e\n      \u003cth\u003ezos_L1\u003c/th\u003e\n      \u003cth\u003ewater_temp_L1\u003c/th\u003e\n      \u003cth\u003epeace_discharge_L1\u003c/th\u003e\n      \u003cth\u003epeace_TN_L1\u003c/th\u003e\n      \u003cth\u003epeace_TP_L1\u003c/th\u003e\n      \u003cth\u003ewind_u_L1\u003c/th\u003e\n      \u003cth\u003ewind_v_L1\u003c/th\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003etime\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003cth\u003e1993-03-29\u003c/th\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e-0.060247\u003c/td\u003e\n      \u003ctd\u003e33.304348\u003c/td\u003e\n      \u003ctd\u003e26.2\u003c/td\u003e\n      \u003ctd\u003e2000.0\u003c/td\u003e\n      \u003ctd\u003e2.02\u003c/td\u003e\n      \u003ctd\u003e1.504000\u003c/td\u003e\n      \u003ctd\u003e-11.346331\u003c/td\u003e\n      \u003ctd\u003e9.188078\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e416.75\u003c/td\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e1822.5\u003c/td\u003e\n      \u003ctd\u003e-0.061417\u003c/td\u003e\n      \u003ctd\u003e25.5\u003c/td\u003e\n      \u003ctd\u003e2550.0\u003c/td\u003e\n      \u003ctd\u003e1.948\u003c/td\u003e\n      \u003ctd\u003e1.565333\u003c/td\u003e\n      \u003ctd\u003e-17.499440\u003c/td\u003e\n      \u003ctd\u003e5.685913\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e1993-04-05\u003c/th\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e-0.037195\u003c/td\u003e\n      \u003ctd\u003e33.326087\u003c/td\u003e\n      \u003ctd\u003e26.4\u003c/td\u003e\n      \u003ctd\u003e3290.0\u003c/td\u003e\n      \u003ctd\u003e5.68\u003c/td\u003e\n      \u003ctd\u003e1.442667\u003c/td\u003e\n      \u003ctd\u003e-17.383315\u003c/td\u003e\n      \u003ctd\u003e-2.753247\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e333.50\u003c/td\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e2050.0\u003c/td\u003e\n      \u003ctd\u003e-0.060247\u003c/td\u003e\n      \u003ctd\u003e26.2\u003c/td\u003e\n      \u003ctd\u003e2000.0\u003c/td\u003e\n      \u003ctd\u003e2.020\u003c/td\u003e\n      \u003ctd\u003e1.504000\u003c/td\u003e\n      \u003ctd\u003e-11.346331\u003c/td\u003e\n      \u003ctd\u003e9.188078\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e1993-04-12\u003c/th\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e-0.008214\u003c/td\u003e\n      \u003ctd\u003e33.347826\u003c/td\u003e\n      \u003ctd\u003e26.3\u003c/td\u003e\n      \u003ctd\u003e3140.0\u003c/td\u003e\n      \u003ctd\u003e9.34\u003c/td\u003e\n      \u003ctd\u003e1.381333\u003c/td\u003e\n      \u003ctd\u003e-12.056565\u003c/td\u003e\n      \u003ctd\u003e9.419621\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e166.75\u003c/td\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e2600.0\u003c/td\u003e\n      \u003ctd\u003e-0.037195\u003c/td\u003e\n      \u003ctd\u003e26.4\u003c/td\u003e\n      \u003ctd\u003e3290.0\u003c/td\u003e\n      \u003ctd\u003e5.680\u003c/td\u003e\n      \u003ctd\u003e1.442667\u003c/td\u003e\n      \u003ctd\u003e-17.383315\u003c/td\u003e\n      \u003ctd\u003e-2.753247\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e1993-04-19\u003c/th\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e-0.012612\u003c/td\u003e\n      \u003ctd\u003e33.369565\u003c/td\u003e\n      \u003ctd\u003e26.5\u003c/td\u003e\n      \u003ctd\u003e2040.0\u003c/td\u003e\n      \u003ctd\u003e13.00\u003c/td\u003e\n      \u003ctd\u003e1.320000\u003c/td\u003e\n      \u003ctd\u003e-9.191863\u003c/td\u003e\n      \u003ctd\u003e-10.954436\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e0.00\u003c/td\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e2745.0\u003c/td\u003e\n      \u003ctd\u003e-0.008214\u003c/td\u003e\n      \u003ctd\u003e26.3\u003c/td\u003e\n      \u003ctd\u003e3140.0\u003c/td\u003e\n      \u003ctd\u003e9.340\u003c/td\u003e\n      \u003ctd\u003e1.381333\u003c/td\u003e\n      \u003ctd\u003e-12.056565\u003c/td\u003e\n      \u003ctd\u003e9.419621\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e1993-04-26\u003c/th\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e-0.034968\u003c/td\u003e\n      \u003ctd\u003e33.391304\u003c/td\u003e\n      \u003ctd\u003e26.9\u003c/td\u003e\n      \u003ctd\u003e1800.0\u003c/td\u003e\n      \u003ctd\u003e9.30\u003c/td\u003e\n      \u003ctd\u003e1.440000\u003c/td\u003e\n      \u003ctd\u003e-12.943406\u003c/td\u003e\n      \u003ctd\u003e3.468175\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e0.00\u003c/td\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e2617.5\u003c/td\u003e\n      \u003ctd\u003e-0.012612\u003c/td\u003e\n      \u003ctd\u003e26.5\u003c/td\u003e\n      \u003ctd\u003e2040.0\u003c/td\u003e\n      \u003ctd\u003e13.000\u003c/td\u003e\n      \u003ctd\u003e1.320000\u003c/td\u003e\n      \u003ctd\u003e-9.191863\u003c/td\u003e\n      \u003ctd\u003e-10.954436\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e5 rows × 26 columns\u003c/p\u003e\n\u003c/div\u003e\n    \u003cdiv class=\"colab-df-buttons\"\u003e\n\n  \u003cdiv class=\"colab-df-container\"\u003e\n    \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3a899256-df90-46c0-87fb-836f1b7e7467')\"\n            title=\"Convert this dataframe to an interactive table.\"\n            style=\"display:none;\"\u003e\n\n  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\"\u003e\n    \u003cpath d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/\u003e\n  \u003c/svg\u003e\n    \u003c/button\u003e\n\n  \u003cstyle\u003e\n    .colab-df-container {\n      display:flex;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    .colab-df-buttons div {\n      margin-bottom: 4px;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  \u003c/style\u003e\n\n    \u003cscript\u003e\n      const buttonEl =\n        document.querySelector('#df-3a899256-df90-46c0-87fb-836f1b7e7467 button.colab-df-convert');\n      buttonEl.style.display =\n        google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n      async function convertToInteractive(key) {\n        const element = document.querySelector('#df-3a899256-df90-46c0-87fb-836f1b7e7467');\n        const dataTable =\n          await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                    [key], {});\n        if (!dataTable) return;\n\n        const docLinkHtml = 'Like what you see? Visit the ' +\n          '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n          + ' to learn more about interactive tables.';\n        element.innerHTML = '';\n        dataTable['output_type'] = 'display_data';\n        await google.colab.output.renderOutput(dataTable, element);\n        const docLink = document.createElement('div');\n        docLink.innerHTML = docLinkHtml;\n        element.appendChild(docLink);\n      }\n    \u003c/script\u003e\n  \u003c/div\u003e\n\n\n    \u003cdiv id=\"df-ddfb3322-816a-4f7d-b66d-f681ce2773d3\"\u003e\n      \u003cbutton class=\"colab-df-quickchart\" onclick=\"quickchart('df-ddfb3322-816a-4f7d-b66d-f681ce2773d3')\"\n                title=\"Suggest charts\"\n                style=\"display:none;\"\u003e\n\n\u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n     width=\"24px\"\u003e\n    \u003cg\u003e\n        \u003cpath d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/\u003e\n    \u003c/g\u003e\n\u003c/svg\u003e\n      \u003c/button\u003e\n\n\u003cstyle\u003e\n  .colab-df-quickchart {\n      --bg-color: #E8F0FE;\n      --fill-color: #1967D2;\n      --hover-bg-color: #E2EBFA;\n      --hover-fill-color: #174EA6;\n      --disabled-fill-color: #AAA;\n      --disabled-bg-color: #DDD;\n  }\n\n  [theme=dark] .colab-df-quickchart {\n      --bg-color: #3B4455;\n      --fill-color: #D2E3FC;\n      --hover-bg-color: #434B5C;\n      --hover-fill-color: #FFFFFF;\n      --disabled-bg-color: #3B4455;\n      --disabled-fill-color: #666;\n  }\n\n  .colab-df-quickchart {\n    background-color: var(--bg-color);\n    border: none;\n    border-radius: 50%;\n    cursor: pointer;\n    display: none;\n    fill: var(--fill-color);\n    height: 32px;\n    padding: 0;\n    width: 32px;\n  }\n\n  .colab-df-quickchart:hover {\n    background-color: var(--hover-bg-color);\n    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n    fill: var(--button-hover-fill-color);\n  }\n\n  .colab-df-quickchart-complete:disabled,\n  .colab-df-quickchart-complete:disabled:hover {\n    background-color: var(--disabled-bg-color);\n    fill: var(--disabled-fill-color);\n    box-shadow: none;\n  }\n\n  .colab-df-spinner {\n    border: 2px solid var(--fill-color);\n    border-color: transparent;\n    border-bottom-color: var(--fill-color);\n    animation:\n      spin 1s steps(1) infinite;\n  }\n\n  @keyframes spin {\n    0% {\n      border-color: transparent;\n      border-bottom-color: var(--fill-color);\n      border-left-color: var(--fill-color);\n    }\n    20% {\n      border-color: transparent;\n      border-left-color: var(--fill-color);\n      border-top-color: var(--fill-color);\n    }\n    30% {\n      border-color: transparent;\n      border-left-color: var(--fill-color);\n      border-top-color: var(--fill-color);\n      border-right-color: var(--fill-color);\n    }\n    40% {\n      border-color: transparent;\n      border-right-color: var(--fill-color);\n      border-top-color: var(--fill-color);\n    }\n    60% {\n      border-color: transparent;\n      border-right-color: var(--fill-color);\n    }\n    80% {\n      border-color: transparent;\n      border-right-color: var(--fill-color);\n      border-bottom-color: var(--fill-color);\n    }\n    90% {\n      border-color: transparent;\n      border-bottom-color: var(--fill-color);\n    }\n  }\n\u003c/style\u003e\n\n      \u003cscript\u003e\n        async function quickchart(key) {\n          const quickchartButtonEl =\n            document.querySelector('#' + key + ' button');\n          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n          quickchartButtonEl.classList.add('colab-df-spinner');\n          try {\n            const charts = await google.colab.kernel.invokeFunction(\n                'suggestCharts', [key], {});\n          } catch (error) {\n            console.error('Error during call to suggestCharts:', error);\n          }\n          quickchartButtonEl.classList.remove('colab-df-spinner');\n          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n        }\n        (() =\u003e {\n          let quickchartButtonEl =\n            document.querySelector('#df-ddfb3322-816a-4f7d-b66d-f681ce2773d3 button');\n          quickchartButtonEl.style.display =\n            google.colab.kernel.accessAllowed ? 'block' : 'none';\n        })();\n      \u003c/script\u003e\n    \u003c/div\u003e\n\n    \u003c/div\u003e\n  \u003c/div\u003e\n","content_type":"text/html"},"application/vnd.google.colaboratory.intrinsic+json":{"content":"{\"type\":\"dataframe\"}","content_type":"application/vnd.google.colaboratory.intrinsic+json"}}},"children":[],"identifier":"dswtkijrjo8j-outputs-1","html_id":"dswtkijrjo8j-outputs-1","key":"ItHnE9qGWk"},{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"\nLast 5 rows:\n"},"children":[],"identifier":"dswtkijrjo8j-outputs-2","html_id":"dswtkijrjo8j-outputs-2","key":"RkrPC4dool"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"             kb       zos  salinity  water_temp  peace_discharge  peace_TN  \\\ntime                                                                         \n2023-11-27  0.0 -0.005114      36.0   27.110714            326.0    2.0675   \n2023-12-04  0.0 -0.003981      36.0   26.992857            231.0    2.1800   \n2023-12-11  0.0 -0.009013      36.0   26.528571            201.0    1.8400   \n2023-12-18  0.0  0.005557      36.0   26.207143            460.0    2.2080   \n2023-12-25  0.0  0.001243      36.0   26.053571            470.0    2.2080   \n\n            peace_TP     wind_u    wind_v  bloom_target  ...  kb_M3_mean  \\\ntime                                                     ...               \n2023-11-27    0.8365 -11.482282 -2.065989             0  ...         0.0   \n2023-12-04    0.8000 -12.302635  0.314994             0  ...         0.0   \n2023-12-11    0.7900 -12.016729 -1.298361             0  ...         0.0   \n2023-12-18    0.9700 -11.410499  4.251085             0  ...         0.0   \n2023-12-25    0.9700 -13.387089  0.506480             0  ...         0.0   \n\n            bloom_target_M3_prop  peace_discharge_4w_avg_L1    zos_L1  \\\ntime                                                                    \n2023-11-27                   0.0                     496.25  0.022788   \n2023-12-04                   0.0                     392.25 -0.005114   \n2023-12-11                   0.0                     323.25 -0.003981   \n2023-12-18                   0.0                     287.50 -0.009013   \n2023-12-25                   0.0                     304.50  0.005557   \n\n            water_temp_L1  peace_discharge_L1  peace_TN_L1  peace_TP_L1  \\\ntime                                                                      \n2023-11-27      27.271429               392.0       1.9550       0.8730   \n2023-12-04      27.110714               326.0       2.0675       0.8365   \n2023-12-11      26.992857               231.0       2.1800       0.8000   \n2023-12-18      26.528571               201.0       1.8400       0.7900   \n2023-12-25      26.207143               460.0       2.2080       0.9700   \n\n            wind_u_L1  wind_v_L1  \ntime                              \n2023-11-27 -11.853374   4.605556  \n2023-12-04 -11.482282  -2.065989  \n2023-12-11 -12.302635   0.314994  \n2023-12-18 -12.016729  -1.298361  \n2023-12-25 -11.410499   4.251085  \n\n[5 rows x 26 columns]","content_type":"text/plain"},"text/html":{"content":"\n  \u003cdiv id=\"df-6262d544-c340-4b6b-9503-d9465d36cb77\" class=\"colab-df-container\"\u003e\n    \u003cdiv\u003e\n\u003cstyle scoped\u003e\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\u003c/style\u003e\n\u003ctable border=\"1\" class=\"dataframe\"\u003e\n  \u003cthead\u003e\n    \u003ctr style=\"text-align: right;\"\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003ekb\u003c/th\u003e\n      \u003cth\u003ezos\u003c/th\u003e\n      \u003cth\u003esalinity\u003c/th\u003e\n      \u003cth\u003ewater_temp\u003c/th\u003e\n      \u003cth\u003epeace_discharge\u003c/th\u003e\n      \u003cth\u003epeace_TN\u003c/th\u003e\n      \u003cth\u003epeace_TP\u003c/th\u003e\n      \u003cth\u003ewind_u\u003c/th\u003e\n      \u003cth\u003ewind_v\u003c/th\u003e\n      \u003cth\u003ebloom_target\u003c/th\u003e\n      \u003cth\u003e...\u003c/th\u003e\n      \u003cth\u003ekb_M3_mean\u003c/th\u003e\n      \u003cth\u003ebloom_target_M3_prop\u003c/th\u003e\n      \u003cth\u003epeace_discharge_4w_avg_L1\u003c/th\u003e\n      \u003cth\u003ezos_L1\u003c/th\u003e\n      \u003cth\u003ewater_temp_L1\u003c/th\u003e\n      \u003cth\u003epeace_discharge_L1\u003c/th\u003e\n      \u003cth\u003epeace_TN_L1\u003c/th\u003e\n      \u003cth\u003epeace_TP_L1\u003c/th\u003e\n      \u003cth\u003ewind_u_L1\u003c/th\u003e\n      \u003cth\u003ewind_v_L1\u003c/th\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003etime\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003cth\u003e2023-11-27\u003c/th\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e-0.005114\u003c/td\u003e\n      \u003ctd\u003e36.0\u003c/td\u003e\n      \u003ctd\u003e27.110714\u003c/td\u003e\n      \u003ctd\u003e326.0\u003c/td\u003e\n      \u003ctd\u003e2.0675\u003c/td\u003e\n      \u003ctd\u003e0.8365\u003c/td\u003e\n      \u003ctd\u003e-11.482282\u003c/td\u003e\n      \u003ctd\u003e-2.065989\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e496.25\u003c/td\u003e\n      \u003ctd\u003e0.022788\u003c/td\u003e\n      \u003ctd\u003e27.271429\u003c/td\u003e\n      \u003ctd\u003e392.0\u003c/td\u003e\n      \u003ctd\u003e1.9550\u003c/td\u003e\n      \u003ctd\u003e0.8730\u003c/td\u003e\n      \u003ctd\u003e-11.853374\u003c/td\u003e\n      \u003ctd\u003e4.605556\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e2023-12-04\u003c/th\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e-0.003981\u003c/td\u003e\n      \u003ctd\u003e36.0\u003c/td\u003e\n      \u003ctd\u003e26.992857\u003c/td\u003e\n      \u003ctd\u003e231.0\u003c/td\u003e\n      \u003ctd\u003e2.1800\u003c/td\u003e\n      \u003ctd\u003e0.8000\u003c/td\u003e\n      \u003ctd\u003e-12.302635\u003c/td\u003e\n      \u003ctd\u003e0.314994\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e392.25\u003c/td\u003e\n      \u003ctd\u003e-0.005114\u003c/td\u003e\n      \u003ctd\u003e27.110714\u003c/td\u003e\n      \u003ctd\u003e326.0\u003c/td\u003e\n      \u003ctd\u003e2.0675\u003c/td\u003e\n      \u003ctd\u003e0.8365\u003c/td\u003e\n      \u003ctd\u003e-11.482282\u003c/td\u003e\n      \u003ctd\u003e-2.065989\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e2023-12-11\u003c/th\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e-0.009013\u003c/td\u003e\n      \u003ctd\u003e36.0\u003c/td\u003e\n      \u003ctd\u003e26.528571\u003c/td\u003e\n      \u003ctd\u003e201.0\u003c/td\u003e\n      \u003ctd\u003e1.8400\u003c/td\u003e\n      \u003ctd\u003e0.7900\u003c/td\u003e\n      \u003ctd\u003e-12.016729\u003c/td\u003e\n      \u003ctd\u003e-1.298361\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e323.25\u003c/td\u003e\n      \u003ctd\u003e-0.003981\u003c/td\u003e\n      \u003ctd\u003e26.992857\u003c/td\u003e\n      \u003ctd\u003e231.0\u003c/td\u003e\n      \u003ctd\u003e2.1800\u003c/td\u003e\n      \u003ctd\u003e0.8000\u003c/td\u003e\n      \u003ctd\u003e-12.302635\u003c/td\u003e\n      \u003ctd\u003e0.314994\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e2023-12-18\u003c/th\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e0.005557\u003c/td\u003e\n      \u003ctd\u003e36.0\u003c/td\u003e\n      \u003ctd\u003e26.207143\u003c/td\u003e\n      \u003ctd\u003e460.0\u003c/td\u003e\n      \u003ctd\u003e2.2080\u003c/td\u003e\n      \u003ctd\u003e0.9700\u003c/td\u003e\n      \u003ctd\u003e-11.410499\u003c/td\u003e\n      \u003ctd\u003e4.251085\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e287.50\u003c/td\u003e\n      \u003ctd\u003e-0.009013\u003c/td\u003e\n      \u003ctd\u003e26.528571\u003c/td\u003e\n      \u003ctd\u003e201.0\u003c/td\u003e\n      \u003ctd\u003e1.8400\u003c/td\u003e\n      \u003ctd\u003e0.7900\u003c/td\u003e\n      \u003ctd\u003e-12.016729\u003c/td\u003e\n      \u003ctd\u003e-1.298361\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e2023-12-25\u003c/th\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e0.001243\u003c/td\u003e\n      \u003ctd\u003e36.0\u003c/td\u003e\n      \u003ctd\u003e26.053571\u003c/td\u003e\n      \u003ctd\u003e470.0\u003c/td\u003e\n      \u003ctd\u003e2.2080\u003c/td\u003e\n      \u003ctd\u003e0.9700\u003c/td\u003e\n      \u003ctd\u003e-13.387089\u003c/td\u003e\n      \u003ctd\u003e0.506480\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e304.50\u003c/td\u003e\n      \u003ctd\u003e0.005557\u003c/td\u003e\n      \u003ctd\u003e26.207143\u003c/td\u003e\n      \u003ctd\u003e460.0\u003c/td\u003e\n      \u003ctd\u003e2.2080\u003c/td\u003e\n      \u003ctd\u003e0.9700\u003c/td\u003e\n      \u003ctd\u003e-11.410499\u003c/td\u003e\n      \u003ctd\u003e4.251085\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e5 rows × 26 columns\u003c/p\u003e\n\u003c/div\u003e\n    \u003cdiv class=\"colab-df-buttons\"\u003e\n\n  \u003cdiv class=\"colab-df-container\"\u003e\n    \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6262d544-c340-4b6b-9503-d9465d36cb77')\"\n            title=\"Convert this dataframe to an interactive table.\"\n            style=\"display:none;\"\u003e\n\n  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\"\u003e\n    \u003cpath d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/\u003e\n  \u003c/svg\u003e\n    \u003c/button\u003e\n\n  \u003cstyle\u003e\n    .colab-df-container {\n      display:flex;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    .colab-df-buttons div {\n      margin-bottom: 4px;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  \u003c/style\u003e\n\n    \u003cscript\u003e\n      const buttonEl =\n        document.querySelector('#df-6262d544-c340-4b6b-9503-d9465d36cb77 button.colab-df-convert');\n      buttonEl.style.display =\n        google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n      async function convertToInteractive(key) {\n        const element = document.querySelector('#df-6262d544-c340-4b6b-9503-d9465d36cb77');\n        const dataTable =\n          await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                    [key], {});\n        if (!dataTable) return;\n\n        const docLinkHtml = 'Like what you see? Visit the ' +\n          '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n          + ' to learn more about interactive tables.';\n        element.innerHTML = '';\n        dataTable['output_type'] = 'display_data';\n        await google.colab.output.renderOutput(dataTable, element);\n        const docLink = document.createElement('div');\n        docLink.innerHTML = docLinkHtml;\n        element.appendChild(docLink);\n      }\n    \u003c/script\u003e\n  \u003c/div\u003e\n\n\n    \u003cdiv id=\"df-8c5c49e3-2b72-43f8-b42a-d958cf65b92c\"\u003e\n      \u003cbutton class=\"colab-df-quickchart\" onclick=\"quickchart('df-8c5c49e3-2b72-43f8-b42a-d958cf65b92c')\"\n                title=\"Suggest charts\"\n                style=\"display:none;\"\u003e\n\n\u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n     width=\"24px\"\u003e\n    \u003cg\u003e\n        \u003cpath d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/\u003e\n    \u003c/g\u003e\n\u003c/svg\u003e\n      \u003c/button\u003e\n\n\u003cstyle\u003e\n  .colab-df-quickchart {\n      --bg-color: #E8F0FE;\n      --fill-color: #1967D2;\n      --hover-bg-color: #E2EBFA;\n      --hover-fill-color: #174EA6;\n      --disabled-fill-color: #AAA;\n      --disabled-bg-color: #DDD;\n  }\n\n  [theme=dark] .colab-df-quickchart {\n      --bg-color: #3B4455;\n      --fill-color: #D2E3FC;\n      --hover-bg-color: #434B5C;\n      --hover-fill-color: #FFFFFF;\n      --disabled-bg-color: #3B4455;\n      --disabled-fill-color: #666;\n  }\n\n  .colab-df-quickchart {\n    background-color: var(--bg-color);\n    border: none;\n    border-radius: 50%;\n    cursor: pointer;\n    display: none;\n    fill: var(--fill-color);\n    height: 32px;\n    padding: 0;\n    width: 32px;\n  }\n\n  .colab-df-quickchart:hover {\n    background-color: var(--hover-bg-color);\n    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n    fill: var(--button-hover-fill-color);\n  }\n\n  .colab-df-quickchart-complete:disabled,\n  .colab-df-quickchart-complete:disabled:hover {\n    background-color: var(--disabled-bg-color);\n    fill: var(--disabled-fill-color);\n    box-shadow: none;\n  }\n\n  .colab-df-spinner {\n    border: 2px solid var(--fill-color);\n    border-color: transparent;\n    border-bottom-color: var(--fill-color);\n    animation:\n      spin 1s steps(1) infinite;\n  }\n\n  @keyframes spin {\n    0% {\n      border-color: transparent;\n      border-bottom-color: var(--fill-color);\n      border-left-color: var(--fill-color);\n    }\n    20% {\n      border-color: transparent;\n      border-left-color: var(--fill-color);\n      border-top-color: var(--fill-color);\n    }\n    30% {\n      border-color: transparent;\n      border-left-color: var(--fill-color);\n      border-top-color: var(--fill-color);\n      border-right-color: var(--fill-color);\n    }\n    40% {\n      border-color: transparent;\n      border-right-color: var(--fill-color);\n      border-top-color: var(--fill-color);\n    }\n    60% {\n      border-color: transparent;\n      border-right-color: var(--fill-color);\n    }\n    80% {\n      border-color: transparent;\n      border-right-color: var(--fill-color);\n      border-bottom-color: var(--fill-color);\n    }\n    90% {\n      border-color: transparent;\n      border-bottom-color: var(--fill-color);\n    }\n  }\n\u003c/style\u003e\n\n      \u003cscript\u003e\n        async function quickchart(key) {\n          const quickchartButtonEl =\n            document.querySelector('#' + key + ' button');\n          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n          quickchartButtonEl.classList.add('colab-df-spinner');\n          try {\n            const charts = await google.colab.kernel.invokeFunction(\n                'suggestCharts', [key], {});\n          } catch (error) {\n            console.error('Error during call to suggestCharts:', error);\n          }\n          quickchartButtonEl.classList.remove('colab-df-spinner');\n          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n        }\n        (() =\u003e {\n          let quickchartButtonEl =\n            document.querySelector('#df-8c5c49e3-2b72-43f8-b42a-d958cf65b92c button');\n          quickchartButtonEl.style.display =\n            google.colab.kernel.accessAllowed ? 'block' : 'none';\n        })();\n      \u003c/script\u003e\n    \u003c/div\u003e\n\n    \u003c/div\u003e\n  \u003c/div\u003e\n","content_type":"text/html"},"application/vnd.google.colaboratory.intrinsic+json":{"content":"{\"type\":\"dataframe\"}","content_type":"application/vnd.google.colaboratory.intrinsic+json"}}},"children":[],"identifier":"dswtkijrjo8j-outputs-3","html_id":"dswtkijrjo8j-outputs-3","key":"CK8w1Ff6n6"},{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"\nFinal shape of df_processed: (1605, 26)\n"},"children":[],"identifier":"dswtkijrjo8j-outputs-4","html_id":"dswtkijrjo8j-outputs-4","key":"lMI6Ucwoaj"}],"identifier":"dswtkijrjo8j-outputs","html_id":"dswtkijrjo8j-outputs","key":"UTO9ns0XPt"}],"identifier":"dswtkijrjo8j","label":"DswtKiJRjo8j","html_id":"dswtkijrjo8j","key":"Ugit29gejS"},{"type":"block","kind":"notebook-content","data":{"id":"SxrAEziyj0El"},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Part 4: Data Splitting (Chronological)","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JGuIQWQc3H"}],"identifier":"part-4-data-splitting-chronological","label":"Part 4: Data Splitting (Chronological)","html_id":"part-4-data-splitting-chronological","implicit":true,"key":"Nfjw4apPL8"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"This part defines and executes a function to split the feature-engineered DataFrame (df_processed from Part 3) into training, validation, and testing sets based on time. A chronological split is essential for time-series data to ensure the model is trained on past data and evaluated on future data, simulating a real-world forecasting scenario.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"P0Fo37D8uA"}],"key":"c4vAJc6oq7"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"This cell defines the split_data_chronological function and applies it to df_processed. It uses the TRAIN_SPLIT_RATIO and VALIDATION_SPLIT_RATIO defined in Part 1 to divide the data.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"KfQTRCLj6W"}],"key":"Np3qxIoBYF"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Explanation:","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"TdKL3fAQwp"}],"key":"UF4Y0FpGxg"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":8,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"Function split_data_chronological:","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"A44vfvH690"}],"key":"JOw2HkUakA"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":9,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Takes the feature-engineered DataFrame (df_processed) and the desired train/validation ratios as input.","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"uYazwLIrGp"}],"key":"ZwWVrD6GBO"}],"key":"CvvGInunzG"},{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Includes checks to ensure the input DataFrame is valid and has a datetime index.","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"ZXX1lcJrQg"}],"key":"eNm0GQPPZ3"}],"key":"onhhYrkjua"},{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Calculates the number of samples for each set based on the ratios.","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"ueYdFkORiZ"}],"key":"u0FEAt6Lll"}],"key":"sIrSPIoeip"},{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Performs the split using integer-location based indexing (iloc) which respects the chronological order. .copy() is used to avoid potential SettingWithCopyWarning later.","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"leTJk9vtkk"}],"key":"rKzv7IS7BM"}],"key":"av7RqDW7lh"},{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Prints the size and date range of each resulting subset.","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"JelnvNcXbC"}],"key":"MOVazJK9EW"}],"key":"R9Lhny68pA"},{"type":"listItem","spread":true,"position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Includes an assertion to double-check that the splits don’t overlap.","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"JuEFzbMzxV"}],"key":"wiAe04v17i"}],"key":"JyRlSYkXKN"},{"type":"listItem","spread":true,"position":{"start":{"line":15,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Returns the three DataFrames: train_df, validation_df, test_df.","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"Nzi4LjX0pJ"}],"key":"YqXzZZKj0w"}],"key":"oTyiUmelvc"}],"key":"j6gezswgNI"}],"key":"HVIvLQXner"},{"type":"listItem","spread":true,"position":{"start":{"line":17,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"text","value":"Main Execution:","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"Esy1RZP6vp"}],"key":"u0pFXLiUYv"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":18,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Calls the function using df_processed (output of Part 3) and the ratios defined in the configuration (Part 1).","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"dQATuGpl4l"}],"key":"uwUvgk328y"}],"key":"k0H68rCrbU"},{"type":"listItem","spread":true,"position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Stores the results in train_df, validation_df, and test_df. These variables will be used in subsequent steps (scaling, sequence creation, EnKF).","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"btDr0Y344D"}],"key":"YDgAxRFPhh"}],"key":"QovehXeuzG"},{"type":"listItem","spread":true,"position":{"start":{"line":20,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Prints the head of train_df for verification.","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"WETpHWgSOO"}],"key":"vWKJxXyT8U"}],"key":"LTvzOI1wsw"}],"key":"WJJICQZMIh"}],"key":"rTpik6ake8"}],"key":"F6TkFjXcY1"},{"type":"paragraph","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"text","value":"After running this cell, you will have the data divided into the necessary subsets for training, validation (tuning/early stopping), and final testing, while preserving the temporal order. The next step (Part 5) will handle imputation and scaling, fitting the necessary objects only on train_df.","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"amdhftfh5R"}],"key":"PoerYyEBat"}],"identifier":"sxraeziyj0el","label":"SxrAEziyj0El","html_id":"sxraeziyj0el","key":"PbTYr9tRpl"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/","height":565},"id":"47ChAVmSjxe-","outputId":"54bdab40-61fe-4da3-b09b-f499bebb33f5","ExecuteTime":{"end_time":"2026-01-07T03:44:43.374949200Z","start_time":"2026-01-07T03:37:30.211789Z"}},"children":[{"type":"code","lang":"python","executable":true,"value":"# --- Configuration (Ensure these are defined from Part 1 or redefined here) ---\nTRAIN_SPLIT_RATIO = 0.70\nVALIDATION_SPLIT_RATIO = 0.15\n# Test split ratio is implicitly calculated\n\n# --- Function Definition ---\ndef split_data_chronological(df, train_ratio, val_ratio):\n    \"\"\"\n    Splits the DataFrame chronologically into train, validation, and test sets.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame with a datetime index, sorted chronologically.\n        train_ratio (float): The proportion of data to use for training (e.g., 0.7).\n        val_ratio (float): The proportion of data to use for validation (e.g., 0.15).\n\n    Returns:\n        tuple: A tuple containing train_df, validation_df, test_df (pd.DataFrames),\n               or (None, None, None) if splitting fails.\n    \"\"\"\n    if df is None or df.empty:\n        print(\"Error in split_data_chronological: Input DataFrame is None or empty.\")\n        return None, None, None\n    if not isinstance(df.index, pd.DatetimeIndex):\n         print(\"Error: DataFrame index must be a DatetimeIndex.\")\n         return None, None, None\n    if not df.index.is_monotonic_increasing:\n         print(\"Warning: DataFrame index is not sorted chronologically. Sorting now...\")\n         df = df.sort_index()\n\n    print(\"\\n--- Splitting Data into Train, Validation, and Test Sets (Chronological) ---\")\n    n_total = len(df)\n    n_train = int(n_total * train_ratio)\n    n_validation = int(n_total * val_ratio)\n    n_test = n_total - n_train - n_validation\n\n    # Ensure calculated splits are valid\n    if n_train \u003c= 0 or n_validation \u003c= 0 or n_test \u003c= 0:\n         print(f\"Error: Not enough data ({n_total} rows) to create non-empty train/validation/test splits.\")\n         print(f\"Calculated splits: Train={n_train}, Val={n_validation}, Test={n_test}\")\n         return None, None, None\n    if n_train + n_validation + n_test != n_total:\n         print(\"Error: Split ratios do not sum correctly.\")\n         return None, None, None\n\n    # Perform chronological split using iloc\n    train_df = df.iloc[:n_train].copy()\n    validation_df = df.iloc[n_train : n_train + n_validation].copy()\n    test_df = df.iloc[n_train + n_validation :].copy() # Takes the rest\n\n    print(f\"Total samples: {n_total}\")\n    print(f\"Training set:   {len(train_df)} samples (Index: {train_df.index.min()} to {train_df.index.max()})\")\n    print(f\"Validation set: {len(validation_df)} samples (Index: {validation_df.index.min()} to {validation_df.index.max()})\")\n    print(f\"Test set:       {len(test_df)} samples (Index: {test_df.index.min()} to {test_df.index.max()})\")\n\n    # Basic check for overlap\n    assert train_df.index.max() \u003c validation_df.index.min(), \"Train/Validation sets overlap!\"\n    assert validation_df.index.max() \u003c test_df.index.min(), \"Validation/Test sets overlap!\"\n\n    print(\"Data splitting complete.\")\n    return train_df, validation_df, test_df\n\n# --- Execute Data Splitting ---\n# Ensure df_processed exists from Part 3\nif __name__ == \"__main__\":\n    try:\n        if 'df_processed' in locals() and df_processed is not None:\n            # Use ratios defined in Part 1 config\n            train_df, validation_df, test_df = split_data_chronological(\n                df_processed,\n                TRAIN_SPLIT_RATIO,\n                VALIDATION_SPLIT_RATIO\n            )\n\n            if train_df is not None:\n                # Display head of training data as confirmation\n                print(\"\\nHead of Training Data (train_df):\")\n                display(train_df.head())\n                # Keep test_df (unscaled) for later use with EnKF\n                print(\"\\nUnscaled test_df also created for EnKF observations.\")\n            else:\n                print(\"\\nData splitting failed.\")\n        else:\n            print(\"\\nError: df_processed not found or is None. Please run Part 3 first.\")\n            # Define placeholders to prevent errors if run out of order in notebook\n            train_df, validation_df, test_df = None, None, None\n\n    except NameError as ne:\n         print(f\"NameError: A configuration variable might be missing. Did Part 1 run correctly? Error: {ne}\")\n    except Exception as e:\n         print(f\"An unexpected error occurred in main execution block: {e}\")\n         train_df, validation_df, test_df = None, None, None\n\n","identifier":"47chavmsjxe--code","enumerator":"8","html_id":"id-47chavmsjxe-code","key":"pQRYwlCrjD"},{"type":"outputs","id":"xtRDyJFT6jhVq9K15J9Ju","children":[{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"\n--- Splitting Data into Train, Validation, and Test Sets (Chronological) ---\nTotal samples: 1605\nTraining set:   1123 samples (Index: 1993-03-29 00:00:00 to 2014-09-29 00:00:00)\nValidation set: 240 samples (Index: 2014-10-06 00:00:00 to 2019-05-06 00:00:00)\nTest set:       242 samples (Index: 2019-05-13 00:00:00 to 2023-12-25 00:00:00)\nData splitting complete.\n\nHead of Training Data (train_df):\n"},"children":[],"identifier":"47chavmsjxe--outputs-0","html_id":"id-47chavmsjxe-outputs-0","key":"M7REft4x2N"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"             kb       zos   salinity  water_temp  peace_discharge  peace_TN  \\\ntime                                                                          \n1993-03-29  0.0 -0.060247  33.304348        26.2           2000.0      2.02   \n1993-04-05  0.0 -0.037195  33.326087        26.4           3290.0      5.68   \n1993-04-12  0.0 -0.008214  33.347826        26.3           3140.0      9.34   \n1993-04-19  0.0 -0.012612  33.369565        26.5           2040.0     13.00   \n1993-04-26  0.0 -0.034968  33.391304        26.9           1800.0      9.30   \n\n            peace_TP     wind_u     wind_v  bloom_target  ...  kb_M3_mean  \\\ntime                                                      ...               \n1993-03-29  1.504000 -11.346331   9.188078             0  ...      416.75   \n1993-04-05  1.442667 -17.383315  -2.753247             0  ...      333.50   \n1993-04-12  1.381333 -12.056565   9.419621             0  ...      166.75   \n1993-04-19  1.320000  -9.191863 -10.954436             0  ...        0.00   \n1993-04-26  1.440000 -12.943406   3.468175             0  ...        0.00   \n\n            bloom_target_M3_prop  peace_discharge_4w_avg_L1    zos_L1  \\\ntime                                                                    \n1993-03-29                   0.0                     1822.5 -0.061417   \n1993-04-05                   0.0                     2050.0 -0.060247   \n1993-04-12                   0.0                     2600.0 -0.037195   \n1993-04-19                   0.0                     2745.0 -0.008214   \n1993-04-26                   0.0                     2617.5 -0.012612   \n\n            water_temp_L1  peace_discharge_L1  peace_TN_L1  peace_TP_L1  \\\ntime                                                                      \n1993-03-29           25.5              2550.0        1.948     1.565333   \n1993-04-05           26.2              2000.0        2.020     1.504000   \n1993-04-12           26.4              3290.0        5.680     1.442667   \n1993-04-19           26.3              3140.0        9.340     1.381333   \n1993-04-26           26.5              2040.0       13.000     1.320000   \n\n            wind_u_L1  wind_v_L1  \ntime                              \n1993-03-29 -17.499440   5.685913  \n1993-04-05 -11.346331   9.188078  \n1993-04-12 -17.383315  -2.753247  \n1993-04-19 -12.056565   9.419621  \n1993-04-26  -9.191863 -10.954436  \n\n[5 rows x 26 columns]","content_type":"text/plain"},"text/html":{"content":"\n  \u003cdiv id=\"df-ce5625cc-39f0-466e-b119-bea6494370d2\" class=\"colab-df-container\"\u003e\n    \u003cdiv\u003e\n\u003cstyle scoped\u003e\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\u003c/style\u003e\n\u003ctable border=\"1\" class=\"dataframe\"\u003e\n  \u003cthead\u003e\n    \u003ctr style=\"text-align: right;\"\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003ekb\u003c/th\u003e\n      \u003cth\u003ezos\u003c/th\u003e\n      \u003cth\u003esalinity\u003c/th\u003e\n      \u003cth\u003ewater_temp\u003c/th\u003e\n      \u003cth\u003epeace_discharge\u003c/th\u003e\n      \u003cth\u003epeace_TN\u003c/th\u003e\n      \u003cth\u003epeace_TP\u003c/th\u003e\n      \u003cth\u003ewind_u\u003c/th\u003e\n      \u003cth\u003ewind_v\u003c/th\u003e\n      \u003cth\u003ebloom_target\u003c/th\u003e\n      \u003cth\u003e...\u003c/th\u003e\n      \u003cth\u003ekb_M3_mean\u003c/th\u003e\n      \u003cth\u003ebloom_target_M3_prop\u003c/th\u003e\n      \u003cth\u003epeace_discharge_4w_avg_L1\u003c/th\u003e\n      \u003cth\u003ezos_L1\u003c/th\u003e\n      \u003cth\u003ewater_temp_L1\u003c/th\u003e\n      \u003cth\u003epeace_discharge_L1\u003c/th\u003e\n      \u003cth\u003epeace_TN_L1\u003c/th\u003e\n      \u003cth\u003epeace_TP_L1\u003c/th\u003e\n      \u003cth\u003ewind_u_L1\u003c/th\u003e\n      \u003cth\u003ewind_v_L1\u003c/th\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003etime\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003e\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003cth\u003e1993-03-29\u003c/th\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e-0.060247\u003c/td\u003e\n      \u003ctd\u003e33.304348\u003c/td\u003e\n      \u003ctd\u003e26.2\u003c/td\u003e\n      \u003ctd\u003e2000.0\u003c/td\u003e\n      \u003ctd\u003e2.02\u003c/td\u003e\n      \u003ctd\u003e1.504000\u003c/td\u003e\n      \u003ctd\u003e-11.346331\u003c/td\u003e\n      \u003ctd\u003e9.188078\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e416.75\u003c/td\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e1822.5\u003c/td\u003e\n      \u003ctd\u003e-0.061417\u003c/td\u003e\n      \u003ctd\u003e25.5\u003c/td\u003e\n      \u003ctd\u003e2550.0\u003c/td\u003e\n      \u003ctd\u003e1.948\u003c/td\u003e\n      \u003ctd\u003e1.565333\u003c/td\u003e\n      \u003ctd\u003e-17.499440\u003c/td\u003e\n      \u003ctd\u003e5.685913\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e1993-04-05\u003c/th\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e-0.037195\u003c/td\u003e\n      \u003ctd\u003e33.326087\u003c/td\u003e\n      \u003ctd\u003e26.4\u003c/td\u003e\n      \u003ctd\u003e3290.0\u003c/td\u003e\n      \u003ctd\u003e5.68\u003c/td\u003e\n      \u003ctd\u003e1.442667\u003c/td\u003e\n      \u003ctd\u003e-17.383315\u003c/td\u003e\n      \u003ctd\u003e-2.753247\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e333.50\u003c/td\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e2050.0\u003c/td\u003e\n      \u003ctd\u003e-0.060247\u003c/td\u003e\n      \u003ctd\u003e26.2\u003c/td\u003e\n      \u003ctd\u003e2000.0\u003c/td\u003e\n      \u003ctd\u003e2.020\u003c/td\u003e\n      \u003ctd\u003e1.504000\u003c/td\u003e\n      \u003ctd\u003e-11.346331\u003c/td\u003e\n      \u003ctd\u003e9.188078\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e1993-04-12\u003c/th\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e-0.008214\u003c/td\u003e\n      \u003ctd\u003e33.347826\u003c/td\u003e\n      \u003ctd\u003e26.3\u003c/td\u003e\n      \u003ctd\u003e3140.0\u003c/td\u003e\n      \u003ctd\u003e9.34\u003c/td\u003e\n      \u003ctd\u003e1.381333\u003c/td\u003e\n      \u003ctd\u003e-12.056565\u003c/td\u003e\n      \u003ctd\u003e9.419621\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e166.75\u003c/td\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e2600.0\u003c/td\u003e\n      \u003ctd\u003e-0.037195\u003c/td\u003e\n      \u003ctd\u003e26.4\u003c/td\u003e\n      \u003ctd\u003e3290.0\u003c/td\u003e\n      \u003ctd\u003e5.680\u003c/td\u003e\n      \u003ctd\u003e1.442667\u003c/td\u003e\n      \u003ctd\u003e-17.383315\u003c/td\u003e\n      \u003ctd\u003e-2.753247\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e1993-04-19\u003c/th\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e-0.012612\u003c/td\u003e\n      \u003ctd\u003e33.369565\u003c/td\u003e\n      \u003ctd\u003e26.5\u003c/td\u003e\n      \u003ctd\u003e2040.0\u003c/td\u003e\n      \u003ctd\u003e13.00\u003c/td\u003e\n      \u003ctd\u003e1.320000\u003c/td\u003e\n      \u003ctd\u003e-9.191863\u003c/td\u003e\n      \u003ctd\u003e-10.954436\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e0.00\u003c/td\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e2745.0\u003c/td\u003e\n      \u003ctd\u003e-0.008214\u003c/td\u003e\n      \u003ctd\u003e26.3\u003c/td\u003e\n      \u003ctd\u003e3140.0\u003c/td\u003e\n      \u003ctd\u003e9.340\u003c/td\u003e\n      \u003ctd\u003e1.381333\u003c/td\u003e\n      \u003ctd\u003e-12.056565\u003c/td\u003e\n      \u003ctd\u003e9.419621\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e1993-04-26\u003c/th\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e-0.034968\u003c/td\u003e\n      \u003ctd\u003e33.391304\u003c/td\u003e\n      \u003ctd\u003e26.9\u003c/td\u003e\n      \u003ctd\u003e1800.0\u003c/td\u003e\n      \u003ctd\u003e9.30\u003c/td\u003e\n      \u003ctd\u003e1.440000\u003c/td\u003e\n      \u003ctd\u003e-12.943406\u003c/td\u003e\n      \u003ctd\u003e3.468175\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e0.00\u003c/td\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e2617.5\u003c/td\u003e\n      \u003ctd\u003e-0.012612\u003c/td\u003e\n      \u003ctd\u003e26.5\u003c/td\u003e\n      \u003ctd\u003e2040.0\u003c/td\u003e\n      \u003ctd\u003e13.000\u003c/td\u003e\n      \u003ctd\u003e1.320000\u003c/td\u003e\n      \u003ctd\u003e-9.191863\u003c/td\u003e\n      \u003ctd\u003e-10.954436\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e5 rows × 26 columns\u003c/p\u003e\n\u003c/div\u003e\n    \u003cdiv class=\"colab-df-buttons\"\u003e\n\n  \u003cdiv class=\"colab-df-container\"\u003e\n    \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ce5625cc-39f0-466e-b119-bea6494370d2')\"\n            title=\"Convert this dataframe to an interactive table.\"\n            style=\"display:none;\"\u003e\n\n  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\"\u003e\n    \u003cpath d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/\u003e\n  \u003c/svg\u003e\n    \u003c/button\u003e\n\n  \u003cstyle\u003e\n    .colab-df-container {\n      display:flex;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    .colab-df-buttons div {\n      margin-bottom: 4px;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  \u003c/style\u003e\n\n    \u003cscript\u003e\n      const buttonEl =\n        document.querySelector('#df-ce5625cc-39f0-466e-b119-bea6494370d2 button.colab-df-convert');\n      buttonEl.style.display =\n        google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n      async function convertToInteractive(key) {\n        const element = document.querySelector('#df-ce5625cc-39f0-466e-b119-bea6494370d2');\n        const dataTable =\n          await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                    [key], {});\n        if (!dataTable) return;\n\n        const docLinkHtml = 'Like what you see? Visit the ' +\n          '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n          + ' to learn more about interactive tables.';\n        element.innerHTML = '';\n        dataTable['output_type'] = 'display_data';\n        await google.colab.output.renderOutput(dataTable, element);\n        const docLink = document.createElement('div');\n        docLink.innerHTML = docLinkHtml;\n        element.appendChild(docLink);\n      }\n    \u003c/script\u003e\n  \u003c/div\u003e\n\n\n    \u003cdiv id=\"df-c8632e87-f471-42ee-b02e-c6b1b5503f80\"\u003e\n      \u003cbutton class=\"colab-df-quickchart\" onclick=\"quickchart('df-c8632e87-f471-42ee-b02e-c6b1b5503f80')\"\n                title=\"Suggest charts\"\n                style=\"display:none;\"\u003e\n\n\u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n     width=\"24px\"\u003e\n    \u003cg\u003e\n        \u003cpath d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/\u003e\n    \u003c/g\u003e\n\u003c/svg\u003e\n      \u003c/button\u003e\n\n\u003cstyle\u003e\n  .colab-df-quickchart {\n      --bg-color: #E8F0FE;\n      --fill-color: #1967D2;\n      --hover-bg-color: #E2EBFA;\n      --hover-fill-color: #174EA6;\n      --disabled-fill-color: #AAA;\n      --disabled-bg-color: #DDD;\n  }\n\n  [theme=dark] .colab-df-quickchart {\n      --bg-color: #3B4455;\n      --fill-color: #D2E3FC;\n      --hover-bg-color: #434B5C;\n      --hover-fill-color: #FFFFFF;\n      --disabled-bg-color: #3B4455;\n      --disabled-fill-color: #666;\n  }\n\n  .colab-df-quickchart {\n    background-color: var(--bg-color);\n    border: none;\n    border-radius: 50%;\n    cursor: pointer;\n    display: none;\n    fill: var(--fill-color);\n    height: 32px;\n    padding: 0;\n    width: 32px;\n  }\n\n  .colab-df-quickchart:hover {\n    background-color: var(--hover-bg-color);\n    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n    fill: var(--button-hover-fill-color);\n  }\n\n  .colab-df-quickchart-complete:disabled,\n  .colab-df-quickchart-complete:disabled:hover {\n    background-color: var(--disabled-bg-color);\n    fill: var(--disabled-fill-color);\n    box-shadow: none;\n  }\n\n  .colab-df-spinner {\n    border: 2px solid var(--fill-color);\n    border-color: transparent;\n    border-bottom-color: var(--fill-color);\n    animation:\n      spin 1s steps(1) infinite;\n  }\n\n  @keyframes spin {\n    0% {\n      border-color: transparent;\n      border-bottom-color: var(--fill-color);\n      border-left-color: var(--fill-color);\n    }\n    20% {\n      border-color: transparent;\n      border-left-color: var(--fill-color);\n      border-top-color: var(--fill-color);\n    }\n    30% {\n      border-color: transparent;\n      border-left-color: var(--fill-color);\n      border-top-color: var(--fill-color);\n      border-right-color: var(--fill-color);\n    }\n    40% {\n      border-color: transparent;\n      border-right-color: var(--fill-color);\n      border-top-color: var(--fill-color);\n    }\n    60% {\n      border-color: transparent;\n      border-right-color: var(--fill-color);\n    }\n    80% {\n      border-color: transparent;\n      border-right-color: var(--fill-color);\n      border-bottom-color: var(--fill-color);\n    }\n    90% {\n      border-color: transparent;\n      border-bottom-color: var(--fill-color);\n    }\n  }\n\u003c/style\u003e\n\n      \u003cscript\u003e\n        async function quickchart(key) {\n          const quickchartButtonEl =\n            document.querySelector('#' + key + ' button');\n          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n          quickchartButtonEl.classList.add('colab-df-spinner');\n          try {\n            const charts = await google.colab.kernel.invokeFunction(\n                'suggestCharts', [key], {});\n          } catch (error) {\n            console.error('Error during call to suggestCharts:', error);\n          }\n          quickchartButtonEl.classList.remove('colab-df-spinner');\n          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n        }\n        (() =\u003e {\n          let quickchartButtonEl =\n            document.querySelector('#df-c8632e87-f471-42ee-b02e-c6b1b5503f80 button');\n          quickchartButtonEl.style.display =\n            google.colab.kernel.accessAllowed ? 'block' : 'none';\n        })();\n      \u003c/script\u003e\n    \u003c/div\u003e\n\n    \u003c/div\u003e\n  \u003c/div\u003e\n","content_type":"text/html"},"application/vnd.google.colaboratory.intrinsic+json":{"content":"{\"type\":\"dataframe\"}","content_type":"application/vnd.google.colaboratory.intrinsic+json"}}},"children":[],"identifier":"47chavmsjxe--outputs-1","html_id":"id-47chavmsjxe-outputs-1","key":"twLr2d6Wof"},{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"\nUnscaled test_df also created for EnKF observations.\n"},"children":[],"identifier":"47chavmsjxe--outputs-2","html_id":"id-47chavmsjxe-outputs-2","key":"syjoA5wvsp"}],"identifier":"47chavmsjxe--outputs","html_id":"id-47chavmsjxe-outputs","key":"GAUBO4SEYI"}],"identifier":"47chavmsjxe-","label":"47ChAVmSjxe-","html_id":"id-47chavmsjxe","key":"kg54Usxec4"},{"type":"block","kind":"notebook-content","data":{"id":"zJZ8x96ukDHt"},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Part 5: Preprocessing (Imputation \u0026 Scaling)","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"J76k79XxzI"}],"identifier":"part-5-preprocessing-imputation-scaling","label":"Part 5: Preprocessing (Imputation \u0026 Scaling)","html_id":"part-5-preprocessing-imputation-scaling","implicit":true,"key":"PpRqugTkqs"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"This step takes the chronologically split dataframes (train_df, validation_df, test_df from Part 4) and performs two crucial preprocessing tasks:","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"a923x4iYiO"}],"key":"V3t8S19T9F"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Imputation: Handles any remaining missing values (NaNs) in the feature columns. It’s vital to fit the imputer only on the training data.\nScaling: Scales the numerical features using the scaler type specified in the configuration (RobustScaler or StandardScaler). The scaler is fit only on the training data.\nThe fitted scaler and the final list of feature columns used are saved.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"rdIQMbrnRf"}],"key":"TtaNT8mmse"},{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"This cell defines the preprocess_data function to handle imputation and scaling, fitting the necessary transformers only on the training set. It then applies this function to the split dataframes generated in Part 4.","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"X3fVXyMWDE"}],"key":"MVdq41hYFX"},{"type":"paragraph","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"Explanation of Fixes:","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"wy9mYBXmvI"}],"key":"sPm2RTlMWP"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":12,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"feature_suffix Definition: The NameError occurred because feature_suffix (which depends on USE_ENHANCED_FEATURES) was used to construct FEATURE_LIST_FILENAME before the main execution block where it would be defined.","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"TREXQDyjel"}],"key":"ew3PH6DLnF"}],"key":"jukjxMYdJS"},{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Solution: The code now defines feature_suffix inside the if ","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"XDItGmqr1R"},{"type":"strong","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"name","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"Fp7AnFeqsM"}],"key":"bfEQPR3ljT"},{"type":"text","value":" == “","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"FAFin8Eq6Z"},{"type":"strong","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"main","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"cjzCx7STGk"}],"key":"Ie8B0EEPX2"},{"type":"text","value":"”: block before calling preprocess_data. Filename templates (scaler_fname_template, flist_fname_template) are passed to the function, and the final filenames are constructed inside the function using the passed feature_suffix.","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"lF3y3qauKw"}],"key":"TSlVI7SPb0"}],"key":"yLxo845EBL"},{"type":"listItem","spread":true,"position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Configuration Checks: Added more robust checks at the beginning of the main execution block to ensure all necessary configuration variables and dataframes exist before proceeding.","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"p2nPSh60g5"}],"key":"yynvQ3yvWv"}],"key":"fRz2BuNksb"},{"type":"listItem","spread":true,"position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Error Handling: Added more specific error messages and try-except blocks within the preprocess_data function for robustness during imputation and scaling transforms.","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"WC060pypxy"}],"key":"SCYc36BNvm"}],"key":"RFp2Mnzkna"},{"type":"listItem","spread":true,"position":{"start":{"line":16,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Clarity: Minor adjustments to print statements for better clarity.\nNow, when you run this corrected Part 5 cell (after ensuring Part 1 and Part 4 have run successfully in your session), it should correctly define feature_suffix, construct the filenames, perform the preprocessing, save the components, and store the results in the train_scaled_df, validation_scaled_df, etc., variables without the NameError.","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"MqjHVJQ8ZJ"}],"key":"ONCQLTC1tD"}],"key":"Cczsyyrilt"}],"key":"S1VSxdHfyc"}],"identifier":"zjz8x96ukdht","label":"zJZ8x96ukDHt","html_id":"zjz8x96ukdht","key":"LAloqDLJnb"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tyRG0McTkHf7","outputId":"c010e4ba-752e-4057-852a-d6bc4b9fd48c","ExecuteTime":{"end_time":"2026-01-07T03:44:43.374949200Z","start_time":"2026-01-07T03:37:30.245630Z"}},"children":[{"type":"code","lang":"python","executable":true,"value":"# --- Part 5: Preprocessing (Imputation \u0026 Scaling) ---\n\ndef preprocess_data(train_df, validation_df, test_df, target_col, scaler_type='Robust',\n                    output_dir='output/', scaler_fname_template=None, flist_fname_template=None, feature_suffix='basic'):\n\n    # 1. Validate Inputs\n    if any(df is None or df.empty for df in [train_df, validation_df, test_df]):\n        print(\"Error: One or more input DataFrames are invalid.\")\n        return (None,) * 5\n\n    print(f\"\\n--- Preprocessing Data (Imputation \u0026 Scaling: {scaler_type}) ---\")\n\n    # 2. Identify Features (Exclude Target)\n    # We use training data to determine features\n    feature_candidates = [c for c in train_df.columns if c != target_col]\n    final_feature_columns = train_df[feature_candidates].select_dtypes(include=np.number).columns.tolist()\n\n    if not final_feature_columns:\n        print(\"Error: No numeric features found.\")\n        return (None,) * 5\n\n    print(f\"Identified {len(final_feature_columns)} numeric features.\")\n\n    # 3. Initialize Transformers\n    imputer = SimpleImputer(strategy='mean')\n\n    if scaler_type.lower() == 'standard':\n        scaler = StandardScaler()\n    else:\n        scaler = RobustScaler() # Default to Robust\n\n    # 4. Fit Transformers (ONLY on Training Data)\n    train_proc_df = train_df.copy()\n\n    # Imputation Fit\n    impute_needed = train_proc_df[final_feature_columns].isnull().values.any()\n    if impute_needed:\n        print(\"Fitting imputer on training data...\")\n        imputer.fit(train_proc_df[final_feature_columns])\n    else:\n        print(\"No missing values in training set. Skipping imputer fitting.\")\n\n    # Scaling Fit (Fit on imputed training data)\n    # If we imputed, we must transform the temp training data to fit the scaler correctly\n    temp_train_features = imputer.transform(train_proc_df[final_feature_columns]) if impute_needed else train_proc_df[final_feature_columns]\n\n    print(f\"Fitting {scaler_type}Scaler on training data...\")\n    scaler.fit(temp_train_features)\n\n    # 5. Transform All Sets (Loop to remove redundancy)\n    processed_dfs = []\n    datasets = [train_df, validation_df, test_df]\n    set_names = ['Train', 'Validation', 'Test']\n\n    for name, df in zip(set_names, datasets):\n        df_copy = df.copy()\n        try:\n            # Apply Imputation\n            if impute_needed:\n                df_copy[final_feature_columns] = imputer.transform(df_copy[final_feature_columns])\n\n            # Apply Scaling\n            df_copy[final_feature_columns] = scaler.transform(df_copy[final_feature_columns])\n            processed_dfs.append(df_copy)\n        except Exception as e:\n            print(f\"Error processing {name} set: {e}\")\n            return (None,) * 5\n\n    train_scaled, val_scaled, test_scaled = processed_dfs\n    print(\"Transformation complete.\")\n\n    # 6. Save Components\n    try:\n        os.makedirs(output_dir, exist_ok=True)\n        # Use provided templates or defaults\n        s_path = scaler_fname_template.format(scaler_type.lower()) if scaler_fname_template else f\"{output_dir}/scaler.joblib\"\n        f_path = flist_fname_template.format(feature_suffix) if flist_fname_template else f\"{output_dir}/features.joblib\"\n\n        joblib.dump(scaler, s_path)\n        joblib.dump(final_feature_columns, f_path)\n        print(f\"Saved scaler and feature list to {output_dir}\")\n    except Exception as e:\n        print(f\"Warning: Could not save scaler/features: {e}\")\n\n    return train_scaled, val_scaled, test_scaled, scaler, final_feature_columns\n\n# --- Execution Block ---\nif __name__ == \"__main__\":\n    # Assumes train_df, validation_df, test_df exist from Part 4\n    # Assumes config variables like SCALER_TYPE exist from Part 1\n\n    if 'train_df' in locals():\n        train_scaled_df, validation_scaled_df, test_scaled_df, scaler, final_feature_columns_used = preprocess_data(\n            train_df, validation_df, test_df,\n            TARGET_BINARY_COL,\n            scaler_type=SCALER_TYPE,\n            output_dir=OUTPUT_DIR,\n            scaler_fname_template=SCALER_FILENAME.replace(f\"_{SCALER_TYPE.lower()}\", \"_{}\"), # Dynamic template\n            flist_fname_template=FEATURE_LIST_FILENAME.replace(f\"_{feature_suffix}\", \"_{}\"),\n            feature_suffix=feature_suffix\n        )","identifier":"tyrg0mctkhf7-code","enumerator":"9","html_id":"tyrg0mctkhf7-code","key":"nkl191DPyw"},{"type":"outputs","id":"_9m8BO7kRvfzDIck0OfAl","children":[{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"\n--- Preprocessing Data (Imputation \u0026 Scaling: Robust) ---\nIdentified 25 numeric features.\nNo missing values in training set. Skipping imputer fitting.\nFitting RobustScaler on training data...\nTransformation complete.\nSaved scaler and feature list to output_refactored/\n"},"children":[],"identifier":"tyrg0mctkhf7-outputs-0","html_id":"tyrg0mctkhf7-outputs-0","key":"KyUVAOl4kb"}],"identifier":"tyrg0mctkhf7-outputs","html_id":"tyrg0mctkhf7-outputs","key":"UJZdrXLGDB"}],"identifier":"tyrg0mctkhf7","label":"tyRG0McTkHf7","html_id":"tyrg0mctkhf7","key":"gjea08GWzL"},{"type":"block","kind":"notebook-content","data":{"id":"BRyqj-kJkOh2"},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Part 6: Sequence Creation","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EV3ucUXIOt"}],"identifier":"part-6-sequence-creation","label":"Part 6: Sequence Creation","html_id":"part-6-sequence-creation","implicit":true,"key":"Yj3FCvE4hq"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"This step takes the scaled dataframes (train_scaled_df, validation_scaled_df, test_scaled_df) and transforms them into the sequence format required by LSTM/GRU models. Each sequence (X) will contain SEQUENCE_LENGTH consecutive time steps of features, and the corresponding target (y) will be the bloom state FORECAST_HORIZON steps after the end of the sequence.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"vcYZg7Tygu"}],"key":"oAVMhYx2Ou"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"This cell defines the create_sequences_from_df function and applies it to the scaled training, validation, and test dataframes generated in Part 5. It uses the SEQUENCE_LENGTH and FORECAST_HORIZON parameters defined in Part 1. The resulting NumPy arrays (X_train, y_train, etc.) are saved to a file.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"Nq9eR6lYhk"}],"key":"EH21U9RNwV"},{"type":"paragraph","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"Explanation:","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"y4T6kUQCE2"}],"key":"eX7SCOGH4O"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":10,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Function create_sequences_from_df:","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"v9uY1ui3d6"}],"key":"AZfVAWQSkH"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":11,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Takes a scaled DataFrame, the list of feature columns to use, the target column name, sequence length, and forecast horizon.","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"rkEaWJW30O"}],"key":"bEqSof96aM"}],"key":"GV4OO9OFpa"},{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Includes checks for valid input and sufficient data length.","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"MxMvnLJJ10"}],"key":"rZcCknMMm8"}],"key":"Bc2w36IYUn"},{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Iterates through the data, creating sequences (X) of length sequence_length and corresponding targets (y) taken forecast_horizon steps after the end of each sequence.","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"WLs5QnRMyO"}],"key":"oDFEUFk10E"}],"key":"NtCUXijbo3"},{"type":"listItem","spread":true,"position":{"start":{"line":14,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Returns the sequences and targets as NumPy arrays.","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"zWQX6o0GgE"}],"key":"tBYwHj2ZxZ"}],"key":"GcribaKSI5"}],"key":"GPk06dXhDZ"}],"key":"jqbvIGrcel"},{"type":"listItem","spread":true,"position":{"start":{"line":16,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"Main Execution:","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"x1JCkGs9Bg"}],"key":"IKjW1QXipS"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":17,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Checks that the necessary scaled DataFrames (train_scaled_df, etc.) and the feature list (final_feature_columns_used) exist from Part 5.","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"QSzGjqp3b8"}],"key":"xNyHdlrxa1"}],"key":"xMCHPBa5Zr"},{"type":"listItem","spread":true,"position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Calls create_sequences_from_df separately for the training, validation, and test sets.","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"ksUT4Ylhr3"}],"key":"uqzXraLoAG"}],"key":"JGj733eezq"},{"type":"listItem","spread":true,"position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Prints the shapes of the resulting NumPy arrays (X_train, y_train, etc.).","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"ynekhYsYsS"}],"key":"zcrwBJ5vwR"}],"key":"rxbKSe9oOa"},{"type":"listItem","spread":true,"position":{"start":{"line":20,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Saves the generated sequences and the feature list used to create them into a single .npz file for easy loading later. The filename dynamically includes the forecast horizon and feature type based on the configuration in Part 1.","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"nthFcl0DcU"}],"key":"LcFgNH5Gze"}],"key":"EPTSKF8sHo"}],"key":"BF0sEmM0ai"}],"key":"bHKklTBqii"}],"key":"pbkS06ereA"},{"type":"paragraph","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"text","value":"After running this cell, you will have the sequence arrays (X_train, y_train, X_val, y_val, X_test, y_test) ready for the LSTM model definition (Part 7) and subsequent training/tuning steps.","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"lu1dQKFoSO"}],"key":"F9IDoM5IDD"}],"identifier":"bryqj-kjkoh2","label":"BRyqj-kJkOh2","html_id":"bryqj-kjkoh2","key":"mmKxZ4lK07"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kk0kagLqkThx","outputId":"5881a1ee-854d-4ab0-a79d-fda73305d7ea","ExecuteTime":{"end_time":"2026-01-07T03:44:43.375949700Z","start_time":"2026-01-07T03:37:30.291068Z"}},"children":[{"type":"code","lang":"python","executable":true,"value":"def create_sequences_from_df(df, seq_length, target_col_idx=0, pred_step=1, return_targets=True):\n    \"\"\"\n    Creates sequences for LSTM model from a DataFrame.\n    \"\"\"\n    xs = []\n    ys = []\n\n    # Convert dataframe to numpy if needed\n    data = df.values if hasattr(df, 'values') else df\n\n    # Loop through data\n    # We stop earlier to account for the prediction step ahead (pred_step)\n    for i in range(len(data) - seq_length - pred_step + 1):\n        x = data[i:(i + seq_length)]\n        xs.append(x)\n\n        if return_targets:\n            # Target is 'pred_step' steps after the sequence ends\n            y = data[i + seq_length + pred_step - 1, target_col_idx]\n            ys.append(y)\n\n    return np.array(xs), np.array(ys)\n\nif __name__ == \"__main__\":\n    if 'train_scaled_df' in locals() and train_scaled_df is not None:\n\n        # Dictionary to automate the loop\n        data_splits = {\n            'train': train_scaled_df,\n            'val':   validation_scaled_df,\n            'test':  test_scaled_df\n        }\n\n        sequences = {}\n        print(f\"\\n--- Creating Sequences (Horizon: {FORECAST_HORIZON}, Seq Length: {SEQUENCE_LENGTH}) ---\")\n\n        for name, df in data_splits.items():\n            print(f\"Generating {name} sequences...\")\n\n            # --- CRITICAL FIX START ---\n            # 1. Determine which columns to use.\n            # We must ensure the TARGET column is included in the data so we can extract 'y'.\n            if 'final_feature_columns_used' in locals():\n                # Make a copy of the list so we don't modify the original variable\n                cols_to_use = list(final_feature_columns_used)\n\n                # If target is missing from features, add it so we can create 'y'\n                if TARGET_BINARY_COL not in cols_to_use:\n                    print(f\"  Note: Adding '{TARGET_BINARY_COL}' back to dataframe for sequence generation.\")\n                    cols_to_use.append(TARGET_BINARY_COL)\n\n                df_subset = df[cols_to_use]\n            else:\n                df_subset = df\n            # --- CRITICAL FIX END ---\n\n            # 2. Find the integer index of the target column inside this new subset\n            try:\n                target_idx = df_subset.columns.get_loc(TARGET_BINARY_COL)\n            except KeyError:\n                raise KeyError(f\"Target column '{TARGET_BINARY_COL}' not found in the dataframe. Check spelling!\")\n\n            # 3. Call the function\n            X, y = create_sequences_from_df(\n                df_subset,              # The Data (Features + Target)\n                SEQUENCE_LENGTH,        # Window size\n                target_idx,             # Index of the target column\n                FORECAST_HORIZON,       # Prediction step\n                True                    # Return targets?\n            )\n\n            # Store in dictionary\n            sequences[f'X_{name}'] = X\n            sequences[f'y_{name}'] = y\n            print(f\"  {name}: X={X.shape}, y={y.shape}\")\n\n        # Unpack for later parts\n        X_train, y_train = sequences['X_train'], sequences['y_train']\n        X_val, y_val     = sequences['X_val'], sequences['y_val']\n        X_test, y_test   = sequences['X_test'], sequences['y_test']\n\n        # Save to file\n        if X_train.size \u003e 0:\n            suffix = feature_suffix if 'feature_suffix' in locals() else \"\"\n            fname = SEQUENCES_FILENAME_TEMPLATE.format(FORECAST_HORIZON, suffix)\n\n            # Save arrays and the column list for reference\n            np.savez(fname, **sequences, feature_columns=df_subset.columns)\n            print(f\"\\nSaved sequences to {fname}\")\n\n    else:\n        print(\"Skipping Part 6: Scaled data not found.\")","identifier":"kk0kaglqkthx-code","enumerator":"10","html_id":"kk0kaglqkthx-code","key":"kdaiVwpHYs"},{"type":"outputs","id":"0ozrHJ3qDqCmJjs3yeppV","children":[{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"\n--- Creating Sequences (Horizon: 1, Seq Length: 12) ---\nGenerating train sequences...\n  Note: Adding 'bloom_target' back to dataframe for sequence generation.\n  train: X=(1111, 12, 26), y=(1111,)\nGenerating val sequences...\n  Note: Adding 'bloom_target' back to dataframe for sequence generation.\n  val: X=(228, 12, 26), y=(228,)\nGenerating test sequences...\n  Note: Adding 'bloom_target' back to dataframe for sequence generation.\n  test: X=(230, 12, 26), y=(230,)\n\nSaved sequences to output_refactored/sequences_horizon1wk_enhanced.npz\n"},"children":[],"identifier":"kk0kaglqkthx-outputs-0","html_id":"kk0kaglqkthx-outputs-0","key":"L7DvraxUbU"}],"identifier":"kk0kaglqkthx-outputs","html_id":"kk0kaglqkthx-outputs","key":"Qw5DJwT8A6"}],"identifier":"kk0kaglqkthx","label":"Kk0kagLqkThx","html_id":"kk0kaglqkthx","key":"AR3PnZGaUh"},{"type":"block","kind":"notebook-content","data":{"id":"D79eTjU0ka0M"},"children":[{"type":"heading","depth":3,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"Part 7: Model Definition (LSTM).","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"oYOKilp2bA"}],"identifier":"part-7-model-definition-lstm","label":"Part 7: Model Definition (LSTM).","html_id":"part-7-model-definition-lstm","implicit":true,"key":"tlQjo3wLMO"},{"type":"paragraph","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"This part defines the function build_lstm_model responsible for creating the LSTM network architecture using TensorFlow/Keras. The function is designed to be flexible: it can use default hyperparameters defined in Part 1, or it can accept a hp object from KerasTuner (which we’ll use later in Part 8) to create models with varying hyperparameters during the tuning process.","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"JM7El94PCY"}],"key":"nPpvvZp9nn"},{"type":"paragraph","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"This cell defines the build_lstm_model function. It specifies the layers (LSTM, Dropout, Dense) and allows for hyperparameter configuration either through defaults or a KerasTuner object. It then demonstrates building the model with default hyperparameters, assuming the input shape is known from Part 6.","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"OV84iw4Duk"}],"key":"wZgN6egrds"},{"type":"paragraph","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"Explanation:","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"yD4KHKpqp2"}],"key":"WNrzfhicaq"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":10,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Function build_lstm_model:","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"nGbfrVc9YD"}],"key":"lx16mukBx3"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":11,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Takes input_shape (required) and an optional KerasTuner hp object.","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"c56lpUgZTj"}],"key":"GV32jLgORu"}],"key":"rnHbrmFqYg"},{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Hyperparameter Handling: If hp is provided (during tuning), it defines hyperparameters using ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"ZqU1Evu1on"},{"type":"link","url":"http://hp.Int","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"hp.Int","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"RWUKJP6caQ"}],"urlSource":"http://hp.Int","key":"VDp4T6oOQ6"},{"type":"text","value":", hp.Float, hp.Choice. If hp is None (when building the default or final model), it uses the DEFAULT_ variables defined in Part 1 (with fallbacks just in case).","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"AB7HqzRlBh"}],"key":"qPrzSve5h8"}],"key":"vWILuLCBcL"},{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Architecture: Defines a two-layer LSTM structure with Dropout. You can easily modify this (e.g., change to GRU, add Dense layers, use Bidirectional) by editing this function.","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"IKb3XZCApK"}],"key":"RZoiTW81y3"}],"key":"X9ynNnDv9h"},{"type":"listItem","spread":true,"position":{"start":{"line":14,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Compilation: Compiles the model inside the function using Adam optimizer and binary cross-entropy loss. This is convenient for KerasTuner.","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"DnltG8J6MG"}],"key":"dRaO7JvV8v"}],"key":"a9gQObllRW"}],"key":"qi9oZAHtBF"}],"key":"iEPbOOVb7J"},{"type":"listItem","spread":true,"position":{"start":{"line":16,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"Main Execution (if ","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"DKN5kJ8TFO"},{"type":"strong","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"name","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"G46WjBrFjo"}],"key":"RIi3lyLwct"},{"type":"text","value":" == “","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"Rmn4CfIqx5"},{"type":"strong","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"main","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"Y9bDmHkneJ"}],"key":"NwdnwX5RZJ"},{"type":"text","value":"”:):","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"qNW3sOrZXy"}],"key":"ZWF6clEGsH"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":17,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Checks if X_train exists (from Part 6) to get the required input_shape.","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"ZLBsqrfmBq"}],"key":"QgKvobIPTi"}],"key":"LNnKym2BZu"},{"type":"listItem","spread":true,"position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Calls build_lstm_model with hp=None to create an instance using the default hyperparameters.","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"vHUOxrfFtT"}],"key":"IpA2iKvhC1"}],"key":"nd3vgSXqxJ"},{"type":"listItem","spread":true,"position":{"start":{"line":19,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Prints the model summary.","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"iOlXOhlnvq"}],"key":"YvJVt77TKE"}],"key":"RTxA8yJXFc"}],"key":"ewWj6RZVwC"}],"key":"pNaqpWnRbE"}],"key":"kvJyL5mtES"},{"type":"paragraph","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"After running this cell, the function build_lstm_model is defined and ready to be used either by KerasTuner (Part 8) or directly for training the default/final model (Part 9). The lstm_model_default variable holds an example compiled model instance (useful for checking).","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"g1p2TGhas2"}],"key":"c0SMSVdEVv"}],"identifier":"d79etju0ka0m","label":"D79eTjU0ka0M","html_id":"d79etju0ka0m","key":"dFl7MmyT3M"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/","height":375},"id":"ztwksvFhkel6","outputId":"48cea08c-deac-43b1-8cc0-d312b23f8fa5","ExecuteTime":{"end_time":"2026-01-07T03:44:43.375949700Z","start_time":"2026-01-07T03:37:30.317485Z"}},"children":[{"type":"code","lang":"python","executable":true,"value":"def build_lstm_model(input_shape, hp=None):\n    \"\"\"\n    Builds and compiles an LSTM model.\n\n    Args:\n        input_shape (tuple): The shape of the input data (time_steps, features).\n        hp (KerasTuner.HyperParameters, optional): Hyperparameters for tuning.\n                                                   If None, uses defaults.\n    Returns:\n        model: A compiled Keras model.\n    \"\"\"\n\n    # ---------------------------------------------------------\n    # 1. Hyperparameter Definition (Dual Mode)\n    # ---------------------------------------------------------\n    # If 'hp' is provided, we are in Tuning Mode.\n    # If 'hp' is None, we use the global defaults (defined in Part 1).\n\n    if hp:\n        # Tuning Mode: Ask KerasTuner to try different values\n        units_1 = hp.Int('lstm_units_1', min_value=32, max_value=128, step=32)\n        units_2 = hp.Int('lstm_units_2', min_value=16, max_value=64, step=16)\n        dropout_rate = hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1)\n        learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n    else:\n        # Default Mode: Use variables from Part 1 (with safety fallbacks)\n        units_1 = globals().get('DEFAULT_LSTM_UNITS', 64)\n        units_2 = globals().get('DEFAULT_LSTM_UNITS', 32) # Using same default or half\n        dropout_rate = globals().get('DEFAULT_DROPOUT', 0.2)\n        learning_rate = globals().get('DEFAULT_LEARNING_RATE', 0.001)\n\n    # ---------------------------------------------------------\n    # 2. Model Architecture\n    # ---------------------------------------------------------\n    model = Sequential()\n\n    # Input Layer\n    model.add(Input(shape=input_shape))\n\n    # LSTM Layer 1 (Must return sequences to feed the next LSTM layer)\n    model.add(LSTM(units=units_1, return_sequences=True))\n    model.add(Dropout(dropout_rate))\n\n    # LSTM Layer 2 (return_sequences=False because next is Dense)\n    model.add(LSTM(units=units_2, return_sequences=False))\n    model.add(Dropout(dropout_rate))\n\n    # Output Layer (1 unit for Binary Classification: Bloom vs No Bloom)\n    model.add(Dense(1, activation='sigmoid'))\n\n    # ---------------------------------------------------------\n    # 3. Compilation\n    # ---------------------------------------------------------\n    model.compile(\n        optimizer=Adam(learning_rate=learning_rate),\n        loss='binary_crossentropy',\n        metrics=[BinaryAccuracy(name='accuracy'), AUC(name='auc'), Recall(name='recall'),Precision(name='precision')]\n    )\n\n    return model\n\n# --- Main Execution (Test Block) ---\nif __name__ == \"__main__\":\n    print(\"--- Testing Model Definition (Step 7) ---\")\n\n    # We need X_train to know the input shape\n    if 'X_train' in locals() and X_train is not None:\n\n        # Determine shape: (Sequence Length, Number of Features)\n        # X_train shape is typically (Samples, Time Steps, Features)\n        # We need the last two dimensions for input_shape\n        input_shape_test = (X_train.shape[1], X_train.shape[2])\n        print(f\"Input Shape detected: {input_shape_test}\")\n\n        # Build a \"Default\" model (hp=None) to verify it works\n        try:\n            model_test = build_lstm_model(input_shape_test, hp=None)\n            print(\"\\nModel built successfully!\")\n            model_test.summary()\n        except Exception as e:\n            print(f\"Error building model: {e}\")\n\n    else:\n        print(\"Warning: X_train not found. Run Step 6 first to test this function.\")\n        print(\"Function 'build_lstm_model' is defined but not tested.\")","identifier":"ztwksvfhkel6-code","enumerator":"11","html_id":"ztwksvfhkel6-code","key":"i95DJOC54I"},{"type":"outputs","id":"aPigXkpNouOhOaCTfyiXr","children":[{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"--- Testing Model Definition (Step 7) ---\nInput Shape detected: (12, 26)\n\nModel built successfully!\n"},"children":[],"identifier":"ztwksvfhkel6-outputs-0","html_id":"ztwksvfhkel6-outputs-0","key":"uo0Mitrwrh"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"\u001b[1mModel: \"sequential\"\u001b[0m\n","content_type":"text/plain"},"text/html":{"content":"\u003cpre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"\u003e\u003cspan style=\"font-weight: bold\"\u003eModel: \"sequential\"\u003c/span\u003e\n\u003c/pre\u003e\n","content_type":"text/html"}}},"children":[],"identifier":"ztwksvfhkel6-outputs-1","html_id":"ztwksvfhkel6-outputs-1","key":"jQub64vhwQ"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m23,296\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m33,024\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","content_type":"text/plain"},"text/html":{"content":"\u003cpre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"\u003e┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u003cspan style=\"font-weight: bold\"\u003e Layer (type)                    \u003c/span\u003e┃\u003cspan style=\"font-weight: bold\"\u003e Output Shape           \u003c/span\u003e┃\u003cspan style=\"font-weight: bold\"\u003e       Param # \u003c/span\u003e┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ lstm (\u003cspan style=\"color: #0087ff; text-decoration-color: #0087ff\"\u003eLSTM\u003c/span\u003e)                     │ (\u003cspan style=\"color: #00d7ff; text-decoration-color: #00d7ff\"\u003eNone\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e12\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e64\u003c/span\u003e)         │        \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e23,296\u003c/span\u003e │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (\u003cspan style=\"color: #0087ff; text-decoration-color: #0087ff\"\u003eDropout\u003c/span\u003e)               │ (\u003cspan style=\"color: #00d7ff; text-decoration-color: #00d7ff\"\u003eNone\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e12\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e64\u003c/span\u003e)         │             \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e0\u003c/span\u003e │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ lstm_1 (\u003cspan style=\"color: #0087ff; text-decoration-color: #0087ff\"\u003eLSTM\u003c/span\u003e)                   │ (\u003cspan style=\"color: #00d7ff; text-decoration-color: #00d7ff\"\u003eNone\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e64\u003c/span\u003e)             │        \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e33,024\u003c/span\u003e │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (\u003cspan style=\"color: #0087ff; text-decoration-color: #0087ff\"\u003eDropout\u003c/span\u003e)             │ (\u003cspan style=\"color: #00d7ff; text-decoration-color: #00d7ff\"\u003eNone\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e64\u003c/span\u003e)             │             \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e0\u003c/span\u003e │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (\u003cspan style=\"color: #0087ff; text-decoration-color: #0087ff\"\u003eDense\u003c/span\u003e)                   │ (\u003cspan style=\"color: #00d7ff; text-decoration-color: #00d7ff\"\u003eNone\u003c/span\u003e, \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e1\u003c/span\u003e)              │            \u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e65\u003c/span\u003e │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\u003c/pre\u003e\n","content_type":"text/html"}}},"children":[],"identifier":"ztwksvfhkel6-outputs-2","html_id":"ztwksvfhkel6-outputs-2","key":"AXxBChHWmg"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m56,385\u001b[0m (220.25 KB)\n","content_type":"text/plain"},"text/html":{"content":"\u003cpre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"\u003e\u003cspan style=\"font-weight: bold\"\u003e Total params: \u003c/span\u003e\u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e56,385\u003c/span\u003e (220.25 KB)\n\u003c/pre\u003e\n","content_type":"text/html"}}},"children":[],"identifier":"ztwksvfhkel6-outputs-3","html_id":"ztwksvfhkel6-outputs-3","key":"xAaM6Itopb"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m56,385\u001b[0m (220.25 KB)\n","content_type":"text/plain"},"text/html":{"content":"\u003cpre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"\u003e\u003cspan style=\"font-weight: bold\"\u003e Trainable params: \u003c/span\u003e\u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e56,385\u003c/span\u003e (220.25 KB)\n\u003c/pre\u003e\n","content_type":"text/html"}}},"children":[],"identifier":"ztwksvfhkel6-outputs-4","html_id":"ztwksvfhkel6-outputs-4","key":"YKZ4dMRNhs"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","content_type":"text/plain"},"text/html":{"content":"\u003cpre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"\u003e\u003cspan style=\"font-weight: bold\"\u003e Non-trainable params: \u003c/span\u003e\u003cspan style=\"color: #00af00; text-decoration-color: #00af00\"\u003e0\u003c/span\u003e (0.00 B)\n\u003c/pre\u003e\n","content_type":"text/html"}}},"children":[],"identifier":"ztwksvfhkel6-outputs-5","html_id":"ztwksvfhkel6-outputs-5","key":"ykxgEdM03u"}],"identifier":"ztwksvfhkel6-outputs","html_id":"ztwksvfhkel6-outputs","key":"tR2NkD68v8"}],"identifier":"ztwksvfhkel6","label":"ztwksvFhkel6","html_id":"ztwksvfhkel6","key":"JUQZDKjts4"},{"type":"block","kind":"notebook-content","data":{"id":"-ZKfYtZkkoCz"},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Part 8: Hyperparameter Tuning with KerasTuner (Conditional)","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TwnUiJTqPN"}],"identifier":"part-8-hyperparameter-tuning-with-kerastuner-conditional","label":"Part 8: Hyperparameter Tuning with KerasTuner (Conditional)","html_id":"part-8-hyperparameter-tuning-with-kerastuner-conditional","implicit":true,"key":"sFTUQoKHL0"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"This cell sets up and runs the KerasTuner search process if the PERFORM_TUNING flag (from Part 1) is set to True. It uses the build_lstm_model function (from Part 7) as the hypermodel builder and searches for the best combination of LSTM units, dropout rate, and learning rate based on validation accuracy.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"j6Dhkxz3Ji"}],"key":"sbG7n5aTTr"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Explanation:","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"YFyTH5WhcX"}],"key":"Nqu4QbTErd"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":7,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Conditional Execution: The entire cell’s logic is wrapped in if can_tune:, which checks if PERFORM_TUNING was set to True in Part 1 and if the keras_tuner library (kt) was successfully imported. If not, it prints a message and skips tuning.","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"TmIL5ZbBvC"}],"key":"UGnZ9WEPVb"}],"key":"JTWupx6TGl"},{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Prerequisite Checks: Inside the if block, it checks if the necessary sequence data (X_train, y_train, etc.) and the build_lstm_model function exist before proceeding.","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"sCbcIeE7LO"}],"key":"eDXSyXeJle"}],"key":"qzQ8jyoFRt"},{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Tuner Setup:","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"P2jFHroWg9"}],"key":"SauhnuiH20"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":10,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Creates a keras_tuner.RandomSearch instance (you could switch to kt.Hyperband for potentially faster convergence).","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"kjKMLsxyT8"}],"key":"DkH3sd2UwB"}],"key":"ZfnLZ2CU76"},{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Passes a lambda function lambda hp: build_lstm_model(input_shape_tune, hp=hp) as the hypermodel builder.","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"EeJqshYZVn"}],"key":"iD7ctRXdV0"}],"key":"wJyHdTXTvY"},{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"This ensures the build_lstm_model function receives the tuner’s hp object to define the model architecture with tunable parameters.","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"Jbk7g9K7GN"}],"key":"jQtj27nySq"}],"key":"p42CcD4ReF"},{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Sets the objective to ‘val_accuracy’ (tune for best accuracy on the validation set).","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"KkgU5Y4SV4"}],"key":"bMEObaAvSh"}],"key":"R7TvfD5UZk"},{"type":"listItem","spread":true,"position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Configures max_trials, directory, project_name, etc.","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"j9mApNkpiq"}],"key":"Cg9QVX9uC5"}],"key":"MQmLqxqeVM"}],"key":"ShXXexUBz5"}],"key":"deM3ooneBI"},{"type":"listItem","spread":true,"position":{"start":{"line":15,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Run Search:","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"XAmkmLFhpA"}],"key":"g7rRHOBZNA"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":16,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Calls ","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"EEJQ0H8VDc"},{"type":"link","url":"http://tuner.search","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"tuner.search","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"XtWmy6RJU4"}],"urlSource":"http://tuner.search","key":"f0L8WL3wcQ"},{"type":"text","value":"(), passing the training and validation data.","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"v5FIuinMQ1"}],"key":"CO73XcaZka"}],"key":"huXUiWNW5E"},{"type":"listItem","spread":true,"position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Uses a dedicated EarlyStopping callback with potentially shorter patience for the tuning phase itself.","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"KqExHGyoDS"}],"key":"kvxYN9XAz0"}],"key":"lJilLYjDSm"}],"key":"wVzQJGurdr"}],"key":"MYiNomn0ng"},{"type":"listItem","spread":true,"position":{"start":{"line":18,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Retrieve Best Hyperparameters:","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"GpfkUaBrC1"}],"key":"c0CfcBCs1O"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":19,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"After the search completes, tuner.get_best_hyperparameters(num_trials=1)[0] retrieves the HyperParameters object corresponding to the best trial.","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"sbptWzdZVr"}],"key":"M9bqzO1lHr"}],"key":"xeM0Dm3yeJ"},{"type":"listItem","spread":true,"position":{"start":{"line":20,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"The values for the tuned hyperparameters (e.g., lstm_units_1, dropout_rate, learning_rate) are extracted and printed.","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"HVwsk4kKVn"}],"key":"Xcv9hkkQTV"}],"key":"hOYsRAylUE"}],"key":"TaVslIHOpR"}],"key":"FQmpKVGbxm"}],"key":"ok9A2la4xb"},{"type":"paragraph","position":{"start":{"line":22,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"text","value":"The best_hps variable stores this object for use in the next step (Part 9).\nIncludes error handling in case the tuner fails or doesn’t return results.\nIf PERFORM_TUNING is False, this cell will simply print a message and set best_hps to None. The next step (Part 9) will then know to use the default hyperparameters defined in Part 1.","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"MRw4TZhLK7"}],"key":"iuNydzVM1J"}],"identifier":"-zkfytzkkocz","label":"-ZKfYtZkkoCz","html_id":"id-zkfytzkkocz","key":"gSQ4gpmQa2"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aSM54gA_ko4e","outputId":"b34a35a0-3cf5-4544-9c06-2bab22214f24","ExecuteTime":{"end_time":"2026-01-07T03:44:43.375949700Z","start_time":"2026-01-07T03:37:30.392565Z"}},"children":[{"type":"code","lang":"python","executable":true,"value":"# 1. Initialize Variables\nbest_hps = None\nbest_model_from_tuner = None\n\n# 2. Check Prerequisites\n# We need X_train (Data) and build_lstm_model (Function)\ncan_tune = True\n\nif 'X_train' not in locals() or X_train is None:\n    print(\"Error: X_train not found. Please run Step 6 (Data Generation) first.\")\n    can_tune = False\n\nif 'build_lstm_model' not in locals():\n    print(\"Error: build_lstm_model not found. Please run Step 7 (Model Definition) first.\")\n    can_tune = False\n\nif 'PERFORM_TUNING' not in locals() or not PERFORM_TUNING:\n    print(\"Skipping Tuning: PERFORM_TUNING flag is False or missing.\")\n    can_tune = False\n\n# 3. Run Tuner\nif can_tune:\n    print(\"\\n--- Setting up KerasTuner ---\")\n\n    # Define Input Shape for the builder\n    # Shape = (Time Steps, Features) -\u003e (X_train.shape[1], X_train.shape[2])\n    input_shape_tune = (X_train.shape[1], X_train.shape[2])\n    print(f\"Tuning Input Shape: {input_shape_tune}\")\n\n    # Initialize the RandomSearch Tuner\n    # Note: We use objective='val_auc' to match the name='auc' in Step 7\n    tuner = kt.RandomSearch(\n        lambda hp: build_lstm_model(input_shape_tune, hp=hp),\n        objective=kt.Objective(\"val_auc\", direction=\"max\"),\n        max_trials=TUNER_MAX_TRIALS,       # Defined in Step 1 (e.g., 10 or 20)\n        executions_per_trial=2,            # Run each trial twice to reduce luck\n        directory=TUNER_PROJECT_DIR,       # \"hab_tuning\"\n        project_name=TUNER_PROJECT_NAME,   # \"bloom_prediction\"\n        overwrite=True                     # Start fresh every time\n    )\n\n    tuner.search_space_summary()\n\n    print(\"\\n--- Starting Search ---\")\n\n    # Early Stopping strictly for the tuning phase (speed things up)\n    tuner_early_stopping = EarlyStopping(monitor='val_loss', patience=4, verbose=0)\n\n    try:\n        # CRITICAL FIX: Pass Numpy arrays directly.\n        # Do not convert to tf.Tensor manually here.\n        tuner.search(\n            X_train, y_train,\n            epochs=TUNER_EPOCHS,           # Defined in Step 1 (e.g., 10 or 20)\n            batch_size=TUNER_BATCH_SIZE,   # Defined in Step 1 (e.g., 32)\n            validation_data=(X_val, y_val),\n            callbacks=[tuner_early_stopping],\n            verbose=1\n        )\n\n        print(\"\\n--- Hyperparameter Search Finished ---\")\n\n        # 4. Retrieve Best Results\n        best_hps_list = tuner.get_best_hyperparameters(num_trials=1)\n\n        if best_hps_list:\n            best_hps = best_hps_list[0]\n            print(\"Best Hyperparameters Found:\")\n            print(f\"  LSTM Units 1:  {best_hps.values.get('lstm_units_1')}\")\n            print(f\"  LSTM Units 2:  {best_hps.values.get('lstm_units_2')}\")\n            print(f\"  Dropout Rate:  {best_hps.values.get('dropout_rate')}\")\n            print(f\"  Learning Rate: {best_hps.values.get('learning_rate')}\")\n\n            # (Optional) You can build the best model immediately if you want\n            # best_model = tuner.hypermodel.build(best_hps)\n        else:\n            print(\"Warning: Tuner finished but returned no hyperparameters.\")\n            best_hps = None\n\n    except Exception as e:\n        print(f\"CRITICAL ERROR during tuning: {e}\")\n        best_hps = None\n\nelse:\n    print(\"Tuner skipped.\")","identifier":"asm54ga_ko4e-code","enumerator":"12","html_id":"asm54ga-ko4e-code","key":"eZviUADBYV"},{"type":"outputs","id":"8BnHSxuJVAND4ZDkXiCln","children":[{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"Trial 10 Complete [00h 00m 12s]\nval_auc: 0.9256471693515778\n\nBest val_auc So Far: 0.9290720522403717\nTotal elapsed time: 00h 02m 41s\n\n--- Hyperparameter Search Finished ---\nBest Hyperparameters Found:\n  LSTM Units 1:  128\n  LSTM Units 2:  32\n  Dropout Rate:  0.1\n  Learning Rate: 0.01\n"},"children":[],"identifier":"asm54ga_ko4e-outputs-0","html_id":"asm54ga-ko4e-outputs-0","key":"vqyavK0pjE"}],"identifier":"asm54ga_ko4e-outputs","html_id":"asm54ga-ko4e-outputs","key":"RFormaHGhz"}],"identifier":"asm54ga_ko4e","label":"aSM54gA_ko4e","html_id":"asm54ga-ko4e","key":"ndPR9wWBEo"},{"type":"block","kind":"notebook-content","data":{"id":"N5nvBjk-leEt"},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Part 9: Model Training","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YaEreiAn1c"}],"identifier":"part-9-model-training","label":"Part 9: Model Training","html_id":"part-9-model-training","implicit":true,"key":"Hm1WPotwzu"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"This cell defines the main training function and executes it. It uses the configuration flags from Part 1 to determine whether to use class weights and whether to use default or tuned hyperparameters (from best_hps variable potentially created in Part 8). It saves the best model weights found during training.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"cv70f58Rn9"}],"key":"s9r8MyqAGW"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Explanation:","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"KDP53nAzEo"}],"key":"WfLA097epC"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":7,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Function train_model:","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"HQUoxNjhj6"}],"key":"nA6heQMpeF"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":8,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Takes training/validation data, input shape, configuration flags (use_class_weight), optional tuned hyperparameters (best_hps), training parameters (epochs, batch_size, patience), and paths/names for saving.","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"aihNgqGdRv"}],"key":"qOo1TVjR5A"}],"key":"rD4Y5whiuA"},{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Builds Model: Calls build_lstm_model (from Part 7), passing best_hps if it’s available (meaning tuning was done in Part 8), otherwise hp=None is passed and the function uses defaults.","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"MgZI0La2bI"}],"key":"ZfTVWB8n1B"}],"key":"eyX5EsGggP"},{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Class Weights: If use_class_weight is True, it calculates the weights using compute_class_weight on y_train.","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"rXUEtEw6xN"}],"key":"Yh5kxrSKCT"}],"key":"hIjDTsQ7eI"},{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Compiles: Ensures the model is compiled with the correct learning rate (either default or from best_hps).","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"kOUnZzdv5y"}],"key":"rJjk4bjkc8"}],"key":"nc1A9mwyFd"},{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Callbacks: Sets up EarlyStopping and ModelCheckpoint. Note that ModelCheckpoint now saves the entire model in the .keras format (recommended over .h5), as we are not using the custom subclassed model in this part. The filename includes the model_type_name.","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"vHAxd10MqE"}],"key":"eYACqbTgSg"}],"key":"s35aB43C5u"},{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Fits Model: Calls ","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"L4k7FMAvhL"},{"type":"link","url":"http://model.fit","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"model.fit","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"szNSQWHk5Z"}],"urlSource":"http://model.fit","key":"iybo2aY42N"},{"type":"text","value":"(), passing the class_weight dictionary if applicable.","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"kj0TtvL81e"}],"key":"DeteCJQpyl"}],"key":"FWkJJnfIcB"},{"type":"listItem","spread":true,"position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Loads Best: After training, it explicitly loads the best model saved by ModelCheckpoint to ensure the returned model represents the best validation performance.","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"TX4PrYslO2"}],"key":"OaA24hqdND"}],"key":"nblinvKVWB"},{"type":"listItem","spread":true,"position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Returns: The trained model object and the training history.","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"WdCljrInsW"}],"key":"Cv13S8el6m"}],"key":"jJ5dAl07CB"}],"key":"Rs6oV7oWSt"}],"key":"gtTIje24yW"},{"type":"listItem","spread":true,"position":{"start":{"line":16,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Main Execution:","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"taK2xfwjnG"}],"key":"mpAsaDfoT3"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":17,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Checks that necessary data (X_train, etc.) and configuration flags exist.","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"OPzFdXohng"}],"key":"kM60uaCxdC"}],"key":"PCKrQOHZVl"},{"type":"listItem","spread":true,"position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Determines the model_type_name based on whether tuning was performed and class weights are used (e.g., “baseline”, “baseline_weighted”, “tuned”, “tuned_weighted”).","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"wX0TrOr4An"}],"key":"nyEm0f9XtH"}],"key":"AMsTM7zYKx"},{"type":"listItem","spread":true,"position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Calls the train_model function with the appropriate arguments.","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"YDQ6yzhDoy"}],"key":"y5i087fwtB"}],"key":"CRa3NFdWdi"},{"type":"listItem","spread":true,"position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Stores the returned trained model in the trained_model variable.","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"D4K98i2oQP"}],"key":"Kry6AuHOuI"}],"key":"S9RvFLcOye"},{"type":"listItem","spread":true,"position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Plots the training/validation loss and accuracy from the returned history object.","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"N2msIlgi0A"}],"key":"MmVZeTKCHw"}],"key":"WX1Wa2Idf9"},{"type":"listItem","spread":true,"position":{"start":{"line":22,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Sets a general model_ready_for_eval flag for the next step (Part 10: Baseline Evaluation).","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"gTHTqRTU9F"}],"key":"lNPop8t7qM"}],"key":"sHhniYGvQi"}],"key":"p1DdEAXwn6"}],"key":"RYjT0ocd7v"}],"key":"GgEwuyPIWT"},{"type":"paragraph","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"text","value":"After running this cell, the trained_model variable will hold the trained LSTM model (either using default or tuned hyperparameters, and potentially trained with class weights), ready for evaluation on the test set in Part 10. The best version of this model is also saved to a file.","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"key":"SoBHsNnTpU"}],"key":"MCw2OlCBxO"}],"identifier":"n5nvbjk-leet","label":"N5nvBjk-leEt","html_id":"n5nvbjk-leet","key":"X7IoUTudr1"},{"type":"block","kind":"notebook-code","data":{"ExecuteTime":{"end_time":"2026-01-07T03:44:43.377227200Z","start_time":"2026-01-07T03:37:30.421Z"},"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"pZozx1KVj7t0","outputId":"8ae10d94-45eb-44c1-a42e-9d7ee94305cf"},"children":[{"type":"code","lang":"python","executable":true,"value":"def train_final_model(x_train, y_train, x_val, y_val,\n                      input_shape,\n                      use_class_weight=False,\n                      best_hps=None,\n                      epochs=50,\n                      batch_size=32,\n                      patience=10,\n                      checkpoint_path_template='output/model_{}.keras',\n                      model_name='baseline'):\n\n    print(f\"\\n--- Training Final Model: {model_name} ---\")\n\n    # 1. Build Model\n    # We assume build_lstm_model is available from Step 7\n    if 'build_lstm_model' not in globals():\n        raise NameError(\"build_lstm_model function not defined. Please run Step 7.\")\n\n    model = build_lstm_model(input_shape, hp=best_hps)\n    if model is None:\n        return None, None\n\n    # 2. Calculate Class Weights (Critical for Blooms)\n    class_weights_dict = None\n    if use_class_weight:\n        print(\"Calculating class weights for imbalanced data...\")\n        try:\n            # Flatten to ensure 1D array for weight calculation\n            y_flat = y_train.flatten().astype(int)\n            classes = np.unique(y_flat)\n            weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_flat)\n            class_weights_dict = dict(zip(classes, weights))\n            print(f\"  Class weights: {class_weights_dict}\")\n        except Exception as e:\n            print(f\"  Error calculating weights: {e}. Using default.\")\n\n    # 3. Compile Model with BETTER METRICS\n    # We add AUC, Precision, and Recall to see if it actually finds blooms\n    learning_rate = best_hps.get('learning_rate') if best_hps else DEFAULT_LEARNING_RATE\n\n    # Re-compile to ensure metrics and optimizer are fresh\n    model.compile(\n        optimizer=Adam(learning_rate=learning_rate),\n        loss=BinaryCrossentropy(),\n        metrics=[\n            BinaryAccuracy(name='accuracy'),\n            AUC(name='auc'),           # \u003c--- Critical for Imbalanced Data\n            Precision(name='precision'),\n            Recall(name='recall')\n        ]\n    )\n    print(f\"Model compiled (LR={learning_rate}). Monitoring AUC/Precision/Recall.\")\n\n    # 4. Setup Callbacks\n    model_path = checkpoint_path_template.format(model_name)\n\n    # EarlyStopping: restore_best_weights=True ensures 'model' var is perfect at the end\n    early_stopping = EarlyStopping(\n        monitor='val_loss',\n        patience=patience,\n        verbose=1,\n        restore_best_weights=True\n    )\n\n    model_checkpoint = ModelCheckpoint(\n        filepath=model_path,\n        monitor='val_loss',\n        save_best_only=True,\n        verbose=1\n    )\n\n    # 5. Train\n    print(f\"\\nStarting training for {epochs} epochs...\")\n\n    # Ensure raw numpy arrays (safer than Tensors here)\n    x_train = np.array(x_train).astype('float32')\n    y_train = np.array(y_train).astype('float32')\n    x_val = np.array(x_val).astype('float32')\n    y_val = np.array(y_val).astype('float32')\n\n    history = model.fit(\n        x_train, y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        validation_data=(x_val, y_val),\n        callbacks=[early_stopping, model_checkpoint],\n        class_weight=class_weights_dict,\n        verbose=1\n    )\n\n    print(f\"\\n--- Training Finished. Best model saved to {model_path} ---\")\n\n    # Because restore_best_weights=True, 'model' is already the best version.\n    return model, history\n\n\n# --- Execute Training ---\nif __name__ == \"__main__\":\n    try:\n        # Check prerequisites\n        if 'X_train' not in locals(): raise NameError(\"Run Step 6 (Sequence Creation) first.\")\n\n        # Define Input Shape\n        input_shape_train = (X_train.shape[1], X_train.shape[2])\n\n        # Naming logic\n        model_name = \"tuned\" if (PERFORM_TUNING and best_hps) else \"baseline\"\n        if USE_CLASS_WEIGHT: model_name += \"_weighted\"\n\n        # TRAIN\n        trained_model, training_history = train_final_model(\n            X_train, y_train, X_val, y_val,\n            input_shape=input_shape_train,\n            use_class_weight=USE_CLASS_WEIGHT,\n            best_hps=best_hps if PERFORM_TUNING else None,\n            epochs=TRAIN_EPOCHS,\n            batch_size=TRAIN_BATCH_SIZE,\n            patience=TRAIN_PATIENCE,\n            checkpoint_path_template=MODEL_CHECKPOINT_TEMPLATE,\n            model_name=model_name\n        )\n\n        if trained_model:\n            # --- PUBLICATION QUALITY PLOTTING ---\n            print(f\"\\nGeneratng publication-quality plots for {model_name}...\")\n\n            import matplotlib.pyplot as plt\n            import os  # \u003c--- Need this to create folders\n\n            # 1. CRITICAL FIX: Create the output directory if it doesn't exist\n            os.makedirs('output', exist_ok=True)\n\n            # 2. Global Style Settings for Academic Papers\n            plt.rcdefaults()\n            params = {\n                'font.family': 'serif',\n                'axes.titlesize': 18,\n                'axes.labelsize': 16,\n                'xtick.labelsize': 14,\n                'ytick.labelsize': 14,\n                'legend.fontsize': 14,\n                'figure.figsize': (10, 6),\n                'lines.linewidth': 2.5\n            }\n            plt.rcParams.update(params)\n\n            # --- FIGURE 1: LOSS CURVE ---\n            plt.figure(dpi=300)\n\n            plt.plot(training_history.history['loss'], label='Training Loss', color='#1f77b4', linestyle='-')\n            plt.plot(training_history.history['val_loss'], label='Validation Loss', color='#d62728', linestyle='--')\n\n            plt.title('Model Loss over Epochs')\n            plt.xlabel('Epoch')\n            plt.ylabel('Binary Crossentropy Loss')\n            plt.legend(frameon=True, fancybox=False, edgecolor='black')\n            plt.grid(True, linestyle=':', alpha=0.6)\n            plt.tight_layout()\n\n            # Save safely now that folder exists\n            plt.savefig(f\"output/plot_loss_{model_name}.png\", bbox_inches='tight')\n            plt.show()\n            print(f\"Saved: output/plot_loss_{model_name}.png\")\n\n            # --- FIGURE 2: AUC / PERFORMANCE CURVE ---\n            plt.figure(dpi=300)\n\n            if 'val_auc' in training_history.history:\n                metric_name = 'AUC'\n                train_data = training_history.history['auc']\n                val_data = training_history.history['val_auc']\n                color_train = '#2ca02c'\n                color_val = '#ff7f0e'\n            else:\n                metric_name = 'Accuracy'\n                train_data = training_history.history['accuracy']\n                val_data = training_history.history['val_accuracy']\n                color_train = 'black'\n                color_val = 'gray'\n\n            plt.plot(train_data, label=f'Training {metric_name}', color=color_train, linestyle='-')\n            plt.plot(val_data, label=f'Validation {metric_name}', color=color_val, linestyle='--')\n\n            plt.title(f'Model {metric_name} Performance')\n            plt.xlabel('Epoch')\n            plt.ylabel(f'{metric_name} Score')\n            plt.legend(frameon=True, fancybox=False, edgecolor='black')\n            plt.grid(True, linestyle=':', alpha=0.6)\n            plt.tight_layout()\n\n            plt.savefig(f\"output/plot_{metric_name.lower()}_{model_name}.png\", bbox_inches='tight')\n            plt.show()\n            print(f\"Saved: output/plot_{metric_name.lower()}_{model_name}.png\")\n\n            model_ready_for_eval = True\n        else:\n            model_ready_for_eval = False\n\n    except Exception as e:\n         print(f\"Error: {e}\")\n         model_ready_for_eval = False","identifier":"pzozx1kvj7t0-code","enumerator":"13","html_id":"pzozx1kvj7t0-code","key":"PPPw56tJ7B"},{"type":"outputs","id":"UJjVsCVTMDcr8MfqzX50C","children":[{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"\n--- Training Final Model: tuned_weighted ---\nCalculating class weights for imbalanced data...\n  Class weights: {np.int64(0): np.float64(0.6527614571092832), np.int64(1): np.float64(2.1365384615384615)}\nModel compiled (LR=0.01). Monitoring AUC/Precision/Recall.\n\nStarting training for 50 epochs...\nEpoch 1/50\n\u001b[1m33/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7266 - auc: 0.7623 - loss: 0.5681 - precision: 0.4174 - recall: 0.6844\nEpoch 1: val_loss improved from inf to 0.47479, saving model to output_refactored/best_lstm_model_tuned_weighted.keras\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.7291 - auc: 0.7679 - loss: 0.5629 - precision: 0.4231 - recall: 0.6897 - val_accuracy: 0.7149 - val_auc: 0.9209 - val_loss: 0.4748 - val_precision: 0.5921 - val_recall: 0.9677\nEpoch 2/50\n\u001b[1m33/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8109 - auc: 0.9159 - loss: 0.3887 - precision: 0.5893 - recall: 0.9056\nEpoch 2: val_loss improved from 0.47479 to 0.38012, saving model to output_refactored/best_lstm_model_tuned_weighted.keras\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8106 - auc: 0.9141 - loss: 0.3908 - precision: 0.5869 - recall: 0.9001 - val_accuracy: 0.7851 - val_auc: 0.9227 - val_loss: 0.3801 - val_precision: 0.6930 - val_recall: 0.8495\nEpoch 3/50\n\u001b[1m33/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8125 - auc: 0.8956 - loss: 0.4167 - precision: 0.5762 - recall: 0.7902\nEpoch 3: val_loss did not improve from 0.38012\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8129 - auc: 0.8958 - loss: 0.4158 - precision: 0.5765 - recall: 0.7911 - val_accuracy: 0.7500 - val_auc: 0.9261 - val_loss: 0.5218 - val_precision: 0.6200 - val_recall: 1.0000\nEpoch 4/50\n\u001b[1m34/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8166 - auc: 0.9158 - loss: 0.3768 - precision: 0.5881 - recall: 0.8814\nEpoch 4: val_loss did not improve from 0.38012\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8169 - auc: 0.9158 - loss: 0.3764 - precision: 0.5876 - recall: 0.8810 - val_accuracy: 0.7982 - val_auc: 0.9179 - val_loss: 0.4197 - val_precision: 0.6975 - val_recall: 0.8925\nEpoch 5/50\n\u001b[1m33/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8182 - auc: 0.9030 - loss: 0.3871 - precision: 0.5642 - recall: 0.8112\nEpoch 5: val_loss did not improve from 0.38012\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8201 - auc: 0.9041 - loss: 0.3855 - precision: 0.5687 - recall: 0.8137 - val_accuracy: 0.8026 - val_auc: 0.9248 - val_loss: 0.3869 - val_precision: 0.7143 - val_recall: 0.8602\nEpoch 6/50\n\u001b[1m33/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8601 - auc: 0.9436 - loss: 0.3022 - precision: 0.6464 - recall: 0.8500\nEpoch 6: val_loss did not improve from 0.38012\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8589 - auc: 0.9429 - loss: 0.3041 - precision: 0.6444 - recall: 0.8512 - val_accuracy: 0.8070 - val_auc: 0.9126 - val_loss: 0.4281 - val_precision: 0.6992 - val_recall: 0.9247\nEpoch 7/50\n\u001b[1m34/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8743 - auc: 0.9397 - loss: 0.3216 - precision: 0.6860 - recall: 0.8777\nEpoch 7: val_loss did not improve from 0.38012\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8737 - auc: 0.9396 - loss: 0.3214 - precision: 0.6845 - recall: 0.8772 - val_accuracy: 0.8070 - val_auc: 0.9049 - val_loss: 0.4391 - val_precision: 0.7130 - val_recall: 0.8817\nEpoch 8/50\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8945 - auc: 0.9622 - loss: 0.2541 - precision: 0.7110 - recall: 0.9191\nEpoch 8: val_loss did not improve from 0.38012\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8941 - auc: 0.9618 - loss: 0.2551 - precision: 0.7106 - recall: 0.9175 - val_accuracy: 0.7675 - val_auc: 0.8939 - val_loss: 0.4210 - val_precision: 0.6786 - val_recall: 0.8172\nEpoch 9/50\n\u001b[1m34/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8959 - auc: 0.9683 - loss: 0.2292 - precision: 0.6880 - recall: 0.9288\nEpoch 9: val_loss did not improve from 0.38012\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8952 - auc: 0.9677 - loss: 0.2309 - precision: 0.6884 - recall: 0.9272 - val_accuracy: 0.7939 - val_auc: 0.8991 - val_loss: 0.4283 - val_precision: 0.7255 - val_recall: 0.7957\nEpoch 10/50\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9089 - auc: 0.9766 - loss: 0.1960 - precision: 0.7378 - recall: 0.9421\nEpoch 10: val_loss did not improve from 0.38012\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9084 - auc: 0.9763 - loss: 0.1970 - precision: 0.7368 - recall: 0.9415 - val_accuracy: 0.7675 - val_auc: 0.8948 - val_loss: 0.6039 - val_precision: 0.6493 - val_recall: 0.9355\nEpoch 11/50\n\u001b[1m33/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8969 - auc: 0.9649 - loss: 0.2501 - precision: 0.7243 - recall: 0.9167\nEpoch 11: val_loss did not improve from 0.38012\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8975 - auc: 0.9647 - loss: 0.2498 - precision: 0.7251 - recall: 0.9172 - val_accuracy: 0.7807 - val_auc: 0.9039 - val_loss: 0.4826 - val_precision: 0.6720 - val_recall: 0.9032\nEpoch 12/50\n\u001b[1m33/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9140 - auc: 0.9797 - loss: 0.1927 - precision: 0.7393 - recall: 0.9427\nEpoch 12: val_loss did not improve from 0.38012\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9143 - auc: 0.9795 - loss: 0.1926 - precision: 0.7418 - recall: 0.9412 - val_accuracy: 0.8070 - val_auc: 0.8981 - val_loss: 0.4655 - val_precision: 0.7248 - val_recall: 0.8495\nEpoch 12: early stopping\nRestoring model weights from the end of the best epoch: 2.\n\n--- Training Finished. Best model saved to output_refactored/best_lstm_model_tuned_weighted.keras ---\n\nGeneratng publication-quality plots for tuned_weighted...\n"},"children":[],"identifier":"pzozx1kvj7t0-outputs-0","html_id":"pzozx1kvj7t0-outputs-0","key":"r9XVKF5uZf"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"\u003cFigure size 3000x1800 with 1 Axes\u003e","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"b86e62b0e945d1d21567d642183f78cb","path":"/build/b86e62b0e945d1d21567d642183f78cb.png"}}},"children":[],"identifier":"pzozx1kvj7t0-outputs-1","html_id":"pzozx1kvj7t0-outputs-1","key":"Y1G5lzbEDn"},{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"Saved: output/plot_loss_tuned_weighted.png\n"},"children":[],"identifier":"pzozx1kvj7t0-outputs-2","html_id":"pzozx1kvj7t0-outputs-2","key":"O0uaJ6TRmJ"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"\u003cFigure size 3000x1800 with 1 Axes\u003e","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"d6cb9c7ac05f37493a8bf8222db5cc21","path":"/build/d6cb9c7ac05f37493a8bf8222db5cc21.png"}}},"children":[],"identifier":"pzozx1kvj7t0-outputs-3","html_id":"pzozx1kvj7t0-outputs-3","key":"lqjqk3YwMy"},{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"Saved: output/plot_auc_tuned_weighted.png\n"},"children":[],"identifier":"pzozx1kvj7t0-outputs-4","html_id":"pzozx1kvj7t0-outputs-4","key":"Ocpoo1I0mi"}],"identifier":"pzozx1kvj7t0-outputs","html_id":"pzozx1kvj7t0-outputs","key":"YcojEai9I8"}],"identifier":"pzozx1kvj7t0","label":"pZozx1KVj7t0","html_id":"pzozx1kvj7t0","key":"rRxp2IhV6h"},{"type":"block","kind":"notebook-content","data":{"id":"SwZcbfubl7Br"},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Part 10: Baseline Evaluation","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QeILWRfabu"}],"identifier":"part-10-baseline-evaluation","label":"Part 10: Baseline Evaluation","html_id":"part-10-baseline-evaluation","implicit":true,"key":"uziWmP7ejy"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"This cell defines the evaluation function and applies it to the model trained in Part 9 (trained_model). It calculates and prints various classification metrics, plots the confusion matrix and predictions over time, and assesses the model’s ability to predict bloom onsets.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"a6ZeMUSrAP"}],"key":"Gwb1eKCOzn"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Explanation:","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"Pf1fAuP6ru"}],"key":"cABDduhBhh"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":7,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Function evaluate_model:","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"j4eiPwQc9P"}],"key":"zI6pJfCVZA"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":8,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Takes the model object, test data (x_test, y_test), the unscaled test dataframe test_df_unscaled (needed for correct timestamps), sequence length, forecast horizon, and a model_name string as input.","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"MDaCfBAHQF"}],"key":"MBDljbrOOe"}],"key":"QPKfxleusl"},{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Predictions: Runs model.predict(x_test) to get probabilities. Converts probabilities to class predictions using a 0.5 threshold.","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"itdp4A4GnX"}],"key":"ql9U0Zsc3m"}],"key":"X2tWPXGADO"},{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Metrics Calculation: Uses scikit-learn functions to calculate accuracy, precision, recall, F1-score (for the positive class ‘1’), AUC-ROC (using probabilities), and Brier score (using probabilities). Includes error handling for AUC calculation if only one class is present in y_test.","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"hvJiriPl2I"}],"key":"RB2SOGh4ZF"}],"key":"XEbUoBWyPY"},{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Reporting: Prints a formatted summary of the metrics and the full classification report.\\","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"DFXrv2RsxN"}],"key":"P0idTVHtra"}],"key":"mCo94n0hah"},{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Confusion Matrix: Calculates and plots the confusion matrix using seaborn for better visualization.","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"vLvHoVjJIJ"}],"key":"USWo5xd64A"}],"key":"OR5AFmGyOJ"},{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Time Series Plot: Determines the correct start index in the test_df_unscaled index based on sequence length and forecast horizon to align the predictions (y_pred_prob) with the actual dates. Plots actuals vs. predicted probabilities. Includes checks for length mismatches.","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"cEFR2wKO93"}],"key":"zTGKCxJfRO"}],"key":"mhpMPQVKPX"},{"type":"listItem","spread":true,"position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Onset Analysis: Implements the logic to find actual bloom onsets (0 -\u003e 1 transitions in y_test) and checks if the model predicted a bloom (class 1) either in the week before the onset or during the onset week. Reports the hit rate.","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"BX8UA67ZDV"}],"key":"CXYmzQGCYn"}],"key":"MDUeTpZpSp"}],"key":"xk05EwGomb"}],"key":"tspLlanVql"},{"type":"listItem","spread":true,"position":{"start":{"line":15,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Main Execution:","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"Tdhbi7CHVY"}],"key":"vdZFUrCYAP"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":16,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Checks if the necessary variables (trained_model, X_test, y_test, test_df, etc.) exist from previous steps.","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"D6CWPrggV9"}],"key":"AdYexXaTr0"}],"key":"RGU6mLi6Kq"},{"type":"listItem","spread":true,"position":{"start":{"line":17,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Calls the evaluate_model function, passing the required arguments. Uses the model_type_trained variable (set at the end of Part 9) to label the output correctly.","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"wV046hErKv"}],"key":"sEGQSr9iDq"}],"key":"txqEAlmYry"}],"key":"evpvfDfPX5"}],"key":"uuSMFuxW6Q"}],"key":"k2vTn5d4cS"},{"type":"paragraph","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"text","value":"After running this cell, you will have a comprehensive evaluation of the model trained in Part 9, including standard metrics, plots, and the crucial bloom onset performance assessment. This provides the baseline against which the EnKF-enhanced model (Part 12/13) will be compared.","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"KBPbLEKkxB"}],"key":"mfOnx55Gdf"}],"identifier":"swzcbfubl7br","label":"SwZcbfubl7Br","html_id":"swzcbfubl7br","key":"RVEzulOJrU"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"mByYeroPl_Va","outputId":"89218487-72ba-4a9c-c2e5-2a9b6c5912f6","ExecuteTime":{"end_time":"2026-01-07T03:44:43.377227200Z","start_time":"2026-01-07T03:37:37.043182Z"}},"children":[{"type":"code","lang":"python","executable":true,"value":"def evaluate_model(model, x_test, y_test, test_df_unscaled,\n                   seq_len, forecast_horizon, model_name=\"Model\"):\n    \"\"\"\n    Evaluates model with Optimal Threshold detection and Publication Plots.\n    Saves figures to 'output/' folder.\n    \"\"\"\n    if model is None or x_test is None or y_test is None:\n        print(\"Error: Missing model or test data.\")\n        return\n\n    # Ensure output directory exists\n    os.makedirs('output', exist_ok=True)\n\n    print(f\"\\n=== Evaluating Model: {model_name} ===\")\n\n    # 1. Generate Probabilities\n    print(\"1. Generating predictions...\")\n    try:\n        # Get raw probabilities (0.0 to 1.0)\n        y_pred_prob = model.predict(x_test, verbose=0)\n        y_test_eval = y_test.reshape(-1, 1) # Ensure 2D shape\n    except Exception as e:\n        print(f\"Error during prediction: {e}\")\n        return\n\n    # 2. Find Optimal Threshold (Fixed Math)\n    # ------------------------------------------------------\n    precision, recall, thresholds = precision_recall_curve(y_test_eval, y_pred_prob)\n\n    # Calculate F-Score safely (avoid 0/0 division)\n    numerator = 2 * precision * recall\n    denominator = precision + recall\n    with np.errstate(divide='ignore', invalid='ignore'):\n        fscore = np.divide(numerator, denominator)\n    fscore = np.nan_to_num(fscore) # Replace NaNs with 0\n\n    # Locate the index of the largest F score\n    ix = np.argmax(fscore)\n\n    # Safety check for index bounds\n    if ix \u003e= len(thresholds):\n        best_thresh = thresholds[-1]\n    else:\n        best_thresh = thresholds[ix]\n\n    print(f\"\\nOptimal Decision Threshold: {best_thresh:.4f} (Max F1: {fscore[ix]:.4f})\")\n    # ------------------------------------------------------\n\n    # Apply Best Threshold\n    y_pred_class = (y_pred_prob \u003e= best_thresh).astype(int)\n\n    # 3. Calculate Metrics (at Best Threshold)\n    print(\"\\n2. Performance Metrics (at Optimal Threshold):\")\n    try:\n        acc = accuracy_score(y_test_eval, y_pred_class)\n        prec = precision_score(y_test_eval, y_pred_class, zero_division=0)\n        rec = recall_score(y_test_eval, y_pred_class, zero_division=0)\n        f1 = f1_score(y_test_eval, y_pred_class, zero_division=0)\n        auc = roc_auc_score(y_test_eval, y_pred_prob)\n        brier = brier_score_loss(y_test_eval, y_pred_prob)\n\n        print(f\"  Accuracy:    {acc:.4f}\")\n        print(f\"  Precision:   {prec:.4f}\")\n        print(f\"  Recall:      {rec:.4f}\")\n        print(f\"  F1-Score:    {f1:.4f}\")\n        print(f\"  AUC-ROC:     {auc:.4f}\")\n        print(f\"  Brier Score: {brier:.4f}\")\n\n        print(f\"\\nClassification Report:\\n{classification_report(y_test_eval, y_pred_class, target_names=['No Bloom', 'Bloom'])}\")\n\n    except Exception as e:\n        print(f\"Metric calculation error: {e}\")\n\n    # 4. Publication-Quality Plots\n    # ----------------------------\n    # Apply Academic Style\n    plt.rcdefaults()\n    params = {'font.family': 'serif', 'figure.figsize': (8, 6), 'figure.dpi': 300,\n              'axes.labelsize': 14, 'axes.titlesize': 16, 'xtick.labelsize': 12, 'ytick.labelsize': 12}\n    plt.rcParams.update(params)\n\n    # --- Plot A: Confusion Matrix ---\n    try:\n        cm = confusion_matrix(y_test_eval, y_pred_class)\n        plt.figure()\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n                    xticklabels=['Pred: No Bloom', 'Pred: Bloom'],\n                    yticklabels=['Actual: No Bloom', 'Actual: Bloom'],\n                    annot_kws={\"size\": 14})\n        plt.title(f'Confusion Matrix ({model_name})\\nThreshold: {best_thresh:.2f}')\n        plt.tight_layout()\n\n        # SAVE FIGURE\n        save_path = f\"output/confusion_matrix_{model_name}.png\"\n        plt.savefig(save_path, bbox_inches='tight')\n        print(f\"Saved: {save_path}\")\n        plt.show()\n    except Exception as e:\n        print(f\"CM Plot Error: {e}\")\n\n    # --- Plot B: Time Series Predictions ---\n    print(f\"\\n3. Plotting Time Series Predictions...\")\n    start_idx = seq_len + forecast_horizon - 1\n    num_preds = len(y_pred_prob)\n\n    if start_idx + num_preds \u003c= len(test_df_unscaled):\n        dates = test_df_unscaled.index[start_idx : start_idx + num_preds]\n\n        plt.figure(figsize=(12, 6))\n        # Plot Actuals (Black Line)\n        plt.plot(dates, y_test_eval[:num_preds], label='Actual Bloom', color='black', alpha=0.6, linewidth=1.5)\n        # Plot Probabilities (Red Line)\n        plt.plot(dates, y_pred_prob[:num_preds], label='Predicted Probability', color='#d62728', alpha=0.8, linewidth=1.5)\n        # Threshold Line\n        plt.axhline(best_thresh, color='gray', linestyle='--', label=f'Threshold ({best_thresh:.2f})')\n\n        plt.title(f'Bloom Predictions vs Actuals ({forecast_horizon}-Step Horizon)')\n        plt.ylabel('Bloom Probability')\n        plt.xlabel('Date')\n        plt.legend(frameon=True, fancybox=False, edgecolor='black', loc='upper right')\n        plt.grid(True, linestyle=':', alpha=0.5)\n        plt.tight_layout()\n\n        # SAVE FIGURE\n        save_path = f\"output/timeseries_preds_{model_name}.png\"\n        plt.savefig(save_path, bbox_inches='tight')\n        print(f\"Saved: {save_path}\")\n        plt.show()\n    else:\n        print(\"Warning: Could not align dates for plotting.\")\n\n    # 5. Bloom Onset Analysis (Custom Logic)\n    print(f\"\\n4. Bloom Onset Analysis (Did we catch the start?)\")\n    y_test_flat = y_test_eval.flatten()\n    y_pred_flat = y_pred_class.flatten()\n\n    onsets = np.where((y_test_flat[:-1] == 0) \u0026 (y_test_flat[1:] == 1))[0] + 1\n    total_onsets = len(onsets)\n\n    hits = 0\n    late_hits = 0\n\n    if total_onsets \u003e 0:\n        for idx in onsets:\n            # Check window: [Week Before, Week Of, Week After]\n            pred_before = y_pred_flat[idx-1] if idx \u003e 0 else 0\n            pred_on     = y_pred_flat[idx]\n            pred_after  = y_pred_flat[idx+1] if idx+1 \u003c len(y_pred_flat) else 0\n\n            if pred_before == 1 or pred_on == 1:\n                hits += 1 # Success (Early or On Time)\n            elif pred_after == 1:\n                late_hits += 1 # Late by 1 week\n\n        print(f\"  Total Onsets: {total_onsets}\")\n        print(f\"  Caught Early/On-Time: {hits} ({hits/total_onsets:.1%})\")\n        print(f\"  Caught 1-Week Late:   {late_hits}\")\n        print(f\"  Missed Completely:    {total_onsets - hits - late_hits}\")\n    else:\n        print(\"  No bloom onsets found in Test Data.\")\n\n# --- Main Execution ---\nif __name__ == \"__main__\":\n    if 'trained_model' in locals() and 'X_test' in locals():\n        # Ensure data is numpy float32\n        X_test = np.array(X_test, dtype='float32')\n        y_test = np.array(y_test, dtype='float32')\n\n        evaluate_model(\n            model=trained_model,\n            x_test=X_test,\n            y_test=y_test,\n            test_df_unscaled=test_df, # Must be defined in Step 4\n            seq_len=SEQUENCE_LENGTH,\n            forecast_horizon=FORECAST_HORIZON,\n            model_name=\"LSTM_Baseline\"\n        )\n    else:\n        print(\"Error: Prerequisites (trained_model, X_test) not found.\")","identifier":"mbyyeropl_va-code","enumerator":"14","html_id":"mbyyeropl-va-code","key":"ZB6zkCB4gf"},{"type":"outputs","id":"DModX9QO6qivotM7RI_oG","children":[{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"\n=== Evaluating Model: LSTM_Baseline ===\n1. Generating predictions...\n\nOptimal Decision Threshold: 0.7803 (Max F1: 0.8662)\n\n2. Performance Metrics (at Optimal Threshold):\n  Accuracy:    0.9087\n  Precision:   0.8395\n  Recall:      0.8947\n  F1-Score:    0.8662\n  AUC-ROC:     0.9532\n  Brier Score: 0.1238\n\nClassification Report:\n              precision    recall  f1-score   support\n\n    No Bloom       0.95      0.92      0.93       154\n       Bloom       0.84      0.89      0.87        76\n\n    accuracy                           0.91       230\n   macro avg       0.89      0.91      0.90       230\nweighted avg       0.91      0.91      0.91       230\n\nSaved: output/confusion_matrix_LSTM_Baseline.png\n"},"children":[],"identifier":"mbyyeropl_va-outputs-0","html_id":"mbyyeropl-va-outputs-0","key":"PV36bPH7uq"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"\u003cFigure size 2400x1800 with 1 Axes\u003e","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"ef8770c7c1e7a96bc41ace227ceb41d8","path":"/build/ef8770c7c1e7a96bc41ace227ceb41d8.png"}}},"children":[],"identifier":"mbyyeropl_va-outputs-1","html_id":"mbyyeropl-va-outputs-1","key":"JERwndAvly"},{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"\n3. Plotting Time Series Predictions...\nSaved: output/timeseries_preds_LSTM_Baseline.png\n"},"children":[],"identifier":"mbyyeropl_va-outputs-2","html_id":"mbyyeropl-va-outputs-2","key":"wvwMO9uyK4"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"\u003cFigure size 3600x1800 with 1 Axes\u003e","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"0e88d802c5f2b8367e0a5d7ed6061a4d","path":"/build/0e88d802c5f2b8367e0a5d7ed6061a4d.png"}}},"children":[],"identifier":"mbyyeropl_va-outputs-3","html_id":"mbyyeropl-va-outputs-3","key":"x7oGfJpTT2"},{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"\n4. Bloom Onset Analysis (Did we catch the start?)\n  Total Onsets: 10\n  Caught Early/On-Time: 6 (60.0%)\n  Caught 1-Week Late:   2\n  Missed Completely:    2\n"},"children":[],"identifier":"mbyyeropl_va-outputs-4","html_id":"mbyyeropl-va-outputs-4","key":"jXasBHK0sA"}],"identifier":"mbyyeropl_va-outputs","html_id":"mbyyeropl-va-outputs","key":"rh1zEMorQo"}],"identifier":"mbyyeropl_va","label":"mByYeroPl_Va","html_id":"mbyyeropl-va","key":"x7Dllwr0tQ"},{"type":"block","kind":"notebook-content","data":{"id":"TYCWxz1amXpp"},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Part 11: EnKF Setup","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xoNa9Sw168"}],"identifier":"part-11-enkf-setup","label":"Part 11: EnKF Setup","html_id":"part-11-enkf-setup","implicit":true,"key":"W8CzNNndUV"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"This cell defines the EnsembleKalmanFilter class (assuming it’s not in a separate enkf.py file for simplicity here, but importing is better practice) and a function setup_enkf to initialize the filter for the test period. It calculates the initial state, noise matrices (based on the ENKF_NOISE_ESTIMATION setting from Part 1), and identifies the necessary column indices.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"TE1c6ZOt87"}],"key":"eCtKrCU85b"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Explanation:","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"Af3ZLtkXKl"}],"key":"JcN4dhm8F3"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":7,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"EnKF Class: Includes the EnsembleKalmanFilter class definition directly in the cell for convenience (assuming it’s not imported from enkf.py).","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"bTkeBqsKNY"}],"key":"CUtSaXW4M3"}],"key":"p2hcGWgNz0"},{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"setup_enkf Function:","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"ua3BJ6Be9P"}],"key":"q78195J0hC"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":11,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Takes necessary configurations and dataframes as input.","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"ss3sDDjaRs"}],"key":"aeh9br9o0p"}],"key":"udao4PWZC4"},{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Initial State/Covariance: Extracts the state vector values (x_initial) from the unscaled test_df at the correct time index (end of the first sequence’s lookback period). Calculates an initial covariance matrix P_initial based on relative uncertainty assumptions (these ratios are tunable).","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"HXvt7DGhBd"}],"key":"QxB9QHzGZx"}],"key":"Mm1dqbZ7og"},{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Noise Matrices (R, Q): Calculates the observation noise R and process noise Q based on the noise_estimation_method.","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"jMzNstBcwI"}],"key":"qqJiLNx4Pp"}],"key":"XWWWBOuMh5"},{"type":"listItem","spread":true,"position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"If ‘manual’, it uses MANUAL_R_DIAG and MANUAL_Q_DIAG (defined in Part 1).","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"i6c0l6jCRc"}],"key":"FsIvhjVqD3"}],"key":"FvBDTV7044"},{"type":"listItem","spread":true,"position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"If ‘basic_stats’, it calculates variances based on the mean (for R) and standard deviation of weekly changes (for Q) from the unscaled train_df, scaled by tunable percentage factors. Includes fallback logic for constant columns.","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"brz9i8OE1e"}],"key":"DgyWeGUzRn"}],"key":"cwDYFitskS"},{"type":"listItem","spread":true,"position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"State Indices: Finds the numerical index corresponding to each ENKF_STATE_VARS within the final_feature_columns_used list (which defines the order in the scaled data and LSTM input). This is crucial for correctly updating the LSTM input later.","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"tXqtdIPdJ4"}],"key":"sHsqXLa2kF"}],"key":"dYsWZd1LNj"},{"type":"listItem","spread":true,"position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Initialization: Creates an instance of the EnsembleKalmanFilter class using the calculated x_initial, P_initial, and other parameters. Stores the calculated R and Q matrices as attributes of the instance for easy access in the next step.","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"ZO3OozCHEM"}],"key":"IcZx9NfSVr"}],"key":"MAuutl9i3R"},{"type":"listItem","spread":true,"position":{"start":{"line":18,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Returns: The initialized enkf object and the list of enkf_state_indices.","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"h4riLL0vCX"}],"key":"qDvtsC96pb"}],"key":"M3JJEbCet0"}],"key":"XQawx4ra5h"}],"key":"tjfkDCZzWX"},{"type":"listItem","spread":true,"position":{"start":{"line":20,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"text","value":"Main Execution:","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"vPfDhfRpt1"}],"key":"eitnAp9se0"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":21,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Checks for prerequisite variables from previous steps.","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"TnTnBwEmIn"}],"key":"sOAlY4RYVl"}],"key":"dNf12NigQ0"},{"type":"listItem","spread":true,"position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Calls setup_enkf with the appropriate arguments from the configuration (Part 1) and dataframes (Parts 4, 5).","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"ppKMlYtS4O"}],"key":"n6DhG1bXQg"}],"key":"VD3LFOlwbV"},{"type":"listItem","spread":true,"position":{"start":{"line":23,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Stores the returned enkf_instance and enkf_indices for use in the forecasting loop (Part 12).","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"ANy3nDqGPR"}],"key":"ZuWOOR3ADC"}],"key":"A7MHPxpd5A"}],"key":"P1j761wpTZ"}],"key":"h1RVrnPbuR"}],"key":"GKq29dkcoa"},{"type":"paragraph","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"text","value":"After running this cell, the enkf_instance is ready, configured with initial conditions and noise characteristics appropriate for the start of the test period. The enkf_indices list tells us which columns in the LSTM input sequence correspond to the variables being updated by the EnKF.","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"wVi7gLjecd"}],"key":"Rd7OuMjOR2"}],"identifier":"tycwxz1amxpp","label":"TYCWxz1amXpp","html_id":"tycwxz1amxpp","key":"a9lwavR8rp"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"z4EXqf8cmcLW","outputId":"452405b5-d00b-4ffd-c430-335bc6ec6eb0","jupyter":{"is_executing":true}},"children":[{"type":"code","lang":"python","executable":true,"value":"class EnsembleKalmanFilter:\n    \"\"\"\n    Stochastic Ensemble Kalman Filter (Burgers et al., 1998).\n    Includes observation perturbation to maintain ensemble variance.\n    \"\"\"\n    def __init__(self, x_init, P_init, dim_z, N):\n        \"\"\"\n        Args:\n            x_init: Initial state mean (dim_x,)\n            P_init: Initial state covariance (dim_x, dim_x)\n            dim_z:  Dimension of observations\n            N:      Number of ensemble members\n        \"\"\"\n        self.dim_x = len(x_init)\n        self.dim_z = dim_z\n        self.N = N\n        self.x = None # Current state mean\n        self.P = None # Current state covariance\n\n        # Initialize Ensemble\n        # We enforce symmetry and positive-definiteness on P_init\n        P_init = (P_init + P_init.T) / 2\n        self.ensemble = self._multivariate_normal(x_init, P_init, N)\n\n        # Calculate initial stats\n        self.x = np.mean(self.ensemble, axis=0)\n        self.P = np.cov(self.ensemble.T)\n\n        print(f\"EnKF Initialized. Members: {N}, State Dim: {self.dim_x}\")\n\n    def predict(self, F_func, Q, dt=1):\n        \"\"\"\n        Propagate state forward: x = F(x) + Noise\n        \"\"\"\n        # 1. Apply Model Dynamics (The \"Physics\" or \"AI\")\n        # We apply F_func to every member of the ensemble\n        self.ensemble = np.apply_along_axis(F_func, 1, self.ensemble)\n\n        # 2. Add Process Noise (System Error)\n        if Q is not None:\n            Q = (Q + Q.T) / 2\n            noise = self._multivariate_normal(np.zeros(self.dim_x), Q, self.N)\n            self.ensemble += noise\n\n        # Update stats\n        self.x = np.mean(self.ensemble, axis=0)\n        self.P = np.cov(self.ensemble.T)\n\n    def update(self, z, R, H=None):\n        \"\"\"\n        Assimilate observation: x = x + K(z - Hx)\n        \"\"\"\n        if H is None: H = np.eye(self.dim_z, self.dim_x)\n\n        # 1. Project Ensemble to Observation Space\n        # Shape: (N, dim_z)\n        Hx = self.ensemble @ H.T\n\n        # 2. Perturb Observations (CRITICAL for Stochastic EnKF)\n        # We treat the observation as a random variable, not a single truth\n        R = (R + R.T) / 2\n        obs_noise = self._multivariate_normal(np.zeros(self.dim_z), R, self.N)\n        z_perturbed = z + obs_noise\n\n        # 3. Calculate Innovation (Residual)\n        # Difference between \"noisy measurement\" and \"predicted measurement\"\n        D = z_perturbed - Hx\n\n        # 4. Calculate Kalman Gain (K)\n        # P_zz = H P H' + R\n        # K = P H' (P_zz)^-1\n        P_prior = np.cov(self.ensemble.T)\n        P_zz = H @ P_prior @ H.T + R\n\n        # Use Pseudo-Inverse for stability\n        K = P_prior @ H.T @ np.linalg.pinv(P_zz)\n\n        # 5. Update Ensemble\n        # x_new = x_old + K * Innovation\n        self.ensemble = self.ensemble + (D @ K.T)\n\n        # Update stats\n        self.x = np.mean(self.ensemble, axis=0)\n        self.P = np.cov(self.ensemble.T)\n\n        return self.x, self.P\n\n    def _multivariate_normal(self, mean, cov, size):\n        \"\"\"Helper to sample safely, adding jitter if matrix is singular.\"\"\"\n        try:\n            return np.random.multivariate_normal(mean, cov, size)\n        except np.linalg.LinAlgError:\n            # Add small \"jitter\" to diagonal to fix numerical instability\n            print(\"Warning: Matrix not positive definite. Adding jitter.\")\n            epsilon = 1e-6 * np.eye(len(mean))\n            return np.random.multivariate_normal(mean, cov + epsilon, size)\n\n\n# --- 2. Setup Function (Robust with Manual Overrides) ---\ndef setup_enkf(enkf_state_vars, n_enkf, train_df, test_df_unscaled,\n               seq_len, feature_columns, manual_r_diag=None, manual_q_diag=None):\n    \"\"\"\n    Configures the EnKF.\n    Allows manual override of Q (Process) and R (Observation) noise diagonals.\n    \"\"\"\n    print(\"\\n--- Setting up EnKF ---\")\n\n    # Validation\n    if not all(col in test_df_unscaled.columns for col in enkf_state_vars):\n        print(f\"Error: Missing state variables in test data.\")\n        return None, None, None, None\n\n    dim_x = len(enkf_state_vars)\n\n    # A. Initial State (x0)\n    start_idx = seq_len - 1\n    x_init = test_df_unscaled[enkf_state_vars].iloc[start_idx].values.astype(float)\n    print(f\"Initial State Vector (t=0): {x_init}\")\n\n    # B. Initial Covariance (P0)\n    P_init = np.diag((np.abs(x_init) * 0.30) ** 2) + 1e-6 * np.eye(dim_x)\n\n    # C. Noise Matrices (Q and R)\n\n    # --- R MATRIX (Observation Noise) ---\n    if manual_r_diag is not None:\n        if len(manual_r_diag) != dim_x:\n            print(f\"Error: Manual R diag length ({len(manual_r_diag)}) does not match state vars ({dim_x})\")\n            return None, None, None, None\n        print(\"-\u003e Using MANUAL R Matrix values.\")\n        R_matrix = np.diag(manual_r_diag)\n    else:\n        print(\"-\u003e Calculating Automatic R Matrix (1% of mean).\")\n        r_diags = []\n        for col in enkf_state_vars:\n            mean_val = train_df[col].mean()\n            r_diags.append((abs(mean_val) * 0.01) ** 2)\n        R_matrix = np.diag(r_diags)\n\n    # --- Q MATRIX (Process Noise) ---\n    if manual_q_diag is not None:\n        if len(manual_q_diag) != dim_x:\n            print(f\"Error: Manual Q diag length ({len(manual_q_diag)}) does not match state vars ({dim_x})\")\n            return None, None, None, None\n        print(\"-\u003e Using MANUAL Q Matrix values.\")\n        Q_matrix = np.diag(manual_q_diag)\n    else:\n        print(\"-\u003e Calculating Automatic Q Matrix (based on volatility).\")\n        q_diags = []\n        for col in enkf_state_vars:\n            volatility = train_df[col].diff().std()\n            if pd.isna(volatility) or volatility == 0: volatility = 1e-3\n            q_diags.append((volatility * 2.0) ** 2)\n        Q_matrix = np.diag(q_diags)\n\n    print(\"Noise Matrices Finalized:\")\n    print(f\"  R (Obs Noise) diag: {np.diag(R_matrix)}\")\n    print(f\"  Q (Proc Noise) diag: {np.diag(Q_matrix)}\")\n\n    # D. Feature Indices\n    try:\n        enkf_indices = [feature_columns.index(var) for var in enkf_state_vars]\n    except ValueError as e:\n        print(f\"Error mapping variables to features: {e}\")\n        return None, None, None, None\n\n    # E. Create Instance\n    enkf = EnsembleKalmanFilter(x_init, P_init, dim_z=dim_x, N=n_enkf)\n    enkf.R = R_matrix\n    enkf.Q = Q_matrix\n\n    return enkf, enkf_indices, R_matrix, Q_matrix\n\n# --- 3. Execute ---\nif __name__ == \"__main__\":\n    try:\n        # Configuration check\n        REQUIRED = ['ENKF_STATE_VARS', 'N_ENKF', 'train_df', 'test_df',\n                    'SEQUENCE_LENGTH', 'final_feature_columns_used']\n\n        for var in REQUIRED:\n            if var not in locals(): raise NameError(f\"{var} is missing.\")\n\n\n        if (ENKF_NOISE_ESTIMATION == 'manual'):\n          MANUAL_Q = MANUAL_Q_DIAG\n          MANUAL_R = MANUAL_R_DIAG\n        else:\n          MANUAL_Q = None\n          MANUAL_R = None\n\n        enkf_instance, enkf_indices, R_matrix_enkf, Q_matrix_enkf = setup_enkf(\n            enkf_state_vars=ENKF_STATE_VARS,\n            n_enkf=N_ENKF,\n            train_df=train_df,\n            test_df_unscaled=test_df,\n            seq_len=SEQUENCE_LENGTH,\n            feature_columns=final_feature_columns_used,\n            manual_r_diag=MANUAL_R,\n            manual_q_diag=MANUAL_Q\n        )\n\n        if enkf_instance:\n            print(\"\\nEnKF Setup Complete.\")\n\n    except Exception as e:\n        print(f\"EnKF Setup Failed: {e}\")\n\nif 'Q_matrix_enkf' in locals() and 'R_matrix_enkf' in locals() and 'ENKF_STATE_VARS' in locals():\n\n    print(\"\\nGenerating publication-quality heatmaps for Q and R matrices...\")\n\n    # --- STYLE SETTINGS (Academic Standard) ---\n    plt.rcdefaults()\n    params = {\n        'font.family': 'serif',\n        'font.serif': ['Times New Roman', 'Times', 'DejaVu Serif'],\n        'axes.titlesize': 18,\n        'axes.labelsize': 14,\n        'xtick.labelsize': 12,\n        'ytick.labelsize': 12,\n        'figure.dpi': 300\n    }\n    plt.rcParams.update(params)\n\n    # Helper function to plot and save\n    def plot_covariance_matrix(matrix, title, filename, labels, cmap='Blues'):\n        plt.figure(figsize=(8, 7))\n\n        sns.heatmap(matrix, annot=True, fmt='.2e', cmap=cmap,\n                    xticklabels=labels, yticklabels=labels,\n                    square=True, cbar_kws={'label': 'Variance / Covariance'},\n                    linewidths=1, linecolor='black')\n\n        plt.title(title, pad=20)\n        plt.xticks(rotation=45, ha='right')\n        plt.yticks(rotation=0)\n        plt.tight_layout()\n\n        save_path = f\"output/{filename}\"\n        plt.savefig(save_path, bbox_inches='tight')\n        print(f\"Saved: {save_path}\")\n        plt.show()\n\n    # --- 1. Plot Q Matrix (Process Noise) ---\n    plot_covariance_matrix(\n        Q_matrix_enkf,\n        title=\"Process Noise Covariance ($Q$)\",\n        filename=\"matrix_Q_heatmap.png\",\n        labels=ENKF_STATE_VARS,\n        cmap=\"Reds\"\n    )\n\n    # --- 2. Plot R Matrix (Observation Noise) ---\n    plot_covariance_matrix(\n        R_matrix_enkf,\n        title=\"Observation Noise Covariance ($R$)\",\n        filename=\"matrix_R_heatmap.png\",\n        labels=ENKF_STATE_VARS,\n        cmap=\"Blues\"\n    )\n\nelse:\n    print(\"Error: Q and R matrices (or state vars) not found. Run Step 11 first.\")","identifier":"z4exqf8cmclw-code","enumerator":"15","html_id":"z4exqf8cmclw-code","key":"UYHfL1EQS6"},{"type":"outputs","id":"euH3eMv2Sa7xtY1VWS-us","children":[{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"\n--- Setting up EnKF ---\nInitial State Vector (t=0): [ 2.98000000e+03  1.84000000e+00  6.50000000e-01  0.00000000e+00\n  7.39418622e-01 -1.05741789e+01]\n-\u003e Using MANUAL R Matrix values.\n-\u003e Using MANUAL Q Matrix values.\nNoise Matrices Finalized:\n  R (Obs Noise) diag: [1.00e+00 4.00e-06 4.00e-08 6.25e-02 6.25e-02 1.00e-06]\n  Q (Proc Noise) diag: [9.00e+04 3.60e-01 3.60e-03 2.25e+02 2.25e+02 1.00e-02]\nEnKF Initialized. Members: 50, State Dim: 6\n\nEnKF Setup Complete.\n\nGenerating publication-quality heatmaps for Q and R matrices...\nSaved: output/matrix_Q_heatmap.png\n"},"children":[],"identifier":"z4exqf8cmclw-outputs-0","html_id":"z4exqf8cmclw-outputs-0","key":"Ka8yi5BRjo"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"\u003cFigure size 2400x2100 with 2 Axes\u003e","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"297a56677509f12f09f292ce01693c36","path":"/build/297a56677509f12f09f292ce01693c36.png"}}},"children":[],"identifier":"z4exqf8cmclw-outputs-1","html_id":"z4exqf8cmclw-outputs-1","key":"l0xPS6BFLA"},{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"Saved: output/matrix_R_heatmap.png\n"},"children":[],"identifier":"z4exqf8cmclw-outputs-2","html_id":"z4exqf8cmclw-outputs-2","key":"kkO6rp02rg"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"\u003cFigure size 2400x2100 with 2 Axes\u003e","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"f383b5ede29d94c371eb8d0ec2739f41","path":"/build/f383b5ede29d94c371eb8d0ec2739f41.png"}}},"children":[],"identifier":"z4exqf8cmclw-outputs-3","html_id":"z4exqf8cmclw-outputs-3","key":"kWxTDffd0i"}],"identifier":"z4exqf8cmclw-outputs","html_id":"z4exqf8cmclw-outputs","key":"DDqh04zo2J"}],"identifier":"z4exqf8cmclw","label":"z4EXqf8cmcLW","html_id":"z4exqf8cmclw","key":"KO4aIJHq9m"},{"type":"block","kind":"notebook-content","data":{"id":"Nb41VBFwmjOe"},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Part 12: EnKF + MC Dropout Forecasting Loop","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"aygkNdow9w"}],"identifier":"part-12-enkf-mc-dropout-forecasting-loop","label":"Part 12: EnKF + MC Dropout Forecasting Loop","html_id":"part-12-enkf-mc-dropout-forecasting-loop","implicit":true,"key":"SLk8gSUXkU"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"This section implements the core forecasting engine, which synthesizes the data assimilation capabilities of the Ensemble Kalman Filter (EnKF) with the predictive power and uncertainty estimation of the LSTM. The loop iterates through the test period, sequentially updating environmental states and generating probabilistic forecasts.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"b1Xyr8Npw4"}],"key":"KL2tcJXqsy"},{"type":"heading","depth":4,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Functionality: ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"U1kBx7n7sk"},{"type":"inlineCode","value":"run_enkf_mc_forecast","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"uh5yoVKDlj"}],"identifier":"functionality-run-enkf-mc-forecast","label":"Functionality: run_enkf_mc_forecast","html_id":"functionality-run-enkf-mc-forecast","implicit":true,"key":"WH3E5AETrV"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"The forecasting simulation is encapsulated in the ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"IP7xtNiMtQ"},{"type":"inlineCode","value":"run_enkf_mc_forecast","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"mJEapaoEI7"},{"type":"text","value":" function, which manages the interaction between real-time observations and the deep learning model.","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"kXu3ci7m53"}],"key":"LZA2fhnW8g"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":9,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"State Assimilation (EnKF):","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"BFpxUnnR9Q"}],"key":"g0YSJwRMwF"},{"type":"text","value":" For each time step, the function retrieves the current observations from the unscaled test data. If ","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"mW4WDFhWXA"},{"type":"inlineCode","value":"perform_enkf","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"MS1jAOVNvs"},{"type":"text","value":" is enabled, the EnKF instance performs a prediction and update step. This process corrects the hydrological state variables (such as river discharge and nutrient levels) by balancing the model’s internal state with actual observed data, effectively reducing the “drift” often found in long-term time-series forecasting.","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"Cdfp3NJ77S"}],"key":"BwzaKUZfpt"}],"key":"MuN971sovg"},{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Input Sequence Modification:","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"fYZL2d3LY1"}],"key":"rZJZQFjygG"},{"type":"text","value":" Once the EnKF provides an updated state, these values are passed through the fitted scaler. The function then takes the standard input sequence from ","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"EZ4cZdHz8q"},{"type":"inlineCode","value":"X_test","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"Vj3y6JCVmH"},{"type":"text","value":" and replaces the state variables in the most recent time step with these assimilated values. This ensures the LSTM makes its next prediction based on the most accurate, ground-truth-adjusted data available.","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"O4FPPGSbRg"}],"key":"G7LrRzwUsx"}],"key":"ReJ1IvWn2t"},{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"Probabilistic Prediction (MC Dropout):","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"NvN9JKgAyn"}],"key":"VqA4u6AM1X"}],"key":"mcVvD0jYxF"}],"key":"SOwvzY3XeG"}],"key":"KbSfjUtuYq"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":12,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"Uncertainty Quantification:","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"OhlO11wTyW"}],"key":"ybbSa3ssLj"},{"type":"text","value":" If ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"hbpmbnDYT5"},{"type":"inlineCode","value":"perform_mc","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"z6jRxX0wpj"},{"type":"text","value":" is active, the model performs multiple forward passes (defined by ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"rdZt1KST1F"},{"type":"inlineCode","value":"n_mc_samples","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"vc5bQKgXtQ"},{"type":"text","value":") for the same input sequence. By keeping dropout layers active during inference (","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"y1KX8liI6I"},{"type":"inlineCode","value":"training=True","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"kKRJ6JUxmO"},{"type":"text","value":"), each pass produces a slightly different prediction, allowing us to capture the model’s predictive uncertainty.","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"FyPky2Cw9c"}],"key":"EcGcolAKSl"}],"key":"Qp9TNTpGml"},{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"Standard Inference:","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"PvRgwIsu00"}],"key":"fD1kkIZwps"},{"type":"text","value":" If MC Dropout is disabled, the model performs a standard ","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"xEObWUlpag"},{"type":"inlineCode","value":"predict()","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"HZoPyM1Rm2"},{"type":"text","value":" call, providing a single deterministic output.","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"uus5fxIX3m"}],"key":"ZbcBI7kxjA"}],"key":"hdJYBhOI2t"}],"key":"HpnKQFko7h"},{"type":"heading","depth":4,"position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"text","value":"Execution and Robustness","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"m0dzRRHFw3"}],"identifier":"execution-and-robustness","label":"Execution and Robustness","html_id":"execution-and-robustness","implicit":true,"key":"rthmcsbeVT"},{"type":"paragraph","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"text","value":"The execution block verifies that all prerequisites—including the trained model, initialized EnKF, and noise matrices—are present before starting the loop.","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"T3jnOfLL8R"}],"key":"q6mOiwAc2x"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":21,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"Error Handling:","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"Pc0lN98G85"}],"key":"Hha9aYD94m"},{"type":"text","value":" The loop is wrapped in try-except blocks to manage potential issues during matrix inversions in the EnKF or scaling errors. If a critical failure occurs, the loop captures the error and returns the results generated up to that point.","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"ZicvihtVIM"}],"key":"REZuCltVe9"}],"key":"cAqTuFYE42"},{"type":"listItem","spread":true,"position":{"start":{"line":22,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"text","value":"Result Trimming:","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"NG5kXLDMU4"}],"key":"qVnpaAsgxu"},{"type":"text","value":" To maintain data integrity, both the prediction results and the corresponding ground truth labels (","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"AwSgUE5J8D"},{"type":"inlineCode","value":"y_test","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"dD2nl1OyXy"},{"type":"text","value":") are trimmed to the exact number of successfully completed iterations.","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"OnQMn4wc3p"}],"key":"aBPWVckg23"}],"key":"kA2CfSk9Ov"}],"key":"QSgkc394Lk"},{"type":"paragraph","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"text","value":"Upon completion, the results are stored in ","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"key":"LuZRSKcBXl"},{"type":"inlineCode","value":"mc_predictions_results","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"key":"d7h0XHo19A"},{"type":"text","value":". These raw probability outputs (either as single points or ensembles) serve as the foundation for the performance metrics and uncertainty analysis performed in ","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"key":"oetz4SeNGl"},{"type":"strong","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"text","value":"Part 13: EnKF + MC Dropout Evaluation","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"key":"trFKPJnDuV"}],"key":"SAdvySlbRv"},{"type":"text","value":".","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"key":"Joiq1X8TzS"}],"key":"QFpp3qLlR6"}],"identifier":"nb41vbfwmjoe","label":"Nb41VBFwmjOe","html_id":"nb41vbfwmjoe","key":"OlJLCOvayZ"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FrayQMEpmkUG","outputId":"9ee71178-1ffc-4194-c866-8d12bcfef2d8","ExecuteTime":{"end_time":"2026-01-07T03:44:43.379234900Z","start_time":"2026-01-07T03:37:39.208476Z"}},"children":[{"type":"code","lang":"python","executable":true,"value":"def run_enkf_mc_forecast(model, enkf_instance, enkf_indices, R_matrix, Q_matrix,\n                         x_test, y_test, test_df_unscaled, scaler,\n                         seq_len, perform_enkf=True, perform_mc=True, n_mc_samples=50):\n    \"\"\"\n    Optimized Forecasting Loop with Vectorized MC Dropout.\n    \"\"\"\n    # 1. Validation \u0026 Setup\n    if model is None or x_test is None: return None, None\n    print(f\"\\n--- Starting Forecast (EnKF={perform_enkf}, MC={perform_mc}) ---\")\n\n    num_steps = len(y_test)\n    n_features = x_test.shape[2]\n\n    # Pre-allocate output array\n    # If MC is off, we still use shape (N, 1) for consistency\n    samples_col = n_mc_samples if perform_mc else 1\n    predictions = np.zeros((num_steps, samples_col))\n\n    # 2. Pre-fetch Scaler Parameters (Speed Optimization)\n    # Accessing scaler attributes inside the loop is slow. Do it once here.\n    if perform_enkf:\n        try:\n            # Handle StandardScaler vs RobustScaler\n            if hasattr(scaler, 'mean_'):\n                mu, sigma = scaler.mean_, scaler.scale_\n            elif hasattr(scaler, 'center_'):\n                mu, sigma = scaler.center_, scaler.scale_\n            else:\n                print(\"Error: Scaler not fitted.\"); return None, None\n\n            # Keep only the stats for the EnKF variables to avoid indexing in loop\n            enkf_mu = mu[enkf_indices]\n            enkf_sigma = sigma[enkf_indices]\n        except Exception as e:\n            print(f\"Scaler Error: {e}\"); return None, None\n\n    # 3. Main Loop\n    # EnKF must be sequential (step t depends on step t-1)\n    for i in tqdm(range(num_steps), desc=\"Forecasting\"):\n\n        # A. Current Timestamp Index\n        # The test_df index corresponding to the *end* of the current sequence\n        df_idx = i + seq_len - 1\n        if df_idx \u003e= len(test_df_unscaled): break\n\n        # B. EnKF Update Step\n        updated_state = None\n        if perform_enkf:\n            # Get Observation (z)\n            z = test_df_unscaled[ENKF_STATE_VARS].iloc[df_idx].values\n\n            # 1. Predict (Move Ensemble Forward)\n            # Note: In a full Hybrid EnKF, the 'F' function would run the LSTM here.\n            # Here we use the persistence assumption for the state transition.\n            enkf_instance.predict(lambda x: x, Q=Q_matrix)\n\n            # 2. Update (Correct with Observation)\n            updated_state, _ = enkf_instance.update(z, R_matrix)\n\n        # C. Update LSTM Input\n        # We copy the sequence so we don't overwrite the original data\n        current_seq = x_test[i].copy() # Shape: (Seq_Len, Features)\n\n        if perform_enkf and updated_state is not None:\n            # Scale the updated state (Vectorized Math)\n            # (Raw - Mean) / Scale\n            scaled_state = (updated_state - enkf_mu) / enkf_sigma\n\n            # Inject into the LAST time step of the sequence\n            # We replace only the columns corresponding to EnKF variables\n            current_seq[-1, enkf_indices] = scaled_state\n\n        # D. LSTM Prediction (Vectorized MC Dropout)\n        # Prepare Tensor\n        # Shape: (1, Seq_Len, Features)\n        input_tensor = tf.convert_to_tensor([current_seq], dtype=tf.float32)\n\n        if perform_mc:\n            # OPTIMIZATION: Tile the input N times to create a batch\n            # New Shape: (n_mc_samples, Seq_Len, Features)\n            batch = tf.tile(input_tensor, [n_mc_samples, 1, 1])\n\n            # ONE single call to the model for all samples\n            # training=True enables Dropout\n            preds = model(batch, training=True)\n\n            # Store results (flatten to 1D array of probabilities)\n            predictions[i, :] = preds[:, 0].numpy()\n        else:\n            # Standard Inference (No Dropout)\n            pred = model(input_tensor, training=False)\n            predictions[i, 0] = pred[0, 0].numpy()\n\n    return predictions, y_test[:len(predictions)]\n\n# --- Execute ---\nif __name__ == \"__main__\":\n    if 'trained_model' in locals() and 'enkf_instance' in locals():\n        if PERFORM_ENKF and enkf_instance is None:\n            print(\"Warning: PERFORM_ENKF is True, but 'enkf_instance' not found.\")\n            print(\"Running in pure LSTM mode (EnKF disabled for this run).\")\n            # Temporarily disable for this function call to prevent crash\n            current_perform_enkf = False\n        else:\n            current_perform_enkf = PERFORM_ENKF\n\n        mc_preds, y_test_trimmed = run_enkf_mc_forecast(\n            model=trained_model,\n            enkf_instance=enkf_instance,\n            enkf_indices=enkf_indices,\n            R_matrix=R_matrix_enkf,\n            Q_matrix=Q_matrix_enkf,\n            x_test=X_test,\n            y_test=y_test,\n            test_df_unscaled=test_df,\n            scaler=scaler, # Ensure this matches your scaler variable name\n            seq_len=SEQUENCE_LENGTH,\n            perform_enkf=True,         # Set to False to test Baseline again\n            perform_mc=True,           # Set to True for Uncertainty\n            n_mc_samples=50            # 50 is a good balance for speed/accuracy\n        )\n\n        if mc_preds is not None:\n            print(f\"✅ Forecast Complete. Prediction Shape: {mc_preds.shape}\")\n    else:\n        print(\"Error: Prerequisites (Part 9 Model or Part 11 EnKF) missing.\")","identifier":"frayqmepmkug-code","enumerator":"16","html_id":"frayqmepmkug-code","key":"gV9C2kkma5"},{"type":"outputs","id":"0pos0XVd5svvsvO0XqzR3","children":[{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"\n--- Starting Forecast (EnKF=True, MC=True) ---\n"},"children":[],"identifier":"frayqmepmkug-outputs-0","html_id":"frayqmepmkug-outputs-0","key":"Q2XgR2RPWb"},{"type":"output","jupyter_data":{"output_type":"stream","name":"stderr","text":"Forecasting: 100%|██████████| 230/230 [00:05\u003c00:00, 42.55it/s]"},"children":[],"identifier":"frayqmepmkug-outputs-1","html_id":"frayqmepmkug-outputs-1","key":"brEpWvhjmp"},{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"✅ Forecast Complete. Prediction Shape: (230, 50)\n"},"children":[],"identifier":"frayqmepmkug-outputs-2","html_id":"frayqmepmkug-outputs-2","key":"kPmcgAuSst"},{"type":"output","jupyter_data":{"output_type":"stream","name":"stderr","text":"\n"},"children":[],"identifier":"frayqmepmkug-outputs-3","html_id":"frayqmepmkug-outputs-3","key":"jSDVF0RWiy"}],"identifier":"frayqmepmkug-outputs","html_id":"frayqmepmkug-outputs","key":"eAPPS8cmLn"}],"identifier":"frayqmepmkug","label":"FrayQMEpmkUG","html_id":"frayqmepmkug","key":"UJTHo2cVtF"},{"type":"block","kind":"notebook-content","data":{"id":"xD0ko6o6mwsw"},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Part 13: EnKF + MC Dropout Evaluation","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JTCfJfc3wE"}],"identifier":"part-13-enkf-mc-dropout-evaluation","label":"Part 13: EnKF + MC Dropout Evaluation","html_id":"part-13-enkf-mc-dropout-evaluation","implicit":true,"key":"XAkuEFpxyB"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"This final analytical section focuses on quantifying the performance of the integrated forecasting system. By evaluating the predictions generated in Part 12, we assess how the combination of Ensemble Kalman Filter (EnKF) state updates and Monte Carlo (MC) Dropout uncertainty estimation improves the model’s ability to predict red tide events.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Wj6jJDlkXE"}],"key":"VCnKWoPAQW"},{"type":"heading","depth":4,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Core Evaluation Components","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"JlHcRb4rC2"}],"identifier":"core-evaluation-components","label":"Core Evaluation Components","html_id":"core-evaluation-components","implicit":true,"key":"dgHGOgLkev"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"The evaluation framework is designed to move beyond simple point-accuracy and instead provide a comprehensive view of model reliability and confidence.","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"Anr6bRDVPK"}],"key":"tv0r0wuQpa"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":9,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"Ensemble Statistics and Uncertainty:","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"fHr55Mr6FF"}],"key":"RWPE3wTaOu"},{"type":"text","value":"\nFor sessions where ","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"IqlQrpdaCD"},{"type":"inlineCode","value":"PERFORM_MC_DROPOUT","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"M7NoN701HB"},{"type":"text","value":" was enabled, the evaluation calculates the mean and standard deviation across all MC samples for each time step. The mean serves as the primary probabilistic forecast, while the standard deviation provides a quantifiable measure of model uncertainty. This allows us to identify periods where the model is highly confident versus periods where environmental volatility leads to wider prediction intervals.","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"rhF8IwsZtr"}],"key":"ajl1hknY5Y"}],"key":"po472MYK75"},{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"Binary Classification Performance:","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"UNadIOff1G"}],"key":"jKyxM6fCRg"},{"type":"text","value":"\nSince the ultimate goal is to predict bloom occurrences, the probabilistic outputs are converted into binary classifications using an optimized threshold. Key performance metrics include:","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"VBCq50OIdq"}],"key":"EDHrVeFrfB"}],"key":"vdjBYJ21zS"}],"key":"H411QD1iYg"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":13,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"Precision and Recall:","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"ABd9FhOAN4"}],"key":"hLoZOCXdcv"},{"type":"text","value":" Determining the model’s ability to correctly identify blooms while minimizing false alarms.","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"OW17IzoGIo"}],"key":"TmyySVqOve"}],"key":"xakpHsN98Y"},{"type":"listItem","spread":true,"position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"F1-Score:","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"cqiFmpkzQU"}],"key":"wEj2F1oGkX"},{"type":"text","value":" Providing a balanced metric that accounts for the inherent class imbalance in red tide data.","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"pIL9X41XCP"}],"key":"JiOemOyI5h"}],"key":"ztsbpdSyXB"},{"type":"listItem","spread":true,"position":{"start":{"line":15,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"text","value":"Confusion Matrix:","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"ezp1yk0Zsc"}],"key":"BE91rC9975"},{"type":"text","value":" Visualizing the distribution of True Positives, True Negatives, False Positives, and False Negatives.","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"QzpFLMIE4x"}],"key":"pIPCAMs3GJ"}],"key":"nkhDm7rQPE"}],"key":"Efxbff6ufH"},{"type":"list","ordered":true,"start":3,"spread":false,"position":{"start":{"line":18,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":18,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"text","value":"Probabilistic Analysis:","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"wsMXafIyfH"}],"key":"ODBhnxiL5p"},{"type":"text","value":"\nThe evaluation includes the calculation of the ","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"TJJropvuv6"},{"type":"strong","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"text","value":"Brier Score","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"T5fZZNcqrf"}],"key":"xsjZ731DOk"},{"type":"text","value":" to measure the accuracy of the probability forecasts and the ","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"WcS8GHDTLC"},{"type":"strong","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"text","value":"Area Under the Precision-Recall Curve (AUPRC)","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"tlpbOD9g4L"}],"key":"tK3OMstN2f"},{"type":"text","value":". These metrics are particularly useful for evaluating models where the event of interest (the bloom) is relatively rare compared to non-bloom periods.","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"yG2BuxqJ13"}],"key":"L8XTgD1Tob"}],"key":"rU5w8ZMyIh"}],"key":"f5abGjk3CM"},{"type":"heading","depth":4,"position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"Visualizing the Results","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"wwYvW998if"}],"identifier":"visualizing-the-results","label":"Visualizing the Results","html_id":"visualizing-the-results","implicit":true,"key":"Am19vkaC72"},{"type":"paragraph","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"The section generates a series of diagnostic plots to illustrate the system’s effectiveness:","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"YdrebJYN50"}],"key":"Wpw5mh2hlX"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":25,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"text","value":"Forecast vs. Actuals:","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"mDKjdlILZ0"}],"key":"AKPLzTgpPf"},{"type":"text","value":" A time-series plot comparing the predicted probabilities (with uncertainty bands) against the actual observed bloom markers.","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"x4yrL9Ta7e"}],"key":"aDeX8rll1n"}],"key":"zk0FfCcdQH"},{"type":"listItem","spread":true,"position":{"start":{"line":26,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"children":[{"type":"text","value":"EnKF Impact Analysis:","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"key":"IiU3HZWip3"}],"key":"VNv6Z72ihT"},{"type":"text","value":" Comparative visualizations showing how the state assimilation influenced the trajectory of the predictions compared to a baseline model without EnKF updates.","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"key":"SeuOCfd5PD"}],"key":"sZrWwXO2lV"}],"key":"eMilH4hrsK"}],"key":"c0bIlMhDFd"},{"type":"paragraph","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"children":[{"type":"text","value":"By the end of this evaluation, we can determine the specific “value-add” of the EnKF + MC Dropout approach, providing a clear picture of how well the hybrid system handles the complexities of ecological forecasting.","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"key":"RL2j8fXWCk"}],"key":"PVs2K6qahQ"}],"identifier":"xd0ko6o6mwsw","label":"xD0ko6o6mwsw","html_id":"xd0ko6o6mwsw","key":"up3d8kamIV"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"BqhJYg-Mmy-5","outputId":"de544cc7-afaf-4f7e-cf08-f9443341df80","ExecuteTime":{"end_time":"2026-01-07T03:44:43.380234200Z","start_time":"2026-01-07T03:37:50.405109Z"}},"children":[{"type":"code","lang":"python","executable":true,"value":"from sklearn.calibration import calibration_curve\ndef evaluate_mc_predictions(mc_preds, y_test, test_df_unscaled, seq_len,\n                            forecast_horizon, model_name=\"EnKF_MC\",\n                            output_dir=\"output\", manual_threshold=None):\n    \"\"\"\n    Evaluates probabilistic predictions (Mean, Uncertainty, Calibration).\n\n    Args:\n        mc_preds: Array of shape (n_samples, n_mc_iterations) or (n_samples, 1)\n        y_test: True labels (n_samples,)\n        test_df_unscaled: DataFrame with timestamps\n        seq_len: Input sequence length (for aligning dates)\n        forecast_horizon: Forecast horizon (for aligning dates)\n        model_name: String for labeling files/plots\n        output_dir: Folder to save results\n        manual_threshold: Float (0.0-1.0) to force a specific decision boundary.\n                          If None, calculates optimal F1 threshold.\n    \"\"\"\n    print(f\"\\n=== Evaluating Uncertainty: {model_name} ===\")\n\n    # 1. Setup Output Directory\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Validation\n    if mc_preds is None or y_test is None: return None\n\n    # 2. Statistics Calculation\n    # -------------------------------------------------------\n    # Mean: The \"Best Guess\"\n    y_mean = np.mean(mc_preds, axis=1)\n\n    # Std Dev: The \"Disagreement\" (Model Uncertainty)\n    y_std = np.std(mc_preds, axis=1)\n\n    # 95% Confidence Interval\n    ci_lower = np.percentile(mc_preds, 2.5, axis=1)\n    ci_upper = np.percentile(mc_preds, 97.5, axis=1)\n\n    # Entropy (Uncertainty)\n    epsilon = 1e-10\n    y_mean_clipped = np.clip(y_mean, epsilon, 1-epsilon)\n    entropy = -(y_mean_clipped * np.log(y_mean_clipped) + (1-y_mean_clipped) * np.log(1-y_mean_clipped))\n\n    # 3. Threshold Logic\n    # -------------------------------------------------------\n    if manual_threshold is not None:\n        best_thresh = manual_threshold\n        print(f\"Using Manual Decision Threshold: {best_thresh:.4f}\")\n    else:\n        # Calculate Optimal F1 Threshold\n        precision, recall, thresholds = precision_recall_curve(y_test, y_mean)\n        numerator = 2 * precision * recall\n        denominator = precision + recall\n        with np.errstate(divide='ignore', invalid='ignore'):\n            fscore = np.nan_to_num(np.divide(numerator, denominator))\n\n        ix = np.argmax(fscore)\n        # Safety check if index is out of bounds\n        best_thresh = thresholds[ix] if ix \u003c len(thresholds) else 0.5\n        print(f\"Optimal Threshold (Auto-F1): {best_thresh:.4f} (Max F1: {fscore[ix]:.4f})\")\n\n    # Apply threshold\n    y_pred_class = (y_mean \u003e= best_thresh).astype(int)\n\n    # 4. Calculate Metrics\n    # -------------------------------------------------------\n    try:\n        metrics = {\n            'accuracy': float(accuracy_score(y_test, y_pred_class)),\n            'auc': float(roc_auc_score(y_test, y_mean)),\n            'brier': float(brier_score_loss(y_test, y_mean)),\n            'precision': float(precision_score(y_test, y_pred_class, zero_division=0)),\n            'recall': float(recall_score(y_test, y_pred_class, zero_division=0)),\n            'f1_score': float(f1_score(y_test, y_pred_class, zero_division=0)),\n            'avg_entropy': float(np.mean(entropy)),\n            'threshold_used': float(best_thresh)\n        }\n\n        print(f\"\\nPerformance Metrics:\")\n        print(f\"  Accuracy:    {metrics['accuracy']:.4f}\")\n        print(f\"  AUC-ROC:     {metrics['auc']:.4f}\")\n        print(f\"  Recall:      {metrics['recall']:.4f} (Caught {metrics['recall']*100:.1f}% of blooms)\")\n        print(f\"  Precision:   {metrics['precision']:.4f}\")\n        print(f\"  Brier Score: {metrics['brier']:.4f}\")\n    except Exception as e:\n        print(f\"Error calculating metrics: {e}\")\n        metrics = {}\n\n    # 5. PLOTTING (Publication Quality)\n    # -------------------------------------------------------\n    # Apply Style\n    plt.rcdefaults()\n    params = {'font.family': 'serif', 'figure.figsize': (10, 6), 'figure.dpi': 300}\n    plt.rcParams.update(params)\n\n    # Plot A: Forecast Time Series\n    start_idx = seq_len + forecast_horizon - 1\n    num_preds = len(y_mean)\n\n    if start_idx + num_preds \u003c= len(test_df_unscaled):\n        dates = test_df_unscaled.index[start_idx : start_idx + num_preds]\n\n        plt.figure()\n        # Plot Actuals\n        plt.plot(dates, y_test[:num_preds], color='black', alpha=0.6, label='Actual Bloom', linewidth=1.5)\n        # Plot Mean Prediction\n        plt.plot(dates, y_mean[:num_preds], color='#d62728', label='Ensemble Mean', linewidth=2)\n        # Plot Confidence Interval\n        plt.fill_between(dates, ci_lower[:num_preds], ci_upper[:num_preds], color='#d62728', alpha=0.2, label='95% CI')\n        # Plot Threshold\n        plt.axhline(best_thresh, color='gray', linestyle='--', label=f'Threshold ({best_thresh:.2f})')\n\n        plt.title(f'Probabilistic Forecast ({model_name})')\n        plt.ylabel('Bloom Probability')\n        plt.legend(loc='upper right', frameon=True)\n        plt.grid(True, linestyle=':', alpha=0.5)\n        plt.tight_layout()\n\n        save_path = os.path.join(output_dir, f\"forecast_{model_name}.png\")\n        plt.savefig(save_path)\n        print(f\"Saved Plot: {save_path}\")\n        plt.show()\n\n    # Plot B: Calibration Curve\n    try:\n        prob_true, prob_pred = calibration_curve(y_test, y_mean, n_bins=10)\n        plt.figure(figsize=(6, 6))\n        plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly Calibrated')\n        plt.plot(prob_pred, prob_true, marker='o', color='blue', label=model_name)\n        plt.title('Calibration Curve (Reliability)')\n        plt.xlabel('Mean Predicted Probability')\n        plt.ylabel('Fraction of Positives (Actual)')\n        plt.legend()\n        plt.grid(True, linestyle=':', alpha=0.5)\n        plt.tight_layout()\n\n        save_path = os.path.join(output_dir, f\"calibration_{model_name}.png\")\n        plt.savefig(save_path)\n        print(f\"Saved Plot: {save_path}\")\n        plt.show()\n    except Exception as e:\n        print(f\"Skipping Calibration Plot: {e}\")\n\n    # Plot C: Entropy Distribution\n    try:\n        correct = (y_pred_class == y_test)\n        plt.figure(figsize=(8, 5))\n        sns.kdeplot(entropy[correct], fill=True, color='green', label='Correct Predictions')\n        sns.kdeplot(entropy[~correct], fill=True, color='red', label='Wrong Predictions')\n        plt.title('Predictive Entropy (Uncertainty Distribution)')\n        plt.xlabel('Entropy (Higher = More Uncertain)')\n        plt.legend()\n        plt.tight_layout()\n\n        save_path = os.path.join(output_dir, f\"entropy_{model_name}.png\")\n        plt.savefig(save_path)\n        print(f\"Saved Plot: {save_path}\")\n        plt.show()\n    except Exception as e:\n        print(f\"Skipping Entropy Plot: {e}\")\n\n    # 6. Return structured data\n    return {\n        'metrics': metrics,  # Clean dictionary for JSON\n        'arrays': {          # Arrays for CSV/Debug\n            'y_mean': y_mean,\n            'y_std': y_std,\n            'entropy': entropy,\n            'y_actual': y_test,\n            'ci_lower': ci_lower,\n            'ci_upper': ci_upper\n        }\n    }\n\n# --- Execute ---\nif __name__ == \"__main__\":\n    if 'mc_preds' in locals() and mc_preds is not None:\n\n        # 1. Run Evaluation (Try manual_threshold=0.3 if you want to fix lag)\n        results = evaluate_mc_predictions(\n            mc_preds=mc_preds,\n            y_test=y_test_trimmed,\n            test_df_unscaled=test_df,\n            seq_len=SEQUENCE_LENGTH,\n            forecast_horizon=FORECAST_HORIZON,\n            model_name=\"EnKF_LSTM_MC\",\n            output_dir=\"output\",\n            manual_threshold=None  # \u003c--- Change to 0.3 to reduce lag!\n        )\n\n        # 2. Save Metrics to JSON\n        if results:\n            json_path = os.path.join(\"output\", \"metrics_enkf_mc.json\")\n            try:\n                with open(json_path, 'w') as f:\n                    json.dump(results['metrics'], f, indent=4)\n                print(f\"✅ Saved Metrics to: {json_path}\")\n            except Exception as e:\n                print(f\"Error saving JSON: {e}\")\n\n            # 3. Save Raw Data to CSV\n            csv_path = os.path.join(\"output\", \"predictions_enkf_mc.csv\")\n            try:\n                df_results = pd.DataFrame(results['arrays'])\n                # Add timestamp index if possible\n                start_idx = SEQUENCE_LENGTH + FORECAST_HORIZON - 1\n                if start_idx + len(df_results) \u003c= len(test_df):\n                    df_results.index = test_df.index[start_idx : start_idx + len(df_results)]\n\n                df_results.to_csv(csv_path)\n                print(f\"✅ Saved Raw Data to: {csv_path}\")\n            except Exception as e:\n                print(f\"Error saving CSV: {e}\")\n\n    else:\n        print(\"Error: 'mc_preds' not found. Run Step 12 first.\")","identifier":"bqhjyg-mmy-5-code","enumerator":"17","html_id":"bqhjyg-mmy-5-code","key":"IzTFHGEjQQ"},{"type":"outputs","id":"fofbs27-3eF5OvNHeJmzz","children":[{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"\n=== Evaluating Uncertainty: EnKF_LSTM_MC ===\nOptimal Threshold (Auto-F1): 0.8037 (Max F1: 0.8649)\n\nPerformance Metrics:\n  Accuracy:    0.9130\n  AUC-ROC:     0.9529\n  Recall:      0.8421 (Caught 84.2% of blooms)\n  Precision:   0.8889\n  Brier Score: 0.1236\nSaved Plot: output/forecast_EnKF_LSTM_MC.png\n"},"children":[],"identifier":"bqhjyg-mmy-5-outputs-0","html_id":"bqhjyg-mmy-5-outputs-0","key":"zXiqF6k5Q1"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"\u003cFigure size 3000x1800 with 1 Axes\u003e","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"ea44aa14eb6f9f5bc8e9257ee43bc189","path":"/build/ea44aa14eb6f9f5bc8e9257ee43bc189.png"}}},"children":[],"identifier":"bqhjyg-mmy-5-outputs-1","html_id":"bqhjyg-mmy-5-outputs-1","key":"l6EWm3qYU5"},{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"Saved Plot: output/calibration_EnKF_LSTM_MC.png\n"},"children":[],"identifier":"bqhjyg-mmy-5-outputs-2","html_id":"bqhjyg-mmy-5-outputs-2","key":"W8DsVAgtok"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"\u003cFigure size 1800x1800 with 1 Axes\u003e","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"b775b9326344335f9f26b7411f2a23aa","path":"/build/b775b9326344335f9f26b7411f2a23aa.png"}}},"children":[],"identifier":"bqhjyg-mmy-5-outputs-3","html_id":"bqhjyg-mmy-5-outputs-3","key":"Wqkqx1ys2U"},{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"Saved Plot: output/entropy_EnKF_LSTM_MC.png\n"},"children":[],"identifier":"bqhjyg-mmy-5-outputs-4","html_id":"bqhjyg-mmy-5-outputs-4","key":"mrzPa1QVLi"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"\u003cFigure size 2400x1500 with 1 Axes\u003e","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"d016f356bf7188afa2fb7e4c31ac2c0c","path":"/build/d016f356bf7188afa2fb7e4c31ac2c0c.png"}}},"children":[],"identifier":"bqhjyg-mmy-5-outputs-5","html_id":"bqhjyg-mmy-5-outputs-5","key":"d8W94CXjxL"},{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"✅ Saved Metrics to: output/metrics_enkf_mc.json\n✅ Saved Raw Data to: output/predictions_enkf_mc.csv\n"},"children":[],"identifier":"bqhjyg-mmy-5-outputs-6","html_id":"bqhjyg-mmy-5-outputs-6","key":"KQmqpVZGP3"}],"identifier":"bqhjyg-mmy-5-outputs","html_id":"bqhjyg-mmy-5-outputs","key":"rzmozr3rzH"}],"identifier":"bqhjyg-mmy-5","label":"BqhJYg-Mmy-5","html_id":"bqhjyg-mmy-5","key":"yxNA7LO8Ca"},{"type":"block","kind":"notebook-content","data":{"id":"zIx8Y878hU4O"},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Part 14: SHAP Analysis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PytUYY5One"}],"identifier":"part-14-shap-analysis","label":"Part 14: SHAP Analysis","html_id":"part-14-shap-analysis","implicit":true,"key":"srTVZIKnSt"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"This section utilizes ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"EuunNqhL7y"},{"type":"strong","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"SHAP (SHapley Additive exPlanations)","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"tZJZYy5zHF"}],"key":"ctrpq1LBOu"},{"type":"text","value":" to interpret the LSTM model’s decision-making process. While deep learning models are often viewed as “black boxes,” SHAP values provide transparency by quantifying the contribution of each environmental feature to the final prediction.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"pC9tOzFy81"}],"key":"RgIDrXKzhD"},{"type":"heading","depth":4,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Core Interpretability Components","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"dNQbx3TqX9"}],"identifier":"core-interpretability-components","label":"Core Interpretability Components","html_id":"core-interpretability-components","implicit":true,"key":"mo27Vq4M6u"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"The analysis focuses on local and global feature importance to understand which drivers—such as nutrient concentrations, river discharge, or lagged bloom states—are most influential in triggering a red tide forecast.","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"DHMGfQterV"}],"key":"hoZrKVZtbK"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":9,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"DeepExplainer for LSTM:","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"RV1uxCoHBd"}],"key":"U6n2dojvVJ"},{"type":"text","value":"\nWe employ ","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"D7lBCi2Cop"},{"type":"inlineCode","value":"shap.DeepExplainer","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"oxsFnU8ZIG"},{"type":"text","value":" to handle the complex, 3-dimensional temporal data characteristic of LSTMs. This approach approximates SHAP values by comparing the model’s output for specific test sequences against a “background” dataset (typically a representative subset of the training data). This reveals how specific fluctuations in environmental conditions shift the probability of a bloom.","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"CR02Bm7n5L"}],"key":"uJcMquFWwM"}],"key":"ZOwuQY3lVG"},{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"Temporal Feature Contribution:","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"IpFmWsrmTj"}],"key":"b9YmC0odn8"},{"type":"text","value":"\nSince the LSTM processes sequences over time, SHAP analysis allows us to visualize which time steps within the input window are most critical. We can identify whether the model is reacting to immediate spikes in river discharge or if it is recognizing long-term patterns in nutrient accumulation across the entire sequence length.","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"Un54IzZgjI"}],"key":"Ep60r6fUf7"}],"key":"vlIpnI60dO"},{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"Global Feature Importance:","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"BIPzZAnL9m"}],"key":"EJ1BVIe8Ku"},{"type":"text","value":"\nBy aggregating the absolute SHAP values across the entire test set, we generate a global ranking of features. This is vital for ecological validation, as it allows us to verify if the model’s “reasoning” aligns with known biological drivers of ","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"tVcvFkALX2"},{"type":"emphasis","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"Karenia brevis","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"OC7wpvzBxZ"}],"key":"mQskqJivzn"},{"type":"text","value":", such as specific salinity levels or nitrogen-to-phosphorus ratios.","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"eBz615m7cH"}],"key":"epRG3Xg5qg"}],"key":"PgnnKzVKDy"}],"key":"kPkOEtfBWn"},{"type":"heading","depth":4,"position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"Visualization and Insights","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"FV6cDO7tuE"}],"identifier":"visualization-and-insights","label":"Visualization and Insights","html_id":"visualization-and-insights","implicit":true,"key":"xAHUpZmb5q"},{"type":"paragraph","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"text","value":"The section produces high-impact visualizations to communicate these findings:","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"K9c4NMe3hD"}],"key":"cktOG6HrdA"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":20,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"text","value":"SHAP Summary Plots:","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"FqOnHCaM51"}],"key":"HCsAvsDF96"},{"type":"text","value":" A comprehensive view showing the magnitude and direction of each feature’s impact. For instance, it might reveal that high values of a specific lagged nutrient feature consistently push the model toward a “bloom” prediction.","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"iewnDDu845"}],"key":"vGTB5MjgMG"}],"key":"QHHPT2LI6v"},{"type":"listItem","spread":true,"position":{"start":{"line":21,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"Force Plots / Waterfall Plots:","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"QWQPBILfPF"}],"key":"TygfTXJKDd"},{"type":"text","value":" Detailed breakdowns of individual prediction samples, showing how different features “pushed” the model’s output from the base value to the final predicted probability.","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"PDekMGltfF"}],"key":"oQT9V9IYGd"}],"key":"m2bm4wtMXS"}],"key":"OWOx4jGCE9"},{"type":"paragraph","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"By integrating SHAP analysis, we transition from simply knowing ","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"vVgIgHR6bN"},{"type":"emphasis","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"what","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"LOctxbDDqU"}],"key":"dtlE31wsli"},{"type":"text","value":" the model predicted to understanding ","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"qxEbbMGV2G"},{"type":"emphasis","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"why","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"t1mv10cxE9"}],"key":"XeNCpxpD2m"},{"type":"text","value":" it made that prediction, providing essential context for environmental managers and researchers.","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"fmdojyWnF9"}],"key":"K2eT7XMgTl"}],"identifier":"zix8y878hu4o","label":"zIx8Y878hU4O","html_id":"zix8y878hu4o","key":"QTmvG2YynO"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["0eef66dc7b3b45f581fc334c6d33db34","03534a03d35d49f983c66f8bc1572b9c","eff0a5292d484968ba7e7bf3cef090c3","eecb5d87e890498487275b62e90adcd4","8b1224ba29d34b84b1981718c4b70ab0","c9dff54d4d6f4fde817049a8bc267cd1","f7a5380eec4d426ab054cfbfe967b776","b27cac4bb2d64bd090640e6a725c0057","486fa3ac2e2e4b2681e63ff0ba064f8e","ac29d24173054a929fc194828a9f1e60","e5f7e2ea050041ed9cc2891b4d3c3eea"]},"id":"94FzvL8sJeXN","outputId":"a28ceafe-8701-4021-8127-60c0c4016ead","ExecuteTime":{"end_time":"2026-01-07T03:44:43.385025500Z","start_time":"2026-01-07T03:37:51.737884Z"}},"children":[{"type":"code","lang":"python","executable":true,"value":"try:\n    # 1. Check \u0026 Fix Data Dimensions\n    # ---------------------------------------------------------\n    if 'X_test' not in locals() or 'trained_model' not in locals():\n        raise NameError(\"Data or Model missing. Please restart kernel and run Steps 1-9.\")\n\n    true_n_steps = X_test.shape[1]    # e.g., 12\n    true_n_features = X_test.shape[2] # e.g., 26\n\n    print(f\"Data Shape: {true_n_steps} steps, {true_n_features} features\")\n\n    # Fix the feature name list if it doesn't match the data\n    current_names = final_feature_columns_used if 'final_feature_columns_used' in locals() else []\n\n    if len(current_names) != true_n_features:\n        print(f\"Mismatch: {len(current_names)} names vs {true_n_features} data features.\")\n        diff = true_n_features - len(current_names)\n        # We assume the extra columns are usually appended at the end\n        plotting_feature_names = current_names + [f\"Extra_Feat_{i+1}\" for i in range(diff)]\n    else:\n        plotting_feature_names = current_names\n\n    # 2. Define Wrapper (The Fix)\n    # ---------------------------------------------------------\n    def model_wrapper(flat_data):\n        \"\"\"\n        Converts flat SHAP input back to 3D and queries the model\n        using Direct Call (not .predict) to avoid TF Graph errors.\n        \"\"\"\n        # Convert Numpy -\u003e Tensor\n        # Reshape to (Samples, Time, Features)\n        reshaped = flat_data.reshape(-1, true_n_steps, true_n_features)\n        tensor_input = tf.convert_to_tensor(reshaped, dtype=tf.float32)\n\n        # Direct call (Eager Mode) - Faster and safer for SHAP\n        probs = trained_model(tensor_input, training=False)\n\n        # Return as Numpy flattened array\n        return probs.numpy().flatten()\n\n    # 3. Optimize Background (K-Means)\n    # ---------------------------------------------------------\n    print(\"Summarizing background data...\")\n    # Flatten train data: (Samples, Time*Feats)\n    X_train_flat = X_train.reshape(X_train.shape[0], -1)\n\n    # Summarize training data into 10 weighted points (Centroids)\n    background_summary = shap.kmeans(X_train_flat, 10)\n\n    # 4. Initialize KernelExplainer\n    # ---------------------------------------------------------\n    print(\"Initializing KernelExplainer...\")\n    explainer = shap.KernelExplainer(model_wrapper, background_summary)\n\n    # 5. Calculate SHAP Values\n    # ---------------------------------------------------------\n    explain_size = 20  # Keep small for speed\n    X_test_flat = X_test.reshape(X_test.shape[0], -1)\n\n    # Pick random samples to explain\n    idxs = np.random.choice(X_test_flat.shape[0], explain_size, replace=False)\n    X_test_sample = X_test_flat[idxs]\n\n    print(f\"Calculating SHAP values for {explain_size} instances...\")\n    # nsamples=auto usually works best, or try 500 if slow\n    shap_values = explainer.shap_values(X_test_sample, nsamples=500)\n\n    print(f\"SHAP calculation complete.\")\n\n    # 6. Reshape for Visualization\n    # ---------------------------------------------------------\n    # (Samples, Flat_Features) -\u003e (Samples, Time, Features)\n    shap_values_3d = np.array(shap_values).reshape(-1, true_n_steps, true_n_features)\n    X_test_sample_3d = X_test_sample.reshape(-1, true_n_steps, true_n_features)\n\n    # 7. PLOT A: Global Feature Importance\n    # ---------------------------------------------------------\n    # Sum SHAP impact across all time steps\n    shap_sum_over_time = np.sum(shap_values_3d, axis=1) # (Samples, Features)\n    features_mean = np.mean(X_test_sample_3d, axis=1)   # (Samples, Features)\n\n    plt.figure(figsize=(10, 8), dpi=300)\n    shap.summary_plot(shap_sum_over_time, features_mean,\n                      feature_names=plotting_feature_names, show=False)\n    plt.title(\"Global Feature Importance\")\n    plt.tight_layout()\n    plt.savefig(\"output/shap_summary_global.png\")\n    plt.show()\n\n    # 8. PLOT B: Temporal Heatmap\n    # ---------------------------------------------------------\n    # Average absolute impact per time step\n    temporal_importance = np.mean(np.abs(shap_values_3d), axis=0) # (Time, Feats)\n\n    plt.figure(figsize=(12, 6), dpi=300)\n    sns.heatmap(temporal_importance.T, cmap='viridis',\n                yticklabels=plotting_feature_names,\n                xticklabels=range(true_n_steps))\n\n    plt.title(\"Temporal Feature Importance\\n(Brighter = Higher Impact at that Lag)\")\n    plt.xlabel(\"Time Lag (Steps into Sequence)\")\n    plt.ylabel(\"Feature\")\n    plt.tight_layout()\n    plt.savefig(\"output/shap_temporal_heatmap.png\")\n    plt.show()\n\n    print(\"✅ SHAP Analysis Completed Successfully.\")\n\nexcept Exception as e:\n    print(f\"❌ SHAP Failed: {e}\")\n    import traceback\n    traceback.print_exc()","identifier":"94fzvl8sjexn-code","enumerator":"18","html_id":"id-94fzvl8sjexn-code","key":"JtX5d2dYi1"},{"type":"outputs","id":"DNqvDicCjBc0uhiRvYqpu","children":[{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"Data Shape: 12 steps, 26 features\nMismatch: 25 names vs 26 data features.\nSummarizing background data...\nInitializing KernelExplainer...\nCalculating SHAP values for 20 instances...\n"},"children":[],"identifier":"94fzvl8sjexn-outputs-0","html_id":"id-94fzvl8sjexn-outputs-0","key":"JR6AmJCr6X"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"  0%|          | 0/20 [00:00\u003c?, ?it/s]","content_type":"text/plain"},"application/vnd.jupyter.widget-view+json":{"content":"{\"version_major\":2,\"version_minor\":0,\"model_id\":\"0eef66dc7b3b45f581fc334c6d33db34\"}","content_type":"application/vnd.jupyter.widget-view+json"}}},"children":[],"identifier":"94fzvl8sjexn-outputs-1","html_id":"id-94fzvl8sjexn-outputs-1","key":"Lwr097X3ex"},{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"SHAP calculation complete.\n"},"children":[],"identifier":"94fzvl8sjexn-outputs-2","html_id":"id-94fzvl8sjexn-outputs-2","key":"Qo8xdiLwo3"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"\u003cFigure size 2400x2850 with 2 Axes\u003e","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"2f3397b85fc3a9579412e046c615d152","path":"/build/2f3397b85fc3a9579412e046c615d152.png"}}},"children":[],"identifier":"94fzvl8sjexn-outputs-3","html_id":"id-94fzvl8sjexn-outputs-3","key":"H03v45ENlf"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"\u003cFigure size 3600x1800 with 2 Axes\u003e","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"06e8a9bca8f286989d47cee486aaa748","path":"/build/06e8a9bca8f286989d47cee486aaa748.png"}}},"children":[],"identifier":"94fzvl8sjexn-outputs-4","html_id":"id-94fzvl8sjexn-outputs-4","key":"lcjShbFKRu"},{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"✅ SHAP Analysis Completed Successfully.\n"},"children":[],"identifier":"94fzvl8sjexn-outputs-5","html_id":"id-94fzvl8sjexn-outputs-5","key":"RLNxyRIIgb"}],"identifier":"94fzvl8sjexn-outputs","html_id":"id-94fzvl8sjexn-outputs","key":"Vb5Chuh6Wc"}],"identifier":"94fzvl8sjexn","label":"94FzvL8sJeXN","html_id":"id-94fzvl8sjexn","key":"wnbFeXamy7"},{"type":"block","kind":"notebook-content","data":{"id":"Zkmoj-onIxNy"},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Part 15: Time-Lagged Cross-Correlation (TLCC) Analysis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AdjYOIHHqi"}],"identifier":"part-15-time-lagged-cross-correlation-tlcc-analysis","label":"Part 15: Time-Lagged Cross-Correlation (TLCC) Analysis","html_id":"part-15-time-lagged-cross-correlation-tlcc-analysis","implicit":true,"key":"TBSxo00H15"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"This section introduces ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"LUp2NKkw9k"},{"type":"strong","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Time-Lagged Cross-Correlation (TLCC)","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Lc3PpRAlRl"}],"key":"VCEYDY0n55"},{"type":"text","value":" to investigate the dynamic relationships between environmental drivers and red tide concentrations. Unlike standard correlation, TLCC identifies leads and lags in the data, revealing how long it takes for a change in an environmental factor (like a nutrient spike) to manifest as a biological response (a bloom).","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Vvz320xOcw"}],"key":"wDNk9tl3zu"},{"type":"heading","depth":4,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Core Analytical Components","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"gCgAor6eW0"}],"identifier":"core-analytical-components","label":"Core Analytical Components","html_id":"core-analytical-components","implicit":true,"key":"rtZ9eVpIEC"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"The analysis provides a temporal roadmap of the ecosystem’s behavior, helping to validate the choice of lookback windows used in the LSTM model.","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"W2xP4e4VNN"}],"key":"h0ab7BxPyx"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":9,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"Identifying Temporal Offsets:","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"TEAAdpZYq3"}],"key":"d38uMihnXw"},{"type":"text","value":"\nTLCC calculates the correlation between two time series—such as river discharge and ","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"bavs3HFhgu"},{"type":"emphasis","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"Karenia brevis","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"z2Iaan0Tgf"}],"key":"yJlC1aaIYT"},{"type":"text","value":" abundance—at various time shifts. By finding the “peak” correlation, we can determine the specific latency of the system. For example, if the peak correlation with Phosphorus occurs at a lag of 14 days, it suggests a two-week window for biological uptake and population growth.","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"Ojs08s4PIs"}],"key":"LTi65gXJ9n"}],"key":"WY6F35Eg4o"},{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"Directionality of Influence:","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"z3h03ruoVP"}],"key":"nfFwiByw2A"},{"type":"text","value":"\nBy observing whether the peak correlation occurs at a positive or negative lag, we can confirm the causal direction. A positive lag (where the environmental variable precedes the bloom) confirms the variable as a leading indicator or “driver,” whereas a zero lag suggests a simultaneous response.","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"hnefyYFQoP"}],"key":"gFW0VSwql8"}],"key":"wmlBb7cBr2"},{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"Stability and Seasonality:","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"Pc6W1vBnKZ"}],"key":"KWwT6zwcDh"},{"type":"text","value":"\nThe analysis can be extended to ","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"UnX074FhYh"},{"type":"strong","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"Windowed TLCC","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"mvj54g8cMk"}],"key":"RJiMwIjn3Y"},{"type":"text","value":", which evaluates how these correlations change over time. This is particularly relevant for Florida’s coastlines, where the relationship between rainfall and red tide may shift between the wet and dry seasons.","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"DRVsnAgaei"}],"key":"u2ybPDAdeF"}],"key":"STrjMelpmd"}],"key":"xt79gVXro4"},{"type":"heading","depth":4,"position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"Insights and Model Refinement","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"aqGwqPUJkK"}],"identifier":"insights-and-model-refinement","label":"Insights and Model Refinement","html_id":"insights-and-model-refinement","implicit":true,"key":"WDFqQSsQUX"},{"type":"paragraph","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"text","value":"The results from this section serve as a critical bridge between data science and marine biology:","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"PjIY8sRLfZ"}],"key":"UjFaEji0lN"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":20,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"text","value":"Feature Engineering Validation:","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"d5W9zvPBzH"}],"key":"ATYHoYIUXM"},{"type":"text","value":" We use these findings to refine the ","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"EScS96imKH"},{"type":"inlineCode","value":"lag_days","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"CIwazOIIp9"},{"type":"text","value":" parameters in Part 4, ensuring the model focuses on the most biologically relevant timeframes.","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"AlVJs9rM8g"}],"key":"KhaaCOTYnI"}],"key":"cGpMvvXjdP"},{"type":"listItem","spread":true,"position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"System Memory:","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"C8EVbFcMi9"}],"key":"Xlkq5ZMgqV"},{"type":"text","value":" TLCC helps quantify the “memory” of the ecosystem, indicating how long an environmental disturbance continues to influence bloom dynamics.","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"ktywycgqY5"}],"key":"iplhMzdjLH"}],"key":"iFJNI45tcD"},{"type":"listItem","spread":true,"position":{"start":{"line":22,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"text","value":"Predictive Confidence:","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"H2lwoshtiW"}],"key":"iTuQ8lwmgP"},{"type":"text","value":" Identifying features with strong, stable leads increases our confidence in the LSTM’s ability to provide early warnings before a bloom is visible in satellite or field data.","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"iF4z1sSklj"}],"key":"rdvhXLM7bs"}],"key":"xhS5j4PyPd"}],"key":"lYpT2Agk6m"},{"type":"paragraph","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"text","value":"By incorporating TLCC, we move beyond static snapshots to a fluid, temporal understanding of the drivers behind red tide events.","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"key":"tVQSudVlLu"}],"key":"idXF5HAmKh"}],"identifier":"zkmoj-onixny","label":"Zkmoj-onIxNy","html_id":"zkmoj-onixny","key":"duRJsyrnxp"},{"type":"block","kind":"notebook-code","data":{"id":"lOJZAAjMLdQx","ExecuteTime":{"end_time":"2026-01-07T03:44:43.386027800Z","start_time":"2026-01-07T03:37:59.138408Z"},"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"dd624703-0f2d-419f-bed5-d143620904e8"},"children":[{"type":"code","lang":"python","executable":true,"value":"def analyze_onset_lag(y_true, y_pred_prob, threshold=0.3, tolerance=4):\n    \"\"\"pynb i\n    Calculates exactly how many weeks late (or early) the model predicts blooms.\n\n    Args:\n        y_true: Binary actuals (0/1)\n        y_pred_prob: Continuous probabilities (0.0-1.0)\n        threshold: Decision threshold\n        tolerance: Max weeks to look for a matching predicted onset\n    \"\"\"\n    print(f\"\\n--- Onset Latency Analysis (Threshold: {threshold}) ---\")\n\n    y_pred_class = (y_pred_prob \u003e= threshold).astype(int)\n\n    # 1. Identify \"Start\" events (0 -\u003e 1 transitions)\n    # We use diff() to find where value changes from 0 to 1\n    actual_onsets = np.where(np.diff(y_true, prepend=0) == 1)[0]\n    pred_onsets = np.where(np.diff(y_pred_class, prepend=0) == 1)[0]\n\n    print(f\"Actual Bloom Starts found: {len(actual_onsets)}\")\n    print(f\"Predicted Bloom Starts found: {len(pred_onsets)}\")\n\n    if len(actual_onsets) == 0:\n        print(\"No blooms in test set to analyze.\")\n        return\n\n    # 2. Match Actual Starts to Nearest Predicted Start\n    lags = []\n\n    for t_actual in actual_onsets:\n        # Find predicted onsets within 'tolerance' weeks window\n        # We look for the closest prediction around the actual event\n        nearby_preds = pred_onsets[np.abs(pred_onsets - t_actual) \u003c= tolerance]\n\n        if len(nearby_preds) \u003e 0:\n            # Find the closest one\n            closest_pred = nearby_preds[np.argmin(np.abs(nearby_preds - t_actual))]\n\n            # Lag = Predicted Time - Actual Time\n            # Positive = Late (Lag)\n            # Negative = Early Warning (Lead)\n            lag = closest_pred - t_actual\n            lags.append(lag)\n\n            status = \"LATE\" if lag \u003e 0 else \"EARLY\" if lag \u003c 0 else \"PERFECT\"\n            print(f\"  Event at Week {t_actual}: Model is {status} by {abs(lag)} weeks.\")\n        else:\n            print(f\"  Event at Week {t_actual}: MISSED COMPLETELY (No prediction within {tolerance} weeks)\")\n            lags.append(np.nan) # Missed event\n\n    # 3. Summary Stats\n    lags_clean = [l for l in lags if not np.isnan(l)]\n    if lags_clean:\n        avg_lag = np.mean(lags_clean)\n        print(f\"\\n\u003e\u003e\u003e AVERAGE LATENCY: {avg_lag:.2f} Weeks\")\n        if avg_lag \u003e 0:\n            print(\"    (Positive = The model is LAGGING)\")\n        else:\n            print(\"    (Negative = The model gives EARLY WARNING)\")\n    else:\n        print(\"\\n\u003e\u003e\u003e No matched events found.\")\n\n    # 4. Visualizing the Lag\n    # Cross-Correlation Plot (Statistical Proof)\n    # Shift predictions back/forward and see where correlation is highest\n    shifts = range(-5, 6) # Shift -5 to +5 weeks\n    correlations = []\n    for s in shifts:\n        # Shift predicted probabilities\n        if s \u003c 0:\n            p_shifted = y_pred_prob[-s:]\n            y_shifted = y_true[:s]\n        elif s \u003e 0:\n            p_shifted = y_pred_prob[:-s]\n            y_shifted = y_true[s:]\n        else:\n            p_shifted = y_pred_prob\n            y_shifted = y_true\n\n        # Calculate correlation\n        if len(y_shifted) \u003e 0:\n            corr = np.corrcoef(y_shifted, p_shifted)[0, 1]\n            correlations.append(corr)\n        else:\n            correlations.append(0)\n\n    plt.figure(figsize=(8, 5), dpi=300)\n    plt.bar(shifts, correlations, color='skyblue', edgecolor='black')\n\n    # Highlight the max\n    max_idx = np.argmax(correlations)\n    best_lag = shifts[max_idx]\n    plt.bar(best_lag, correlations[max_idx], color='red', label=f'Peak Correlation (Lag={best_lag})')\n\n    plt.title(\"Time-Lagged Cross-Correlation (TLCC)\")\n    plt.xlabel(\"Lag (Weeks)\\n\u003c-- Model Leads (Good) | Model Lags (Bad) --\u003e\")\n    plt.ylabel(\"Correlation with Actuals\")\n    plt.axvline(0, color='black', linestyle='--')\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(\"output/lag_analysis.png\")\n    plt.show()\n\n# --- Execute ---\nif __name__ == \"__main__\":\n    if 'mc_preds' in locals() and 'y_test_trimmed' in locals():\n        # Get mean probs\n        probs = np.mean(mc_preds, axis=1)\n\n        # Run with your chosen threshold (e.g., 0.3)\n        analyze_onset_lag(y_test_trimmed.flatten(), probs.flatten(), threshold=0.3)","identifier":"lojzaajmldqx-code","enumerator":"19","html_id":"lojzaajmldqx-code","key":"qlITKRv9Pg"},{"type":"outputs","id":"hFod345M9-CRs2GBbLjhu","children":[{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"\n--- Onset Latency Analysis (Threshold: 0.3) ---\nActual Bloom Starts found: 10\nPredicted Bloom Starts found: 4\n  Event at Week 8: Model is PERFECT by 0 weeks.\n  Event at Week 70: MISSED COMPLETELY (No prediction within 4 weeks)\n  Event at Week 98: MISSED COMPLETELY (No prediction within 4 weeks)\n  Event at Week 103: MISSED COMPLETELY (No prediction within 4 weeks)\n  Event at Week 112: MISSED COMPLETELY (No prediction within 4 weeks)\n  Event at Week 168: Model is LATE by 1 weeks.\n  Event at Week 179: MISSED COMPLETELY (No prediction within 4 weeks)\n  Event at Week 182: MISSED COMPLETELY (No prediction within 4 weeks)\n  Event at Week 190: MISSED COMPLETELY (No prediction within 4 weeks)\n  Event at Week 196: MISSED COMPLETELY (No prediction within 4 weeks)\n\n\u003e\u003e\u003e AVERAGE LATENCY: 0.50 Weeks\n    (Positive = The model is LAGGING)\n"},"children":[],"identifier":"lojzaajmldqx-outputs-0","html_id":"lojzaajmldqx-outputs-0","key":"LdBGtgYJVO"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"\u003cFigure size 2400x1500 with 1 Axes\u003e","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"03025f3dd8a708372714dfae593a7247","path":"/build/03025f3dd8a708372714dfae593a7247.png"}}},"children":[],"identifier":"lojzaajmldqx-outputs-1","html_id":"lojzaajmldqx-outputs-1","key":"GbptaAOSvL"}],"identifier":"lojzaajmldqx-outputs","html_id":"lojzaajmldqx-outputs","key":"MnDrST6IrM"}],"identifier":"lojzaajmldqx","label":"lOJZAAjMLdQx","html_id":"lojzaajmldqx","key":"Oh5M6GVOGn"}],"key":"HBUAu7v32I"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{}},"domain":"http://localhost:3000"},"project":{"title":"Mitigating prediction drift in harmful algal bloom forecasting: A hybrid deep learning and ensemble Kalman filter framework","authors":[{"nameParsed":{"literal":"Victor M. Blanco","given":"Victor M.","family":"Blanco"},"name":"Victor M. Blanco","id":"contributors-Users\\victo\\PyCharmMiscProject\\myst-generated-uid-0"},{"nameParsed":{"literal":"Daniel Krutky","given":"Daniel","family":"Krutky"},"name":"Daniel Krutky","id":"contributors-Users\\victo\\PyCharmMiscProject\\myst-generated-uid-1"},{"nameParsed":{"literal":"Peter Nguyen","given":"Peter","family":"Nguyen"},"name":"Peter Nguyen","id":"contributors-Users\\victo\\PyCharmMiscProject\\myst-generated-uid-2"},{"nameParsed":{"literal":"Ahmed S. Elshall","given":"Ahmed S.","family":"Elshall"},"name":"Ahmed S. Elshall","id":"contributors-Users\\victo\\PyCharmMiscProject\\myst-generated-uid-3"},{"nameParsed":{"literal":"Ming Ye","given":"Ming","family":"Ye"},"name":"Ming Ye","id":"contributors-Users\\victo\\PyCharmMiscProject\\myst-generated-uid-4"},{"nameParsed":{"literal":"Michael L. Parsons","given":"Michael L.","family":"Parsons"},"name":"Michael L. Parsons","id":"contributors-Users\\victo\\PyCharmMiscProject\\myst-generated-uid-5"}],"keywords":["harmful algae blooms","red tides","machine learning","deep learning","ensemble Kalman filter","LSTM","uncertainty analysis","data assimilation","Florida"],"id":"98e22f5a-97c3-43e5-9695-946bb291d410","exports":[],"bibliography":[],"index":"lstm-enkf","pages":[]}}},"actionData":null,"errors":null},"future":{"unstable_dev":false,"unstable_postcss":false,"unstable_tailwind":false,"v2_errorBoundary":true,"v2_headers":true,"v2_meta":true,"v2_normalizeFormMethod":true,"v2_routeConvention":true}};</script><script type="module" async="">import "/build/manifest-3595279D.js";
import * as route0 from "/build/root-SIO6LUTY.js";
import * as route1 from "/build/routes/_index-QUJ6PDLY.js";
window.__remixRouteModules = {"root":route0,"routes/_index":route1};

import("/build/entry.client-PCJPW7TK.js");</script></body></html>