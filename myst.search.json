{"version":"1","records":[{"hierarchy":{"lvl1":"Outline"},"type":"lvl1","url":"/lstm-enkf","position":0},{"hierarchy":{"lvl1":"Outline"},"content":"This project presents a novel integration of Deep Learning and Sequential Data Assimilation to forecast Karenia brevis (red tide) blooms. While LSTMs are powerful at capturing non-linear temporal dependencies, they often suffer from “drift” when applied to long-term forecasting without mid-course corrections. Our framework solves this by treating the LSTM as the “transition model” within an Ensemble Kalman Filter (EnKF) loop.\n\nResearch Objectives\nThe primary goal of this research is to develop a robust, self-correcting forecasting system for harmful algal blooms along the Florida coast.\n\nArchitecture Design: Construct an LSTM neural network optimized for high-dimensional environmental time-series and extreme class imbalance.\n\nHybrid Integration: Develop a mathematical bridge that allows LSTM state outputs to be assimilated and corrected by an Ensemble Kalman Filter (EnKF) in real-time.\n\nUncertainty Quantification: Implement Monte Carlo (MC) Dropout to shift from deterministic “point-forecasts” to probabilistic risk assessments.\n\nEcological Interpretability: Quantify the sensitivity of the model to specific environmental drivers like nutrient loading and river discharge using SHAP values.\n\nResearch Questions\nTo evaluate the effectiveness of this hybrid framework, this project seeks to answer:\n\nCan data assimilation mitigate LSTM drift? To what extent does the periodic injection of physical observations via EnKF improve the long-term stability of K. brevis forecasts?\n\nHow does ecological memory impact accuracy? Does the inclusion of long-term rolling aggregates and multi-day lags significantly outperform models relying only on immediate environmental snapshots?\n\nWhat is the “Reliability-Resolution” trade-off? How well does the MC Dropout ensemble capture the actual variance of bloom occurrences?\n\nWhich environmental drivers dominate the model’s decision-making? Does the model prioritize nutrient concentrations, physical transport (discharge), or biological persistence when predicting a bloom?\n\nTechnical Roadmap\n\nData Foundation & Enhanced Engineering\nThe predictive power of this framework rests on capturing the “ecological memory” of the Florida coast.\n\nFeature Engineering (Parts 1-4): We move beyond raw concentrations to create Rolling Aggregates (7, 14, and 30-day windows) and Time-Lagged Variables. This allows the model to “see” nutrient accumulation trends and delayed river discharge impacts.\n\nRobust Preprocessing: Using RobustScaler, we ensure that extreme outliers—common in nutrient data during hurricane or high-flow events—do not skew the model’s weight distribution.\n\nThe LSTM Predictive Engine\nArchitecture & Tuning (Parts 5-9): We implement a stacked LSTM architecture designed to process 3D temporal sequences. To combat extreme class imbalance, we utilize Custom Class Weighting in the loss function to penalize the misclassification of rare bloom events more heavily.\n\nHyperparameter Optimization: We utilize Bayesian optimization to determine the ideal hidden layer depth, dropout rates, and optimal sequence length (lookback).\n\nThe Hybrid Loop: Data Assimilation (EnKF)\nThis is the core innovation: anchoring deep learning predictions to physical reality.\n\nThe Forecast Step: The LSTM generates a state estimate for the next time step.\n\nThe Assimilation Step (Parts 10-12): When a new physical observation (e.g., a nitrogen measurement) becomes available, the EnKF calculates the Kalman Gain. This corrects the model’s internal state variables before the next recursive forecast, effectively resetting the “drift.”\n\nMC Dropout: By keeping dropout active during inference, we generate an ensemble of predictions to quantify Epistemic Uncertainty.\n\nAdvanced Evaluation & Explainability\nProbabilistic Evaluation (Part 13): We evaluate using Brier Skill Scores to measure the calibration of our probability forecasts and Precision-Recall Curves to assess bloom detection reliability.\n\nOpening the Black Box (Parts 14-15): * SHAP Analysis: Identifying which specific nutrients are driving “high probability” forecasts.\n\nTLCC Analysis: Validating the lead-time of features to ensure the LSTM is learning biologically plausible causal relationships.\n\n","type":"content","url":"/lstm-enkf","position":1},{"hierarchy":{"lvl1":"Outline","lvl2":"Library installation"},"type":"lvl2","url":"/lstm-enkf#library-installation","position":2},{"hierarchy":{"lvl1":"Outline","lvl2":"Library installation"},"content":"\n\n!pip install tensorflow.keras\n!pip install keras_tuner\nimport json\nimport os\n\nimport joblib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport shap\nimport tensorflow as tf\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score,\n                             brier_score_loss, accuracy_score, precision_score,\n                             recall_score, f1_score)\nfrom sklearn.metrics import (precision_recall_curve)\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, Input\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.metrics import BinaryAccuracy, AUC, Recall, Precision\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\ntry:\n    from tqdm.notebook import tqdm\n    print(\"Using tqdm.notebook for progress bars.\")\nexcept ImportError:\n    try:\n        from tqdm import tqdm\n        print(\"Using standard tqdm for progress bars.\")\n    except ImportError:\n        print(\"Warning: tqdm not installed. Progress bars will not be shown.\")\n        # Define a dummy tqdm function if not installed\n        def tqdm(iterable=None, *args, **kwargs):\n            if iterable is None:\n                # Handle case where tqdm is called without an iterable\n                class DummyTqdm:\n                    def __enter__(self): return self\n                    def __exit__(self, *args): pass\n                    def update(self, n=1): pass\n                    def close(self): pass\n                    def set_description(self, desc): pass\n                return DummyTqdm()\n            else:\n                return iterable\n\nprint(\"Imported libraries.\")\n\n\n\n","type":"content","url":"/lstm-enkf#library-installation","position":3},{"hierarchy":{"lvl1":"Outline","lvl3":"Part 1: Configuration & Setup","lvl2":"Library installation"},"type":"lvl3","url":"/lstm-enkf#part-1-configuration-setup","position":4},{"hierarchy":{"lvl1":"Outline","lvl3":"Part 1: Configuration & Setup","lvl2":"Library installation"},"content":"Define all file paths (input data, saved components like scaler/model).\n\nSet core parameters: SEQUENCE_LENGTH, FORECAST_HORIZON, BLOOM_THRESHOLD.\n\nDefine feature lists: BASE_FEATURES, ENKF_STATE_VARS, ENHANCED_KB_LAGS (optional).\n\nControl Flags/Options:\n\nUSE_ENHANCED_FEATURES: Boolean (True/False) to switch between basic and enhanced feature engineering.\n\nSCALER_TYPE: String (‘Standard’ or ‘Robust’) to choose the scaler.\n\nUSE_CLASS_WEIGHT: Boolean (True/False) to enable/disable class weighting during training.\n\nPERFORM_TUNING: Boolean (True/False) to run hyperparameter tuning or use defaults.\n\nSet tuning parameters: MAX_TRIALS, TUNER_EPOCHS.\n\nSet training parameters: EPOCHS, BATCH_SIZE, PATIENCE.\n\nSet EnKF parameters: N_ENKF (ensemble size), define noise matrix estimation approach (e.g., ‘basic_stats’ or ‘manual’).\n\nSet MC Dropout parameters: N_MC_SAMPLES.\n\n# Set these flags to control the workflow execution\n# === Feature Engineering ===\n# Set to True to use detailed lags (like RF paper), False: Use basic lags\nUSE_ENHANCED_FEATURES = True\n\n# === Preprocessing ===\n# Options: 'Robust' or 'Standard'\nSCALER_TYPE = 'Robust'\n\n# === Training ===\n# Set to True to apply class weighting during LSTM training\nUSE_CLASS_WEIGHT = True\n# Set to True to run KerasTuner hyperparameter search, False to use default HPs\nPERFORM_TUNING = True\n\n# === Advanced Steps ===\n# Set to True to run EnKF data assimilation during testing/forecasting\nPERFORM_ENKF = True\n\n\n\n# --- File Paths ---\nINPUT_DATA_PATH = 'data_weekly_interpolated.csv'\n# Directory for optional external files (e.g., Caloosahatchee) - Ensure this exists if used\nEXTERNAL_DATA_DIR = 'external_data/'\n# Directory to save results, models, scalers (use a distinct name)\nOUTPUT_DIR = 'output_refactored/'\n# Construct filenames dynamically based on config where appropriate\nscaler_suffix = SCALER_TYPE.lower()\nfeature_suffix = 'enhanced' if USE_ENHANCED_FEATURES else 'basic'\nSCALER_FILENAME = os.path.join(OUTPUT_DIR, f'red_tide_scaler_{scaler_suffix}.joblib')\nFEATURE_LIST_FILENAME = os.path.join(OUTPUT_DIR, f'red_tide_feature_list_{feature_suffix}.joblib')\n# Template for sequence files: {horizon}{feature_suffix}\nSEQUENCES_FILENAME_TEMPLATE = os.path.join(OUTPUT_DIR, 'sequences_horizon{}wk_{}.npz')\n# Template for model checkpoint files: {model_type} e.g., baseline_weighted\n# Using .keras extension for saving the full model (architecture + weights + optimizer state)\nMODEL_CHECKPOINT_TEMPLATE = os.path.join(OUTPUT_DIR, 'best_lstm_model_{}.keras')\n# Use .weights.h5 if saving only weights (e.g., for subclassed models like physics-informed)\nMODEL_WEIGHTS_CHECKPOINT_TEMPLATE = os.path.join(OUTPUT_DIR, 'best_lstm_model_{}.weights.h5')\n\nTUNER_PROJECT_DIR = 'keras_tuner_dir_refactored' # Use a new name\nTUNER_PROJECT_NAME = 'red_tide_lstm_tuning'\n\n# Create output directory if it doesn't exist\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nprint(f\"Output directory: {OUTPUT_DIR}\")\n\n\n\n# --- Core Parameters ---\nDATETIME_COL = 'time'\nTARGET_COL = 'kb' # Original cell count column\nTARGET_BINARY_COL = 'bloom_target' # Binary target column name\nBLOOM_THRESHOLD = 1e5 # Cells/L\nSEQUENCE_LENGTH = 12  # Default sequence length (can be tuned)\nFORECAST_HORIZON = 1   # Default forecast horizon (1-week or 4-week)\n\n\n\n# --- Feature Engineering Configuration ---\nBASE_FEATURES = [ # Features from the core dataset to consider initially\n    'zos', 'water_temp',\n    'peace_discharge', 'peace_TN', 'peace_TP',\n    'wind_u', 'wind_v'\n]\n# Define basic lag configuration\nBASIC_LAG_CONFIG = {\n    'kb_lags': [1, 2],\n    'env_lags': list(range(1, 7)) # e.g., 1 to 6 weeks\n}\n# Define enhanced lag configuration (matching RF paper more closely)\nENHANCED_LAG_CONFIG = {\n    # 'kb_lags': [1, 2],\n    'kb_rolling_windows': [(1, 4), (5, 8), (9, 12)], # (start_lag, end_lag) for mean & prop\n    'discharge_rolling_window': 4, # e.g., 4-week avg discharge lag 1\n    'env_lags': [1] # Lags for other env vars in enhanced mode\n}\n\n# --- Data Splitting ---\nTRAIN_SPLIT_RATIO = 0.70\nVALIDATION_SPLIT_RATIO = 0.15\n# Test split is the remainder\n\n# --- LSTM Model Default Hyperparameters (Used if PERFORM_TUNING is False) ---\nDEFAULT_LSTM_UNITS = 64\nDEFAULT_DROPOUT_RATE = 0.3\nDEFAULT_LEARNING_RATE = 0.001\n\n# --- KerasTuner Configuration (Used if PERFORM_TUNING is True) ---\nTUNER_MAX_TRIALS = 10\nTUNER_EPOCHS = 30\nTUNER_BATCH_SIZE = 32\n\n# --- Training Parameters ---\nTRAIN_EPOCHS = 50\nTRAIN_BATCH_SIZE = 32\nTRAIN_PATIENCE = 10 # For EarlyStopping\n\n# --- EnKF Configuration ---\nENKF_STATE_VARS = [\n    'peace_discharge', 'peace_TN', 'peace_TP', 'kb','wind_u', 'wind_v'\n]\n# Variables to assimilate\nN_ENKF = 50 # Ensemble size\n# Noise Estimation Approach ('basic_stats' uses train set stats, 'manual' requires defining R_diag, Q_diag below)\nENKF_NOISE_ESTIMATION = 'manual'\n# Manual noise variances (used only if ENKF_NOISE_ESTIMATION = 'manual') - Define placeholder values\nMANUAL_R_DIAG = [\n    (100.0 * 0.01)**2, # Discharge (Trust the gauge)\n    (0.2 * 0.01)**2,   # TN (Trust the sample)\n    (0.02 * 0.01)**2,  # TP\n    (5.0 * 0.05)**2,   # Wind U (Wind is noisy, maybe 5% error)\n    (5.0 * 0.05)**2,   # Wind V\n    (0.1 * 0.01)**2    # ZOS (Trust the satellite) # Example observation noise variances\n    ]\nMANUAL_Q_DIAG = [\n    (150.0 * 2.0)**2,  # Discharge can spike massively\n    (0.3 * 2.0)**2,    # TN spikes\n    (0.03 * 2.0)**2,   # TP spikes\n    (10.0 * 1.5)**2,   # Wind changes direction rapidly\n    (10.0 * 1.5)**2,   # Wind V\n    (0.1 * 1.0)**2     # ZOS changes\n]\n\n# --- MC Dropout Configuration ---\nN_MC_SAMPLES = 50\n\n\n# --- Print Setup Summary ---\nprint(\"\\n--- Workflow Configuration Summary ---\")\nprint(f\"Enhanced Features Enabled: {USE_ENHANCED_FEATURES}\")\nprint(f\"Scaler Type Selected: {SCALER_TYPE}\")\nprint(f\"Class Weighting Enabled: {USE_CLASS_WEIGHT}\")\nprint(f\"Hyperparameter Tuning Enabled: {PERFORM_TUNING}\")\nprint(f\"EnKF Enabled: {PERFORM_ENKF}\")\nprint(f\"Sequence Length: {SEQUENCE_LENGTH}\")\nprint(f\"Forecast Horizon: {FORECAST_HORIZON} week(s)\")\nif PERFORM_TUNING:\n    print(f\"Tuner Max Trials: {TUNER_MAX_TRIALS}, Epochs per Trial: {TUNER_EPOCHS}\")\nelse:\n    print(f\"Using Default LSTM HPs: Units={DEFAULT_LSTM_UNITS}, Dropout={DEFAULT_DROPOUT_RATE}, LR={DEFAULT_LEARNING_RATE}\")\nif PERFORM_ENKF:\n    print(f\"EnKF Ensemble Size: {N_ENKF}, Noise Estimation: {ENKF_NOISE_ESTIMATION}\")\nprint(\"------------------------------------\")\n\n# Check if optional modules were imported if flags are set\nif PERFORM_TUNING and kt is None:\n    print(\"\\nWarning: KerasTuner (kt) not imported/installed, but PERFORM_TUNING is True. Tuning will be skipped.\")\n    PERFORM_TUNING = False # Disable tuning if library not available\nif PERFORM_ENKF and 'EnsembleKalmanFilter' not in locals():\n     # We will define EnKF class later, but good to note dependency\n     print(\"\\nNote: EnKF flag is True, ensure EnsembleKalmanFilter class is defined/imported later.\")\n\n\n\n","type":"content","url":"/lstm-enkf#part-1-configuration-setup","position":5},{"hierarchy":{"lvl1":"Outline","lvl3":"Part 2: Data Loading & Initial Processing","lvl2":"Library installation"},"type":"lvl3","url":"/lstm-enkf#part-2-data-loading-initial-processing","position":6},{"hierarchy":{"lvl1":"Outline","lvl3":"Part 2: Data Loading & Initial Processing","lvl2":"Library installation"},"content":"This cell defines and executes a function to load the raw weekly data, handle the datetime index, calculate U/V wind components, and create the binary target variable based on the bloom threshold. It uses the configuration parameters defined in Part 1.\n\nExplanation:\n\ncalculate_wind_components(df): Takes a DataFrame, calculates U and V wind components if ‘wind_direction’ and ‘wind_speed’ exist, drops the original columns, and returns the modified DataFrame and a list of the newly added column names. Includes basic NaN handling for the calculation.\n\ncreate_target(...): Takes the DataFrame and target configuration, ensures the original target column is numeric, drops rows where the target is NaN (important for supervised learning), creates the binary bloom_target column, prints the class distribution, and returns the DataFrame.\n\nload_and_prepare_data(...): Orchestrates the initial steps: loads the CSV, sets the datetime index, calls calculate_wind_components, calls create_target, and returns the resulting DataFrame (df_initial). Includes error handling for file not found or missing columns.\n\nmerge_external_data(...) (Optional Placeholder): Provides a structure for loading and merging additional datasets based on a dictionary of file paths. It performs a left merge to keep all original data points. Note: This function needs actual file paths and assumes external CSVs have a compatible datetime index.\n\nMain Execution Block (if name == “main”:): Calls load_and_prepare_data using the configuration variables. Includes a commented-out section showing how merge_external_data would be called if needed. Prints final info about the df_initial DataFrame. Includes basic NameError handling in case Part 1 wasn’t run.\n\nAfter running this cell, the df_initial DataFrame should contain the core data with wind components calculated and the binary bloom target created, ready for the feature engineering steps in Part 3.\n\n# Ensure configuration variables from Part 1 are accessible in the environment\n# Example: INPUT_DATA_PATH, DATETIME_COL, TARGET_COL, BLOOM_THRESHOLD, TARGET_BINARY_COL\n\ndef calculate_wind_components(df):\n    \"\"\"Calculates U and V wind components if columns exist.\"\"\"\n    if 'wind_direction' in df.columns and 'wind_speed' in df.columns:\n        print(\"Calculating wind U/V components...\")\n        # Ensure numeric types, handle potential NaNs before calculation\n        df['wind_direction'] = pd.to_numeric(df['wind_direction'], errors='coerce')\n        df['wind_speed'] = pd.to_numeric(df['wind_speed'], errors='coerce')\n\n        # Temporarily fill NaNs with 0 for calculation if they exist\n        wind_cols_to_check = ['wind_direction', 'wind_speed']\n        if df[wind_cols_to_check].isnull().any().any():\n            print(\"Warning: NaNs found in wind direction/speed. Temporarily filling with 0 for component calculation.\")\n            df[wind_cols_to_check] = df[wind_cols_to_check].fillna(0)\n\n        wind_dir_rad = np.deg2rad(df['wind_direction'])\n        wind_speed = df['wind_speed']\n        # Meteorological convention: wind direction 'coming from'\n        df['wind_u'] = -wind_speed * np.sin(wind_dir_rad)\n        df['wind_v'] = -wind_speed * np.cos(wind_dir_rad)\n        # Drop original wind columns\n        df = df.drop(columns=['wind_direction', 'wind_speed'])\n        print(\"Calculated wind U/V components and dropped original columns.\")\n        added_cols = ['wind_u', 'wind_v']\n    else:\n        print(\"Warning: 'wind_direction' or 'wind_speed' not found. Skipping component calculation.\")\n        added_cols = []\n    return df, added_cols\n\ndef create_target(df, target_col, bloom_threshold, target_binary_col):\n    \"\"\"Creates the binary target variable.\"\"\"\n    if target_col not in df.columns:\n        print(f\"Error: Target column '{target_col}' not found.\")\n        return None # Return None if target column is missing\n\n    print(f\"Creating binary target '{target_binary_col}' using threshold {bloom_threshold:.0f} cells/L...\")\n    df[target_col] = pd.to_numeric(df[target_col], errors='coerce')\n\n    # Handle potential NaNs in target before comparison by dropping rows\n    initial_rows = len(df)\n    if df[target_col].isnull().any():\n        print(f\"Warning: NaNs found in target column '{target_col}'. Dropping rows with NaN target.\")\n        df.dropna(subset=[target_col], inplace=True)\n        print(f\"Dropped {initial_rows - len(df)} rows with NaN in '{target_col}'. New shape: {df.shape}\")\n\n    df[target_binary_col] = (df[target_col] >= bloom_threshold).astype(int)\n    print(f\"Target distribution (%):\\n{df[target_binary_col].value_counts(normalize=True) * 100}\")\n    return df\n\ndef load_and_prepare_data(filepath, datetime_col, target_col, bloom_threshold, target_binary_col):\n    \"\"\"Loads data, handles datetime, calculates wind components, creates target.\"\"\"\n    print(f\"Loading data from: {filepath}\")\n    try:\n        df = pd.read_csv(filepath)\n        # Handle datetime index\n        if datetime_col not in df.columns:\n            raise ValueError(f\"Datetime column '{datetime_col}' not found.\")\n        df[datetime_col] = pd.to_datetime(df[datetime_col])\n        df = df.sort_values(datetime_col).set_index(datetime_col)\n        print(f\"Data loaded successfully. Shape: {df.shape}, Time range: {df.index.min()} to {df.index.max()}\")\n        print(f\"Initial NaN counts:\\n{df.isnull().sum()}\")\n\n        # Calculate wind components\n        df, wind_cols = calculate_wind_components(df)\n\n        # Create binary target\n        df = create_target(df, target_col, bloom_threshold, target_binary_col)\n\n        if df is not None:\n            print(\"\\n--- Initial Data Preparation Complete ---\")\n            print(f\"DataFrame shape after initial processing: {df.shape}\")\n            print(\"Columns:\", df.columns.tolist())\n            print(\"\\nFirst 5 rows:\")\n            print(df.head())\n        return df\n\n    except FileNotFoundError:\n        print(f\"Error: Input data file not found at {filepath}\")\n        return None\n    except ValueError as ve:\n        print(f\"ValueError during data preparation: {ve}\")\n        return None\n    except Exception as e:\n        print(f\"An unexpected error occurred during data loading/preparation: {e}\")\n        return None\n\n# --- Optional: Function Placeholder for Merging External Data ---\ndef merge_external_data(df, external_files_dict):\n    \"\"\"\n    Loads and merges external data (e.g., Caloosahatchee Q/NOx) into the main dataframe.\n    Args:\n        df (pd.DataFrame): The main dataframe with datetime index.\n        external_files_dict (dict): Dictionary where keys are column names (e.g., 'caloos_Q')\n                                     and values are file paths to the external CSV data.\n                                     External CSVs must have a compatible datetime index.\n    Returns:\n        pd.DataFrame: DataFrame with external data merged (left join).\n    \"\"\"\n    print(\"\\n--- Merging External Data (Placeholder) ---\")\n    if not external_files_dict:\n        print(\"No external data files specified.\")\n        return df\n\n    df_merged = df.copy()\n    for col_name, file_path in external_files_dict.items():\n        try:\n            print(f\"Loading external data for '{col_name}' from '{file_path}'...\")\n            # Assuming external CSV has datetime index named same as DATETIME_COL or is the index\n            df_ext = pd.read_csv(file_path, index_col=DATETIME_COL, parse_dates=True) # Adjust index_col if needed\n            df_ext = df_ext[[col_name]] # Keep only the specified column\n            # Perform left merge\n            df_merged = df_merged.merge(df_ext, left_index=True, right_index=True, how='left')\n            print(f\"Merged '{col_name}'. NaN count: {df_merged[col_name].isnull().sum()}\")\n        except FileNotFoundError:\n            print(f\"Warning: External data file not found: {file_path}. Skipping '{col_name}'.\")\n        except KeyError:\n             print(f\"Warning: Column '{col_name}' not found in file {file_path}. Skipping.\")\n        except Exception as e:\n            print(f\"Error merging external file {file_path}: {e}\")\n\n    print(\"External data merging complete.\")\n    return df_merged\n\n\n# --- Execute Data Loading and Preparation ---\n# Ensure config variables from Part 1 are defined before running this\nif __name__ == \"__main__\":\n    try:\n        df_initial = load_and_prepare_data(\n            INPUT_DATA_PATH,\n            DATETIME_COL,\n            TARGET_COL,\n            BLOOM_THRESHOLD,\n            TARGET_BINARY_COL\n        )\n\n        if df_initial is not None:\n            # Display basic info about the prepared dataframe\n            print(\"\\n--- Dataframe after initial processing (df_initial) ---\")\n            df_initial.info()\n        else:\n            print(\"\\nData loading/preparation failed.\")\n\n    except NameError as ne:\n         print(f\"NameError: A configuration variable might be missing. Did Part 1 run correctly? Error: {ne}\")\n    except Exception as e:\n         print(f\"An unexpected error occurred in main execution block: {e}\")\n\n\n\n\n\n","type":"content","url":"/lstm-enkf#part-2-data-loading-initial-processing","position":7},{"hierarchy":{"lvl1":"Outline","lvl3":"Part 3: Feature Engineering","lvl2":"Library installation"},"type":"lvl3","url":"/lstm-enkf#part-3-feature-engineering","position":8},{"hierarchy":{"lvl1":"Outline","lvl3":"Part 3: Feature Engineering","lvl2":"Library installation"},"content":"This part defines and executes a function create_features that adds lagged features to the initially processed DataFrame (df_initial from Part 2). It uses the configuration flags set in Part 1 (USE_ENHANCED_FEATURES) to determine whether to create basic lags or the more detailed lags inspired by the Random Forest paper.\n\nThis cell defines the feature engineering function and applies it to the df_initial DataFrame. It creates lagged versions of environmental variables and, optionally, more detailed lagged and rolling aggregate features for K. brevis counts.\n\nExplanation:\n\nConfiguration: It first defines or ensures access to the necessary configuration variables from Part 1 (like USE_ENHANCED_FEATURES, lag definitions, feature lists).\n\ncreate_features(...) Function:\n\nTakes the initial DataFrame (df_initial) and configuration details as input.\n\nSelects either BASIC_LAG_CONFIG or ENHANCED_LAG_CONFIG based on the USE_ENHANCED_FEATURES flag.\n\nKB Lags: Creates simple weekly lags (_L1, _L2) for the raw kb column.\n\nKB Rolling Aggregates (Enhanced Only): Calculates rolling mean (_M1_mean, _M2_mean, etc.) and rolling proportion of bloom weeks (_M1_prop, _M2_prop, etc.) over specified windows (e.g., 4, 8, 12 weeks), shifted back appropriately.\n\nDischarge Rolling Average (Enhanced Only): Calculates a rolling average (e.g., 4-week) for the primary discharge column, lagged by 1 week.\n\nEnvironmental Lags: Creates simple weekly lags for all other specified environmental/hydrological variables (BASE_FEATURES). The number of lags depends on whether basic or enhanced mode is selected.\n\nNaN Handling: Tracks the maximum lag introduced by any operation and drops the corresponding number of initial rows from the DataFrame to ensure sequences are complete.\n\nAll-NaN Column Check: Includes a check to identify and optionally drop columns that might become entirely NaN after lagging (important for sparse data).\n\nReturns the final DataFrame (df_processed) with all engineered features.\n\nMain Execution: Calls the create_features function with the appropriate arguments based on the configuration flags. Prints the head and tail of the resulting df_processed DataFrame.\n\nAfter running this cell, df_processed will contain the data ready for splitting (Part 4) and subsequent preprocessing/modeling steps. The number of columns will vary depending on whether basic or enhanced features were generated.\n\n# --- Configuration (Ensure these are defined from Part 1 or redefined here) ---\nTARGET_COL = 'kb'\nTARGET_BINARY_COL = 'bloom_target'\nUSE_ENHANCED_FEATURES = True # Set based on Part 1 config\n\n# Define lag configurations (can be pulled from Part 1 config)\nBASIC_LAG_CONFIG = {\n    'kb_lags': [1, 2],\n    'env_lags': list(range(1, 7))\n}\nENHANCED_LAG_CONFIG = {\n    'kb_lags': [1, 2],\n    'kb_rolling_windows': [(1, 4), (5, 8), (9, 12)], # (start_lag, end_lag)\n    'discharge_rolling_window': 4, # Window size for rolling avg discharge\n    'env_lags': [1] # Lags for other env vars in enhanced mode\n}\n# Features to lag (environmental/hydrological) - should include wind components if created\nBASE_FEATURES = [\n    'zos', 'water_temp',\n    'peace_discharge', 'peace_TN', 'peace_TP',\n    'wind_u', 'wind_v'\n]\n\n# --- Function Definition ---\ndef create_features(df, target_col, target_binary_col, base_features, use_enhanced=False, basic_cfg=None, enhanced_cfg=None):\n    \"\"\"Creates lagged and rolling features based on configuration.\"\"\"\n    if df is None:\n        print(\"Error in create_features: Input DataFrame is None.\")\n        return None\n\n    print(f\"\\n--- Creating Features (Enhanced Mode: {use_enhanced}) ---\")\n    df_featured = df.copy()\n    all_created_feature_cols = list(df.columns) # Start with existing columns\n\n    # Select config based on flag\n    if use_enhanced:\n        cfg = enhanced_cfg if enhanced_cfg else ENHANCED_LAG_CONFIG\n        env_lags = cfg.get('env_lags', [1])\n        kb_lags = cfg.get('kb_lags', [])\n        kb_rolling_windows = cfg.get('kb_rolling_windows', [])\n        discharge_rolling_window = cfg.get('discharge_rolling_window', None)\n        print(\"Using ENHANCED feature configuration.\")\n    else:\n        cfg = basic_cfg if basic_cfg else BASIC_LAG_CONFIG\n        env_lags = cfg.get('env_lags', [])\n        kb_lags = cfg.get('kb_lags', [])\n        kb_rolling_windows = [] # No rolling features in basic mode\n        discharge_rolling_window = None\n        print(\"Using BASIC feature configuration.\")\n\n    max_lag_needed = 0 # Track the maximum lag introduced\n\n    # --- Lagged K. brevis Features (Raw Counts) ---\n    if target_col in df.columns and kb_lags:\n        print(f\"Creating lagged features for target: {target_col}...\")\n        df[target_col] = pd.to_numeric(df[target_col], errors='coerce')\n        for lag in kb_lags:\n            col_name = f'{target_col}_L{lag}'\n            df_featured[col_name] = df[target_col].shift(lag)\n            all_created_feature_cols.append(col_name)\n            max_lag_needed = max(max_lag_needed, lag)\n        print(f\"  Created KB weekly lags: L{', L'.join(map(str, kb_lags))}\")\n    elif kb_lags:\n        print(f\"Warning: Target column '{target_col}' not found for lagging.\")\n\n    # --- Rolling K. brevis Features (Enhanced Mode Only) ---\n    if use_enhanced and kb_rolling_windows and target_col in df.columns and target_binary_col in df.columns:\n        print(\"Creating rolling aggregate features for KB...\")\n        target_binary_1e5 = df[target_binary_col]\n        monthly_lags_created = []\n        max_roll_lag = 0\n        for i, (start_lag, end_lag) in enumerate(kb_rolling_windows):\n            window_size = end_lag - start_lag + 1\n            month_lag_id = f'M{i+1}'\n            monthly_lags_created.append(month_lag_id)\n            max_roll_lag = max(max_roll_lag, end_lag) # Max lag needed for rolling window\n\n            # Mean of raw kb over the window, shifted back\n            col_name_mean = f'{target_col}_{month_lag_id}_mean'\n            df_featured[col_name_mean] = df[target_col].rolling(window=window_size, min_periods=1).mean().shift(start_lag)\n            all_created_feature_cols.append(col_name_mean)\n\n            # Proportion of bloom weeks (target=1) over the window, shifted back\n            col_name_prop = f'{target_binary_col}_{month_lag_id}_prop'\n            df_featured[col_name_prop] = target_binary_1e5.rolling(window=window_size, min_periods=1).mean().shift(start_lag)\n            all_created_feature_cols.append(col_name_prop)\n        print(f\"  Created approx KB monthly lags (mean, prop_bloom): {', '.join(monthly_lags_created)}\")\n        max_lag_needed = max(max_lag_needed, max_roll_lag) # Rolling needs lookback up to end_lag\n\n    # --- Rolling Discharge Feature (Enhanced Mode Only) ---\n    discharge_col = 'peace_discharge' # Or potentially 'caloos_Q' if prioritized/available\n    if use_enhanced and discharge_rolling_window and discharge_col in df.columns:\n         print(f\"Creating rolling average feature for {discharge_col}...\")\n         window_size = discharge_rolling_window\n         # Typically want average over past month, lagged by 1 week\n         start_lag = 1\n         col_name_discharge_roll = f'{discharge_col}_{window_size}w_avg_L{start_lag}'\n         df_featured[col_name_discharge_roll] = df[discharge_col].rolling(window=window_size, min_periods=1).mean().shift(start_lag)\n         all_created_feature_cols.append(col_name_discharge_roll)\n         max_lag_needed = max(max_lag_needed, window_size + start_lag -1) # Max lookback needed\n         print(f\"  Created {col_name_discharge_roll}\")\n\n\n    # --- Lagged Environmental/Hydrological Features ---\n    if env_lags:\n        print(\"\\nCreating lagged environmental/hydrological features...\")\n        lagged_env_cols_added_names = []\n        max_env_lag = 0\n        for feature in base_features:\n             if feature in df.columns:\n                df[feature] = pd.to_numeric(df[feature], errors='coerce') # Ensure numeric\n                for lag in env_lags:\n                    col_name = f'{feature}_L{lag}'\n                    df_featured[col_name] = df[feature].shift(lag)\n                    all_created_feature_cols.append(col_name)\n                    max_env_lag = max(max_env_lag, lag)\n                lagged_env_cols_added_names.append(feature)\n             else:\n                print(f\"  Warning: Feature '{feature}' not found for lagging.\")\n        if lagged_env_cols_added_names:\n            print(f\"  Lagged features created for: {', '.join(lagged_env_cols_added_names)} using lags L{', L'.join(map(str, env_lags))}\")\n        max_lag_needed = max(max_lag_needed, max_env_lag)\n\n    # --- Drop Rows with NaNs from Lagging/Rolling ---\n    print(f\"\\nMaximum lag/window introduced: {max_lag_needed} weeks.\")\n    initial_rows = len(df_featured)\n    if max_lag_needed > 0:\n        df_processed = df_featured.iloc[max_lag_needed:].copy()\n        print(f\"Dropped first {max_lag_needed} rows due to NaNs generated by feature engineering.\")\n        print(f\"Shape after dropping initial NaNs: {df_processed.shape}\")\n    else:\n        df_processed = df_featured.copy()\n        print(\"No lagging applied or max lag was 0, no initial rows dropped.\")\n\n    # Final check for all-NaN columns (can happen with rolling/shifting if data is sparse)\n    all_nan_cols = df_processed.columns[df_processed.isnull().all()].tolist()\n    if all_nan_cols:\n        print(f\"\\nWarning: The following columns consist entirely of NaNs and will be dropped: {all_nan_cols}\")\n        df_processed = df_processed.dropna(axis=1, how='all')\n        print(f\"Shape after dropping all-NaN columns: {df_processed.shape}\")\n\n    # Ensure no duplicate columns (though unlikely with this structure)\n    df_processed = df_processed.loc[:,~df_processed.columns.duplicated()]\n\n    print(\"\\n--- Feature Engineering Complete ---\")\n    print(f\"Final number of columns (features + targets): {len(df_processed.columns)}\")\n    return df_processed\n\n# --- Execute Feature Engineering ---\n# Ensure df_initial exists from Part 2 and config variables from Part 1\nif __name__ == \"__main__\":\n    try:\n        if 'df_initial' in locals() and df_initial is not None:\n            # Determine which config to use based on the flag from Part 1\n            lag_config_to_use = ENHANCED_LAG_CONFIG if USE_ENHANCED_FEATURES else BASIC_LAG_CONFIG\n            features_to_lag_list = BASE_FEATURES # Modify if external data was added\n\n            df_processed = create_features(\n                df_initial,\n                TARGET_COL,\n                TARGET_BINARY_COL,\n                features_to_lag_list,\n                use_enhanced=USE_ENHANCED_FEATURES,\n                basic_cfg=BASIC_LAG_CONFIG,\n                enhanced_cfg=ENHANCED_LAG_CONFIG\n            )\n\n            if df_processed is not None:\n                print(\"\\nFirst 5 rows of processed data (df_processed):\")\n                display(df_processed.head())\n                print(\"\\nLast 5 rows:\")\n                display(df_processed.tail())\n                print(f\"\\nFinal shape of df_processed: {df_processed.shape}\")\n            else:\n                print(\"\\nFeature engineering failed.\")\n        else:\n            print(\"\\nError: df_initial not found or is None. Please run Part 2 first.\")\n\n    except NameError as ne:\n         print(f\"NameError: A configuration variable might be missing. Did Part 1 run correctly? Error: {ne}\")\n    except Exception as e:\n         print(f\"An unexpected error occurred in main execution block: {e}\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/lstm-enkf#part-3-feature-engineering","position":9},{"hierarchy":{"lvl1":"Outline","lvl3":"Part 4: Data Splitting (Chronological)","lvl2":"Library installation"},"type":"lvl3","url":"/lstm-enkf#part-4-data-splitting-chronological","position":10},{"hierarchy":{"lvl1":"Outline","lvl3":"Part 4: Data Splitting (Chronological)","lvl2":"Library installation"},"content":"This part defines and executes a function to split the feature-engineered DataFrame (df_processed from Part 3) into training, validation, and testing sets based on time. A chronological split is essential for time-series data to ensure the model is trained on past data and evaluated on future data, simulating a real-world forecasting scenario.\n\nThis cell defines the split_data_chronological function and applies it to df_processed. It uses the TRAIN_SPLIT_RATIO and VALIDATION_SPLIT_RATIO defined in Part 1 to divide the data.\n\nExplanation:\n\nFunction split_data_chronological:\n\nTakes the feature-engineered DataFrame (df_processed) and the desired train/validation ratios as input.\n\nIncludes checks to ensure the input DataFrame is valid and has a datetime index.\n\nCalculates the number of samples for each set based on the ratios.\n\nPerforms the split using integer-location based indexing (iloc) which respects the chronological order. .copy() is used to avoid potential SettingWithCopyWarning later.\n\nPrints the size and date range of each resulting subset.\n\nIncludes an assertion to double-check that the splits don’t overlap.\n\nReturns the three DataFrames: train_df, validation_df, test_df.\n\nMain Execution:\n\nCalls the function using df_processed (output of Part 3) and the ratios defined in the configuration (Part 1).\n\nStores the results in train_df, validation_df, and test_df. These variables will be used in subsequent steps (scaling, sequence creation, EnKF).\n\nPrints the head of train_df for verification.\n\nAfter running this cell, you will have the data divided into the necessary subsets for training, validation (tuning/early stopping), and final testing, while preserving the temporal order. The next step (Part 5) will handle imputation and scaling, fitting the necessary objects only on train_df.\n\n# --- Configuration (Ensure these are defined from Part 1 or redefined here) ---\nTRAIN_SPLIT_RATIO = 0.70\nVALIDATION_SPLIT_RATIO = 0.15\n# Test split ratio is implicitly calculated\n\n# --- Function Definition ---\ndef split_data_chronological(df, train_ratio, val_ratio):\n    \"\"\"\n    Splits the DataFrame chronologically into train, validation, and test sets.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame with a datetime index, sorted chronologically.\n        train_ratio (float): The proportion of data to use for training (e.g., 0.7).\n        val_ratio (float): The proportion of data to use for validation (e.g., 0.15).\n\n    Returns:\n        tuple: A tuple containing train_df, validation_df, test_df (pd.DataFrames),\n               or (None, None, None) if splitting fails.\n    \"\"\"\n    if df is None or df.empty:\n        print(\"Error in split_data_chronological: Input DataFrame is None or empty.\")\n        return None, None, None\n    if not isinstance(df.index, pd.DatetimeIndex):\n         print(\"Error: DataFrame index must be a DatetimeIndex.\")\n         return None, None, None\n    if not df.index.is_monotonic_increasing:\n         print(\"Warning: DataFrame index is not sorted chronologically. Sorting now...\")\n         df = df.sort_index()\n\n    print(\"\\n--- Splitting Data into Train, Validation, and Test Sets (Chronological) ---\")\n    n_total = len(df)\n    n_train = int(n_total * train_ratio)\n    n_validation = int(n_total * val_ratio)\n    n_test = n_total - n_train - n_validation\n\n    # Ensure calculated splits are valid\n    if n_train <= 0 or n_validation <= 0 or n_test <= 0:\n         print(f\"Error: Not enough data ({n_total} rows) to create non-empty train/validation/test splits.\")\n         print(f\"Calculated splits: Train={n_train}, Val={n_validation}, Test={n_test}\")\n         return None, None, None\n    if n_train + n_validation + n_test != n_total:\n         print(\"Error: Split ratios do not sum correctly.\")\n         return None, None, None\n\n    # Perform chronological split using iloc\n    train_df = df.iloc[:n_train].copy()\n    validation_df = df.iloc[n_train : n_train + n_validation].copy()\n    test_df = df.iloc[n_train + n_validation :].copy() # Takes the rest\n\n    print(f\"Total samples: {n_total}\")\n    print(f\"Training set:   {len(train_df)} samples (Index: {train_df.index.min()} to {train_df.index.max()})\")\n    print(f\"Validation set: {len(validation_df)} samples (Index: {validation_df.index.min()} to {validation_df.index.max()})\")\n    print(f\"Test set:       {len(test_df)} samples (Index: {test_df.index.min()} to {test_df.index.max()})\")\n\n    # Basic check for overlap\n    assert train_df.index.max() < validation_df.index.min(), \"Train/Validation sets overlap!\"\n    assert validation_df.index.max() < test_df.index.min(), \"Validation/Test sets overlap!\"\n\n    print(\"Data splitting complete.\")\n    return train_df, validation_df, test_df\n\n# --- Execute Data Splitting ---\n# Ensure df_processed exists from Part 3\nif __name__ == \"__main__\":\n    try:\n        if 'df_processed' in locals() and df_processed is not None:\n            # Use ratios defined in Part 1 config\n            train_df, validation_df, test_df = split_data_chronological(\n                df_processed,\n                TRAIN_SPLIT_RATIO,\n                VALIDATION_SPLIT_RATIO\n            )\n\n            if train_df is not None:\n                # Display head of training data as confirmation\n                print(\"\\nHead of Training Data (train_df):\")\n                display(train_df.head())\n                # Keep test_df (unscaled) for later use with EnKF\n                print(\"\\nUnscaled test_df also created for EnKF observations.\")\n            else:\n                print(\"\\nData splitting failed.\")\n        else:\n            print(\"\\nError: df_processed not found or is None. Please run Part 3 first.\")\n            # Define placeholders to prevent errors if run out of order in notebook\n            train_df, validation_df, test_df = None, None, None\n\n    except NameError as ne:\n         print(f\"NameError: A configuration variable might be missing. Did Part 1 run correctly? Error: {ne}\")\n    except Exception as e:\n         print(f\"An unexpected error occurred in main execution block: {e}\")\n         train_df, validation_df, test_df = None, None, None\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/lstm-enkf#part-4-data-splitting-chronological","position":11},{"hierarchy":{"lvl1":"Outline","lvl3":"Part 5: Preprocessing (Imputation & Scaling)","lvl2":"Library installation"},"type":"lvl3","url":"/lstm-enkf#part-5-preprocessing-imputation-scaling","position":12},{"hierarchy":{"lvl1":"Outline","lvl3":"Part 5: Preprocessing (Imputation & Scaling)","lvl2":"Library installation"},"content":"This step takes the chronologically split dataframes (train_df, validation_df, test_df from Part 4) and performs two crucial preprocessing tasks:\n\nImputation: Handles any remaining missing values (NaNs) in the feature columns. It’s vital to fit the imputer only on the training data.\nScaling: Scales the numerical features using the scaler type specified in the configuration (RobustScaler or StandardScaler). The scaler is fit only on the training data.\nThe fitted scaler and the final list of feature columns used are saved.\n\nThis cell defines the preprocess_data function to handle imputation and scaling, fitting the necessary transformers only on the training set. It then applies this function to the split dataframes generated in Part 4.\n\nExplanation of Fixes:\n\nfeature_suffix Definition: The NameError occurred because feature_suffix (which depends on USE_ENHANCED_FEATURES) was used to construct FEATURE_LIST_FILENAME before the main execution block where it would be defined.\n\nSolution: The code now defines feature_suffix inside the if name == “main”: block before calling preprocess_data. Filename templates (scaler_fname_template, flist_fname_template) are passed to the function, and the final filenames are constructed inside the function using the passed feature_suffix.\n\nConfiguration Checks: Added more robust checks at the beginning of the main execution block to ensure all necessary configuration variables and dataframes exist before proceeding.\n\nError Handling: Added more specific error messages and try-except blocks within the preprocess_data function for robustness during imputation and scaling transforms.\n\nClarity: Minor adjustments to print statements for better clarity.\nNow, when you run this corrected Part 5 cell (after ensuring Part 1 and Part 4 have run successfully in your session), it should correctly define feature_suffix, construct the filenames, perform the preprocessing, save the components, and store the results in the train_scaled_df, validation_scaled_df, etc., variables without the NameError.\n\n# --- Part 5: Preprocessing (Imputation & Scaling) ---\n\ndef preprocess_data(train_df, validation_df, test_df, target_col, scaler_type='Robust',\n                    output_dir='output/', scaler_fname_template=None, flist_fname_template=None, feature_suffix='basic'):\n\n    # 1. Validate Inputs\n    if any(df is None or df.empty for df in [train_df, validation_df, test_df]):\n        print(\"Error: One or more input DataFrames are invalid.\")\n        return (None,) * 5\n\n    print(f\"\\n--- Preprocessing Data (Imputation & Scaling: {scaler_type}) ---\")\n\n    # 2. Identify Features (Exclude Target)\n    # We use training data to determine features\n    feature_candidates = [c for c in train_df.columns if c != target_col]\n    final_feature_columns = train_df[feature_candidates].select_dtypes(include=np.number).columns.tolist()\n\n    if not final_feature_columns:\n        print(\"Error: No numeric features found.\")\n        return (None,) * 5\n\n    print(f\"Identified {len(final_feature_columns)} numeric features.\")\n\n    # 3. Initialize Transformers\n    imputer = SimpleImputer(strategy='mean')\n\n    if scaler_type.lower() == 'standard':\n        scaler = StandardScaler()\n    else:\n        scaler = RobustScaler() # Default to Robust\n\n    # 4. Fit Transformers (ONLY on Training Data)\n    train_proc_df = train_df.copy()\n\n    # Imputation Fit\n    impute_needed = train_proc_df[final_feature_columns].isnull().values.any()\n    if impute_needed:\n        print(\"Fitting imputer on training data...\")\n        imputer.fit(train_proc_df[final_feature_columns])\n    else:\n        print(\"No missing values in training set. Skipping imputer fitting.\")\n\n    # Scaling Fit (Fit on imputed training data)\n    # If we imputed, we must transform the temp training data to fit the scaler correctly\n    temp_train_features = imputer.transform(train_proc_df[final_feature_columns]) if impute_needed else train_proc_df[final_feature_columns]\n\n    print(f\"Fitting {scaler_type}Scaler on training data...\")\n    scaler.fit(temp_train_features)\n\n    # 5. Transform All Sets (Loop to remove redundancy)\n    processed_dfs = []\n    datasets = [train_df, validation_df, test_df]\n    set_names = ['Train', 'Validation', 'Test']\n\n    for name, df in zip(set_names, datasets):\n        df_copy = df.copy()\n        try:\n            # Apply Imputation\n            if impute_needed:\n                df_copy[final_feature_columns] = imputer.transform(df_copy[final_feature_columns])\n\n            # Apply Scaling\n            df_copy[final_feature_columns] = scaler.transform(df_copy[final_feature_columns])\n            processed_dfs.append(df_copy)\n        except Exception as e:\n            print(f\"Error processing {name} set: {e}\")\n            return (None,) * 5\n\n    train_scaled, val_scaled, test_scaled = processed_dfs\n    print(\"Transformation complete.\")\n\n    # 6. Save Components\n    try:\n        os.makedirs(output_dir, exist_ok=True)\n        # Use provided templates or defaults\n        s_path = scaler_fname_template.format(scaler_type.lower()) if scaler_fname_template else f\"{output_dir}/scaler.joblib\"\n        f_path = flist_fname_template.format(feature_suffix) if flist_fname_template else f\"{output_dir}/features.joblib\"\n\n        joblib.dump(scaler, s_path)\n        joblib.dump(final_feature_columns, f_path)\n        print(f\"Saved scaler and feature list to {output_dir}\")\n    except Exception as e:\n        print(f\"Warning: Could not save scaler/features: {e}\")\n\n    return train_scaled, val_scaled, test_scaled, scaler, final_feature_columns\n\n# --- Execution Block ---\nif __name__ == \"__main__\":\n    # Assumes train_df, validation_df, test_df exist from Part 4\n    # Assumes config variables like SCALER_TYPE exist from Part 1\n\n    if 'train_df' in locals():\n        train_scaled_df, validation_scaled_df, test_scaled_df, scaler, final_feature_columns_used = preprocess_data(\n            train_df, validation_df, test_df,\n            TARGET_BINARY_COL,\n            scaler_type=SCALER_TYPE,\n            output_dir=OUTPUT_DIR,\n            scaler_fname_template=SCALER_FILENAME.replace(f\"_{SCALER_TYPE.lower()}\", \"_{}\"), # Dynamic template\n            flist_fname_template=FEATURE_LIST_FILENAME.replace(f\"_{feature_suffix}\", \"_{}\"),\n            feature_suffix=feature_suffix\n        )\n\n\n\n","type":"content","url":"/lstm-enkf#part-5-preprocessing-imputation-scaling","position":13},{"hierarchy":{"lvl1":"Outline","lvl3":"Part 6: Sequence Creation","lvl2":"Library installation"},"type":"lvl3","url":"/lstm-enkf#part-6-sequence-creation","position":14},{"hierarchy":{"lvl1":"Outline","lvl3":"Part 6: Sequence Creation","lvl2":"Library installation"},"content":"This step takes the scaled dataframes (train_scaled_df, validation_scaled_df, test_scaled_df) and transforms them into the sequence format required by LSTM/GRU models. Each sequence (X) will contain SEQUENCE_LENGTH consecutive time steps of features, and the corresponding target (y) will be the bloom state FORECAST_HORIZON steps after the end of the sequence.\n\nThis cell defines the create_sequences_from_df function and applies it to the scaled training, validation, and test dataframes generated in Part 5. It uses the SEQUENCE_LENGTH and FORECAST_HORIZON parameters defined in Part 1. The resulting NumPy arrays (X_train, y_train, etc.) are saved to a file.\n\nExplanation:\n\nFunction create_sequences_from_df:\n\nTakes a scaled DataFrame, the list of feature columns to use, the target column name, sequence length, and forecast horizon.\n\nIncludes checks for valid input and sufficient data length.\n\nIterates through the data, creating sequences (X) of length sequence_length and corresponding targets (y) taken forecast_horizon steps after the end of each sequence.\n\nReturns the sequences and targets as NumPy arrays.\n\nMain Execution:\n\nChecks that the necessary scaled DataFrames (train_scaled_df, etc.) and the feature list (final_feature_columns_used) exist from Part 5.\n\nCalls create_sequences_from_df separately for the training, validation, and test sets.\n\nPrints the shapes of the resulting NumPy arrays (X_train, y_train, etc.).\n\nSaves the generated sequences and the feature list used to create them into a single .npz file for easy loading later. The filename dynamically includes the forecast horizon and feature type based on the configuration in Part 1.\n\nAfter running this cell, you will have the sequence arrays (X_train, y_train, X_val, y_val, X_test, y_test) ready for the LSTM model definition (Part 7) and subsequent training/tuning steps.\n\ndef create_sequences_from_df(df, seq_length, target_col_idx=0, pred_step=1, return_targets=True):\n    \"\"\"\n    Creates sequences for LSTM model from a DataFrame.\n    \"\"\"\n    xs = []\n    ys = []\n\n    # Convert dataframe to numpy if needed\n    data = df.values if hasattr(df, 'values') else df\n\n    # Loop through data\n    # We stop earlier to account for the prediction step ahead (pred_step)\n    for i in range(len(data) - seq_length - pred_step + 1):\n        x = data[i:(i + seq_length)]\n        xs.append(x)\n\n        if return_targets:\n            # Target is 'pred_step' steps after the sequence ends\n            y = data[i + seq_length + pred_step - 1, target_col_idx]\n            ys.append(y)\n\n    return np.array(xs), np.array(ys)\n\nif __name__ == \"__main__\":\n    if 'train_scaled_df' in locals() and train_scaled_df is not None:\n\n        # Dictionary to automate the loop\n        data_splits = {\n            'train': train_scaled_df,\n            'val':   validation_scaled_df,\n            'test':  test_scaled_df\n        }\n\n        sequences = {}\n        print(f\"\\n--- Creating Sequences (Horizon: {FORECAST_HORIZON}, Seq Length: {SEQUENCE_LENGTH}) ---\")\n\n        for name, df in data_splits.items():\n            print(f\"Generating {name} sequences...\")\n\n            # --- CRITICAL FIX START ---\n            # 1. Determine which columns to use.\n            # We must ensure the TARGET column is included in the data so we can extract 'y'.\n            if 'final_feature_columns_used' in locals():\n                # Make a copy of the list so we don't modify the original variable\n                cols_to_use = list(final_feature_columns_used)\n\n                # If target is missing from features, add it so we can create 'y'\n                if TARGET_BINARY_COL not in cols_to_use:\n                    print(f\"  Note: Adding '{TARGET_BINARY_COL}' back to dataframe for sequence generation.\")\n                    cols_to_use.append(TARGET_BINARY_COL)\n\n                df_subset = df[cols_to_use]\n            else:\n                df_subset = df\n            # --- CRITICAL FIX END ---\n\n            # 2. Find the integer index of the target column inside this new subset\n            try:\n                target_idx = df_subset.columns.get_loc(TARGET_BINARY_COL)\n            except KeyError:\n                raise KeyError(f\"Target column '{TARGET_BINARY_COL}' not found in the dataframe. Check spelling!\")\n\n            # 3. Call the function\n            X, y = create_sequences_from_df(\n                df_subset,              # The Data (Features + Target)\n                SEQUENCE_LENGTH,        # Window size\n                target_idx,             # Index of the target column\n                FORECAST_HORIZON,       # Prediction step\n                True                    # Return targets?\n            )\n\n            # Store in dictionary\n            sequences[f'X_{name}'] = X\n            sequences[f'y_{name}'] = y\n            print(f\"  {name}: X={X.shape}, y={y.shape}\")\n\n        # Unpack for later parts\n        X_train, y_train = sequences['X_train'], sequences['y_train']\n        X_val, y_val     = sequences['X_val'], sequences['y_val']\n        X_test, y_test   = sequences['X_test'], sequences['y_test']\n\n        # Save to file\n        if X_train.size > 0:\n            suffix = feature_suffix if 'feature_suffix' in locals() else \"\"\n            fname = SEQUENCES_FILENAME_TEMPLATE.format(FORECAST_HORIZON, suffix)\n\n            # Save arrays and the column list for reference\n            np.savez(fname, **sequences, feature_columns=df_subset.columns)\n            print(f\"\\nSaved sequences to {fname}\")\n\n    else:\n        print(\"Skipping Part 6: Scaled data not found.\")\n\n\n\n","type":"content","url":"/lstm-enkf#part-6-sequence-creation","position":15},{"hierarchy":{"lvl1":"Outline","lvl3":"Part 7: Model Definition (LSTM).","lvl2":"Library installation"},"type":"lvl3","url":"/lstm-enkf#part-7-model-definition-lstm","position":16},{"hierarchy":{"lvl1":"Outline","lvl3":"Part 7: Model Definition (LSTM).","lvl2":"Library installation"},"content":"This part defines the function build_lstm_model responsible for creating the LSTM network architecture using TensorFlow/Keras. The function is designed to be flexible: it can use default hyperparameters defined in Part 1, or it can accept a hp object from KerasTuner (which we’ll use later in Part 8) to create models with varying hyperparameters during the tuning process.\n\nThis cell defines the build_lstm_model function. It specifies the layers (LSTM, Dropout, Dense) and allows for hyperparameter configuration either through defaults or a KerasTuner object. It then demonstrates building the model with default hyperparameters, assuming the input shape is known from Part 6.\n\nExplanation:\n\nFunction build_lstm_model:\n\nTakes input_shape (required) and an optional KerasTuner hp object.\n\nHyperparameter Handling: If hp is provided (during tuning), it defines hyperparameters using \n\nhp.Int, hp.Float, hp.Choice. If hp is None (when building the default or final model), it uses the DEFAULT_ variables defined in Part 1 (with fallbacks just in case).\n\nArchitecture: Defines a two-layer LSTM structure with Dropout. You can easily modify this (e.g., change to GRU, add Dense layers, use Bidirectional) by editing this function.\n\nCompilation: Compiles the model inside the function using Adam optimizer and binary cross-entropy loss. This is convenient for KerasTuner.\n\nMain Execution (if name == “main”:):\n\nChecks if X_train exists (from Part 6) to get the required input_shape.\n\nCalls build_lstm_model with hp=None to create an instance using the default hyperparameters.\n\nPrints the model summary.\n\nAfter running this cell, the function build_lstm_model is defined and ready to be used either by KerasTuner (Part 8) or directly for training the default/final model (Part 9). The lstm_model_default variable holds an example compiled model instance (useful for checking).\n\ndef build_lstm_model(input_shape, hp=None):\n    \"\"\"\n    Builds and compiles an LSTM model.\n\n    Args:\n        input_shape (tuple): The shape of the input data (time_steps, features).\n        hp (KerasTuner.HyperParameters, optional): Hyperparameters for tuning.\n                                                   If None, uses defaults.\n    Returns:\n        model: A compiled Keras model.\n    \"\"\"\n\n    # ---------------------------------------------------------\n    # 1. Hyperparameter Definition (Dual Mode)\n    # ---------------------------------------------------------\n    # If 'hp' is provided, we are in Tuning Mode.\n    # If 'hp' is None, we use the global defaults (defined in Part 1).\n\n    if hp:\n        # Tuning Mode: Ask KerasTuner to try different values\n        units_1 = hp.Int('lstm_units_1', min_value=32, max_value=128, step=32)\n        units_2 = hp.Int('lstm_units_2', min_value=16, max_value=64, step=16)\n        dropout_rate = hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1)\n        learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n    else:\n        # Default Mode: Use variables from Part 1 (with safety fallbacks)\n        units_1 = globals().get('DEFAULT_LSTM_UNITS', 64)\n        units_2 = globals().get('DEFAULT_LSTM_UNITS', 32) # Using same default or half\n        dropout_rate = globals().get('DEFAULT_DROPOUT', 0.2)\n        learning_rate = globals().get('DEFAULT_LEARNING_RATE', 0.001)\n\n    # ---------------------------------------------------------\n    # 2. Model Architecture\n    # ---------------------------------------------------------\n    model = Sequential()\n\n    # Input Layer\n    model.add(Input(shape=input_shape))\n\n    # LSTM Layer 1 (Must return sequences to feed the next LSTM layer)\n    model.add(LSTM(units=units_1, return_sequences=True))\n    model.add(Dropout(dropout_rate))\n\n    # LSTM Layer 2 (return_sequences=False because next is Dense)\n    model.add(LSTM(units=units_2, return_sequences=False))\n    model.add(Dropout(dropout_rate))\n\n    # Output Layer (1 unit for Binary Classification: Bloom vs No Bloom)\n    model.add(Dense(1, activation='sigmoid'))\n\n    # ---------------------------------------------------------\n    # 3. Compilation\n    # ---------------------------------------------------------\n    model.compile(\n        optimizer=Adam(learning_rate=learning_rate),\n        loss='binary_crossentropy',\n        metrics=[BinaryAccuracy(name='accuracy'), AUC(name='auc'), Recall(name='recall'),Precision(name='precision')]\n    )\n\n    return model\n\n# --- Main Execution (Test Block) ---\nif __name__ == \"__main__\":\n    print(\"--- Testing Model Definition (Step 7) ---\")\n\n    # We need X_train to know the input shape\n    if 'X_train' in locals() and X_train is not None:\n\n        # Determine shape: (Sequence Length, Number of Features)\n        # X_train shape is typically (Samples, Time Steps, Features)\n        # We need the last two dimensions for input_shape\n        input_shape_test = (X_train.shape[1], X_train.shape[2])\n        print(f\"Input Shape detected: {input_shape_test}\")\n\n        # Build a \"Default\" model (hp=None) to verify it works\n        try:\n            model_test = build_lstm_model(input_shape_test, hp=None)\n            print(\"\\nModel built successfully!\")\n            model_test.summary()\n        except Exception as e:\n            print(f\"Error building model: {e}\")\n\n    else:\n        print(\"Warning: X_train not found. Run Step 6 first to test this function.\")\n        print(\"Function 'build_lstm_model' is defined but not tested.\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/lstm-enkf#part-7-model-definition-lstm","position":17},{"hierarchy":{"lvl1":"Outline","lvl3":"Part 8: Hyperparameter Tuning with KerasTuner (Conditional)","lvl2":"Library installation"},"type":"lvl3","url":"/lstm-enkf#part-8-hyperparameter-tuning-with-kerastuner-conditional","position":18},{"hierarchy":{"lvl1":"Outline","lvl3":"Part 8: Hyperparameter Tuning with KerasTuner (Conditional)","lvl2":"Library installation"},"content":"This cell sets up and runs the KerasTuner search process if the PERFORM_TUNING flag (from Part 1) is set to True. It uses the build_lstm_model function (from Part 7) as the hypermodel builder and searches for the best combination of LSTM units, dropout rate, and learning rate based on validation accuracy.\n\nExplanation:\n\nConditional Execution: The entire cell’s logic is wrapped in if can_tune:, which checks if PERFORM_TUNING was set to True in Part 1 and if the keras_tuner library (kt) was successfully imported. If not, it prints a message and skips tuning.\n\nPrerequisite Checks: Inside the if block, it checks if the necessary sequence data (X_train, y_train, etc.) and the build_lstm_model function exist before proceeding.\n\nTuner Setup:\n\nCreates a keras_tuner.RandomSearch instance (you could switch to kt.Hyperband for potentially faster convergence).\n\nPasses a lambda function lambda hp: build_lstm_model(input_shape_tune, hp=hp) as the hypermodel builder.\n\nThis ensures the build_lstm_model function receives the tuner’s hp object to define the model architecture with tunable parameters.\n\nSets the objective to ‘val_accuracy’ (tune for best accuracy on the validation set).\n\nConfigures max_trials, directory, project_name, etc.\n\nRun Search:\n\nCalls \n\ntuner.search(), passing the training and validation data.\n\nUses a dedicated EarlyStopping callback with potentially shorter patience for the tuning phase itself.\n\nRetrieve Best Hyperparameters:\n\nAfter the search completes, tuner.get_best_hyperparameters(num_trials=1)[0] retrieves the HyperParameters object corresponding to the best trial.\n\nThe values for the tuned hyperparameters (e.g., lstm_units_1, dropout_rate, learning_rate) are extracted and printed.\n\nThe best_hps variable stores this object for use in the next step (Part 9).\nIncludes error handling in case the tuner fails or doesn’t return results.\nIf PERFORM_TUNING is False, this cell will simply print a message and set best_hps to None. The next step (Part 9) will then know to use the default hyperparameters defined in Part 1.\n\n# 1. Initialize Variables\nbest_hps = None\nbest_model_from_tuner = None\n\n# 2. Check Prerequisites\n# We need X_train (Data) and build_lstm_model (Function)\ncan_tune = True\n\nif 'X_train' not in locals() or X_train is None:\n    print(\"Error: X_train not found. Please run Step 6 (Data Generation) first.\")\n    can_tune = False\n\nif 'build_lstm_model' not in locals():\n    print(\"Error: build_lstm_model not found. Please run Step 7 (Model Definition) first.\")\n    can_tune = False\n\nif 'PERFORM_TUNING' not in locals() or not PERFORM_TUNING:\n    print(\"Skipping Tuning: PERFORM_TUNING flag is False or missing.\")\n    can_tune = False\n\n# 3. Run Tuner\nif can_tune:\n    print(\"\\n--- Setting up KerasTuner ---\")\n\n    # Define Input Shape for the builder\n    # Shape = (Time Steps, Features) -> (X_train.shape[1], X_train.shape[2])\n    input_shape_tune = (X_train.shape[1], X_train.shape[2])\n    print(f\"Tuning Input Shape: {input_shape_tune}\")\n\n    # Initialize the RandomSearch Tuner\n    # Note: We use objective='val_auc' to match the name='auc' in Step 7\n    tuner = kt.RandomSearch(\n        lambda hp: build_lstm_model(input_shape_tune, hp=hp),\n        objective=kt.Objective(\"val_auc\", direction=\"max\"),\n        max_trials=TUNER_MAX_TRIALS,       # Defined in Step 1 (e.g., 10 or 20)\n        executions_per_trial=2,            # Run each trial twice to reduce luck\n        directory=TUNER_PROJECT_DIR,       # \"hab_tuning\"\n        project_name=TUNER_PROJECT_NAME,   # \"bloom_prediction\"\n        overwrite=True                     # Start fresh every time\n    )\n\n    tuner.search_space_summary()\n\n    print(\"\\n--- Starting Search ---\")\n\n    # Early Stopping strictly for the tuning phase (speed things up)\n    tuner_early_stopping = EarlyStopping(monitor='val_loss', patience=4, verbose=0)\n\n    try:\n        # CRITICAL FIX: Pass Numpy arrays directly.\n        # Do not convert to tf.Tensor manually here.\n        tuner.search(\n            X_train, y_train,\n            epochs=TUNER_EPOCHS,           # Defined in Step 1 (e.g., 10 or 20)\n            batch_size=TUNER_BATCH_SIZE,   # Defined in Step 1 (e.g., 32)\n            validation_data=(X_val, y_val),\n            callbacks=[tuner_early_stopping],\n            verbose=1\n        )\n\n        print(\"\\n--- Hyperparameter Search Finished ---\")\n\n        # 4. Retrieve Best Results\n        best_hps_list = tuner.get_best_hyperparameters(num_trials=1)\n\n        if best_hps_list:\n            best_hps = best_hps_list[0]\n            print(\"Best Hyperparameters Found:\")\n            print(f\"  LSTM Units 1:  {best_hps.values.get('lstm_units_1')}\")\n            print(f\"  LSTM Units 2:  {best_hps.values.get('lstm_units_2')}\")\n            print(f\"  Dropout Rate:  {best_hps.values.get('dropout_rate')}\")\n            print(f\"  Learning Rate: {best_hps.values.get('learning_rate')}\")\n\n            # (Optional) You can build the best model immediately if you want\n            # best_model = tuner.hypermodel.build(best_hps)\n        else:\n            print(\"Warning: Tuner finished but returned no hyperparameters.\")\n            best_hps = None\n\n    except Exception as e:\n        print(f\"CRITICAL ERROR during tuning: {e}\")\n        best_hps = None\n\nelse:\n    print(\"Tuner skipped.\")\n\n\n\n","type":"content","url":"/lstm-enkf#part-8-hyperparameter-tuning-with-kerastuner-conditional","position":19},{"hierarchy":{"lvl1":"Outline","lvl3":"Part 9: Model Training","lvl2":"Library installation"},"type":"lvl3","url":"/lstm-enkf#part-9-model-training","position":20},{"hierarchy":{"lvl1":"Outline","lvl3":"Part 9: Model Training","lvl2":"Library installation"},"content":"This cell defines the main training function and executes it. It uses the configuration flags from Part 1 to determine whether to use class weights and whether to use default or tuned hyperparameters (from best_hps variable potentially created in Part 8). It saves the best model weights found during training.\n\nExplanation:\n\nFunction train_model:\n\nTakes training/validation data, input shape, configuration flags (use_class_weight), optional tuned hyperparameters (best_hps), training parameters (epochs, batch_size, patience), and paths/names for saving.\n\nBuilds Model: Calls build_lstm_model (from Part 7), passing best_hps if it’s available (meaning tuning was done in Part 8), otherwise hp=None is passed and the function uses defaults.\n\nClass Weights: If use_class_weight is True, it calculates the weights using compute_class_weight on y_train.\n\nCompiles: Ensures the model is compiled with the correct learning rate (either default or from best_hps).\n\nCallbacks: Sets up EarlyStopping and ModelCheckpoint. Note that ModelCheckpoint now saves the entire model in the .keras format (recommended over .h5), as we are not using the custom subclassed model in this part. The filename includes the model_type_name.\n\nFits Model: Calls \n\nmodel.fit(), passing the class_weight dictionary if applicable.\n\nLoads Best: After training, it explicitly loads the best model saved by ModelCheckpoint to ensure the returned model represents the best validation performance.\n\nReturns: The trained model object and the training history.\n\nMain Execution:\n\nChecks that necessary data (X_train, etc.) and configuration flags exist.\n\nDetermines the model_type_name based on whether tuning was performed and class weights are used (e.g., “baseline”, “baseline_weighted”, “tuned”, “tuned_weighted”).\n\nCalls the train_model function with the appropriate arguments.\n\nStores the returned trained model in the trained_model variable.\n\nPlots the training/validation loss and accuracy from the returned history object.\n\nSets a general model_ready_for_eval flag for the next step (Part 10: Baseline Evaluation).\n\nAfter running this cell, the trained_model variable will hold the trained LSTM model (either using default or tuned hyperparameters, and potentially trained with class weights), ready for evaluation on the test set in Part 10. The best version of this model is also saved to a file.\n\ndef train_final_model(x_train, y_train, x_val, y_val,\n                      input_shape,\n                      use_class_weight=False,\n                      best_hps=None,\n                      epochs=50,\n                      batch_size=32,\n                      patience=10,\n                      checkpoint_path_template='output/model_{}.keras',\n                      model_name='baseline'):\n\n    print(f\"\\n--- Training Final Model: {model_name} ---\")\n\n    # 1. Build Model\n    # We assume build_lstm_model is available from Step 7\n    if 'build_lstm_model' not in globals():\n        raise NameError(\"build_lstm_model function not defined. Please run Step 7.\")\n\n    model = build_lstm_model(input_shape, hp=best_hps)\n    if model is None:\n        return None, None\n\n    # 2. Calculate Class Weights (Critical for Blooms)\n    class_weights_dict = None\n    if use_class_weight:\n        print(\"Calculating class weights for imbalanced data...\")\n        try:\n            # Flatten to ensure 1D array for weight calculation\n            y_flat = y_train.flatten().astype(int)\n            classes = np.unique(y_flat)\n            weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_flat)\n            class_weights_dict = dict(zip(classes, weights))\n            print(f\"  Class weights: {class_weights_dict}\")\n        except Exception as e:\n            print(f\"  Error calculating weights: {e}. Using default.\")\n\n    # 3. Compile Model with BETTER METRICS\n    # We add AUC, Precision, and Recall to see if it actually finds blooms\n    learning_rate = best_hps.get('learning_rate') if best_hps else DEFAULT_LEARNING_RATE\n\n    # Re-compile to ensure metrics and optimizer are fresh\n    model.compile(\n        optimizer=Adam(learning_rate=learning_rate),\n        loss=BinaryCrossentropy(),\n        metrics=[\n            BinaryAccuracy(name='accuracy'),\n            AUC(name='auc'),           # <--- Critical for Imbalanced Data\n            Precision(name='precision'),\n            Recall(name='recall')\n        ]\n    )\n    print(f\"Model compiled (LR={learning_rate}). Monitoring AUC/Precision/Recall.\")\n\n    # 4. Setup Callbacks\n    model_path = checkpoint_path_template.format(model_name)\n\n    # EarlyStopping: restore_best_weights=True ensures 'model' var is perfect at the end\n    early_stopping = EarlyStopping(\n        monitor='val_loss',\n        patience=patience,\n        verbose=1,\n        restore_best_weights=True\n    )\n\n    model_checkpoint = ModelCheckpoint(\n        filepath=model_path,\n        monitor='val_loss',\n        save_best_only=True,\n        verbose=1\n    )\n\n    # 5. Train\n    print(f\"\\nStarting training for {epochs} epochs...\")\n\n    # Ensure raw numpy arrays (safer than Tensors here)\n    x_train = np.array(x_train).astype('float32')\n    y_train = np.array(y_train).astype('float32')\n    x_val = np.array(x_val).astype('float32')\n    y_val = np.array(y_val).astype('float32')\n\n    history = model.fit(\n        x_train, y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        validation_data=(x_val, y_val),\n        callbacks=[early_stopping, model_checkpoint],\n        class_weight=class_weights_dict,\n        verbose=1\n    )\n\n    print(f\"\\n--- Training Finished. Best model saved to {model_path} ---\")\n\n    # Because restore_best_weights=True, 'model' is already the best version.\n    return model, history\n\n\n# --- Execute Training ---\nif __name__ == \"__main__\":\n    try:\n        # Check prerequisites\n        if 'X_train' not in locals(): raise NameError(\"Run Step 6 (Sequence Creation) first.\")\n\n        # Define Input Shape\n        input_shape_train = (X_train.shape[1], X_train.shape[2])\n\n        # Naming logic\n        model_name = \"tuned\" if (PERFORM_TUNING and best_hps) else \"baseline\"\n        if USE_CLASS_WEIGHT: model_name += \"_weighted\"\n\n        # TRAIN\n        trained_model, training_history = train_final_model(\n            X_train, y_train, X_val, y_val,\n            input_shape=input_shape_train,\n            use_class_weight=USE_CLASS_WEIGHT,\n            best_hps=best_hps if PERFORM_TUNING else None,\n            epochs=TRAIN_EPOCHS,\n            batch_size=TRAIN_BATCH_SIZE,\n            patience=TRAIN_PATIENCE,\n            checkpoint_path_template=MODEL_CHECKPOINT_TEMPLATE,\n            model_name=model_name\n        )\n\n        if trained_model:\n            # --- PUBLICATION QUALITY PLOTTING ---\n            print(f\"\\nGeneratng publication-quality plots for {model_name}...\")\n\n            import matplotlib.pyplot as plt\n            import os  # <--- Need this to create folders\n\n            # 1. CRITICAL FIX: Create the output directory if it doesn't exist\n            os.makedirs('output', exist_ok=True)\n\n            # 2. Global Style Settings for Academic Papers\n            plt.rcdefaults()\n            params = {\n                'font.family': 'serif',\n                'axes.titlesize': 18,\n                'axes.labelsize': 16,\n                'xtick.labelsize': 14,\n                'ytick.labelsize': 14,\n                'legend.fontsize': 14,\n                'figure.figsize': (10, 6),\n                'lines.linewidth': 2.5\n            }\n            plt.rcParams.update(params)\n\n            # --- FIGURE 1: LOSS CURVE ---\n            plt.figure(dpi=300)\n\n            plt.plot(training_history.history['loss'], label='Training Loss', color='#1f77b4', linestyle='-')\n            plt.plot(training_history.history['val_loss'], label='Validation Loss', color='#d62728', linestyle='--')\n\n            plt.title('Model Loss over Epochs')\n            plt.xlabel('Epoch')\n            plt.ylabel('Binary Crossentropy Loss')\n            plt.legend(frameon=True, fancybox=False, edgecolor='black')\n            plt.grid(True, linestyle=':', alpha=0.6)\n            plt.tight_layout()\n\n            # Save safely now that folder exists\n            plt.savefig(f\"output/plot_loss_{model_name}.png\", bbox_inches='tight')\n            plt.show()\n            print(f\"Saved: output/plot_loss_{model_name}.png\")\n\n            # --- FIGURE 2: AUC / PERFORMANCE CURVE ---\n            plt.figure(dpi=300)\n\n            if 'val_auc' in training_history.history:\n                metric_name = 'AUC'\n                train_data = training_history.history['auc']\n                val_data = training_history.history['val_auc']\n                color_train = '#2ca02c'\n                color_val = '#ff7f0e'\n            else:\n                metric_name = 'Accuracy'\n                train_data = training_history.history['accuracy']\n                val_data = training_history.history['val_accuracy']\n                color_train = 'black'\n                color_val = 'gray'\n\n            plt.plot(train_data, label=f'Training {metric_name}', color=color_train, linestyle='-')\n            plt.plot(val_data, label=f'Validation {metric_name}', color=color_val, linestyle='--')\n\n            plt.title(f'Model {metric_name} Performance')\n            plt.xlabel('Epoch')\n            plt.ylabel(f'{metric_name} Score')\n            plt.legend(frameon=True, fancybox=False, edgecolor='black')\n            plt.grid(True, linestyle=':', alpha=0.6)\n            plt.tight_layout()\n\n            plt.savefig(f\"output/plot_{metric_name.lower()}_{model_name}.png\", bbox_inches='tight')\n            plt.show()\n            print(f\"Saved: output/plot_{metric_name.lower()}_{model_name}.png\")\n\n            model_ready_for_eval = True\n        else:\n            model_ready_for_eval = False\n\n    except Exception as e:\n         print(f\"Error: {e}\")\n         model_ready_for_eval = False\n\n\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/lstm-enkf#part-9-model-training","position":21},{"hierarchy":{"lvl1":"Outline","lvl3":"Part 10: Baseline Evaluation","lvl2":"Library installation"},"type":"lvl3","url":"/lstm-enkf#part-10-baseline-evaluation","position":22},{"hierarchy":{"lvl1":"Outline","lvl3":"Part 10: Baseline Evaluation","lvl2":"Library installation"},"content":"This cell defines the evaluation function and applies it to the model trained in Part 9 (trained_model). It calculates and prints various classification metrics, plots the confusion matrix and predictions over time, and assesses the model’s ability to predict bloom onsets.\n\nExplanation:\n\nFunction evaluate_model:\n\nTakes the model object, test data (x_test, y_test), the unscaled test dataframe test_df_unscaled (needed for correct timestamps), sequence length, forecast horizon, and a model_name string as input.\n\nPredictions: Runs model.predict(x_test) to get probabilities. Converts probabilities to class predictions using a 0.5 threshold.\n\nMetrics Calculation: Uses scikit-learn functions to calculate accuracy, precision, recall, F1-score (for the positive class ‘1’), AUC-ROC (using probabilities), and Brier score (using probabilities). Includes error handling for AUC calculation if only one class is present in y_test.\n\nReporting: Prints a formatted summary of the metrics and the full classification report.\\\n\nConfusion Matrix: Calculates and plots the confusion matrix using seaborn for better visualization.\n\nTime Series Plot: Determines the correct start index in the test_df_unscaled index based on sequence length and forecast horizon to align the predictions (y_pred_prob) with the actual dates. Plots actuals vs. predicted probabilities. Includes checks for length mismatches.\n\nOnset Analysis: Implements the logic to find actual bloom onsets (0 -> 1 transitions in y_test) and checks if the model predicted a bloom (class 1) either in the week before the onset or during the onset week. Reports the hit rate.\n\nMain Execution:\n\nChecks if the necessary variables (trained_model, X_test, y_test, test_df, etc.) exist from previous steps.\n\nCalls the evaluate_model function, passing the required arguments. Uses the model_type_trained variable (set at the end of Part 9) to label the output correctly.\n\nAfter running this cell, you will have a comprehensive evaluation of the model trained in Part 9, including standard metrics, plots, and the crucial bloom onset performance assessment. This provides the baseline against which the EnKF-enhanced model (Part 12/13) will be compared.\n\ndef evaluate_model(model, x_test, y_test, test_df_unscaled,\n                   seq_len, forecast_horizon, model_name=\"Model\"):\n    \"\"\"\n    Evaluates model with Optimal Threshold detection and Publication Plots.\n    Saves figures to 'output/' folder.\n    \"\"\"\n    if model is None or x_test is None or y_test is None:\n        print(\"Error: Missing model or test data.\")\n        return\n\n    # Ensure output directory exists\n    os.makedirs('output', exist_ok=True)\n\n    print(f\"\\n=== Evaluating Model: {model_name} ===\")\n\n    # 1. Generate Probabilities\n    print(\"1. Generating predictions...\")\n    try:\n        # Get raw probabilities (0.0 to 1.0)\n        y_pred_prob = model.predict(x_test, verbose=0)\n        y_test_eval = y_test.reshape(-1, 1) # Ensure 2D shape\n    except Exception as e:\n        print(f\"Error during prediction: {e}\")\n        return\n\n    # 2. Find Optimal Threshold (Fixed Math)\n    # ------------------------------------------------------\n    precision, recall, thresholds = precision_recall_curve(y_test_eval, y_pred_prob)\n\n    # Calculate F-Score safely (avoid 0/0 division)\n    numerator = 2 * precision * recall\n    denominator = precision + recall\n    with np.errstate(divide='ignore', invalid='ignore'):\n        fscore = np.divide(numerator, denominator)\n    fscore = np.nan_to_num(fscore) # Replace NaNs with 0\n\n    # Locate the index of the largest F score\n    ix = np.argmax(fscore)\n\n    # Safety check for index bounds\n    if ix >= len(thresholds):\n        best_thresh = thresholds[-1]\n    else:\n        best_thresh = thresholds[ix]\n\n    print(f\"\\nOptimal Decision Threshold: {best_thresh:.4f} (Max F1: {fscore[ix]:.4f})\")\n    # ------------------------------------------------------\n\n    # Apply Best Threshold\n    y_pred_class = (y_pred_prob >= best_thresh).astype(int)\n\n    # 3. Calculate Metrics (at Best Threshold)\n    print(\"\\n2. Performance Metrics (at Optimal Threshold):\")\n    try:\n        acc = accuracy_score(y_test_eval, y_pred_class)\n        prec = precision_score(y_test_eval, y_pred_class, zero_division=0)\n        rec = recall_score(y_test_eval, y_pred_class, zero_division=0)\n        f1 = f1_score(y_test_eval, y_pred_class, zero_division=0)\n        auc = roc_auc_score(y_test_eval, y_pred_prob)\n        brier = brier_score_loss(y_test_eval, y_pred_prob)\n\n        print(f\"  Accuracy:    {acc:.4f}\")\n        print(f\"  Precision:   {prec:.4f}\")\n        print(f\"  Recall:      {rec:.4f}\")\n        print(f\"  F1-Score:    {f1:.4f}\")\n        print(f\"  AUC-ROC:     {auc:.4f}\")\n        print(f\"  Brier Score: {brier:.4f}\")\n\n        print(f\"\\nClassification Report:\\n{classification_report(y_test_eval, y_pred_class, target_names=['No Bloom', 'Bloom'])}\")\n\n    except Exception as e:\n        print(f\"Metric calculation error: {e}\")\n\n    # 4. Publication-Quality Plots\n    # ----------------------------\n    # Apply Academic Style\n    plt.rcdefaults()\n    params = {'font.family': 'serif', 'figure.figsize': (8, 6), 'figure.dpi': 300,\n              'axes.labelsize': 14, 'axes.titlesize': 16, 'xtick.labelsize': 12, 'ytick.labelsize': 12}\n    plt.rcParams.update(params)\n\n    # --- Plot A: Confusion Matrix ---\n    try:\n        cm = confusion_matrix(y_test_eval, y_pred_class)\n        plt.figure()\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n                    xticklabels=['Pred: No Bloom', 'Pred: Bloom'],\n                    yticklabels=['Actual: No Bloom', 'Actual: Bloom'],\n                    annot_kws={\"size\": 14})\n        plt.title(f'Confusion Matrix ({model_name})\\nThreshold: {best_thresh:.2f}')\n        plt.tight_layout()\n\n        # SAVE FIGURE\n        save_path = f\"output/confusion_matrix_{model_name}.png\"\n        plt.savefig(save_path, bbox_inches='tight')\n        print(f\"Saved: {save_path}\")\n        plt.show()\n    except Exception as e:\n        print(f\"CM Plot Error: {e}\")\n\n    # --- Plot B: Time Series Predictions ---\n    print(f\"\\n3. Plotting Time Series Predictions...\")\n    start_idx = seq_len + forecast_horizon - 1\n    num_preds = len(y_pred_prob)\n\n    if start_idx + num_preds <= len(test_df_unscaled):\n        dates = test_df_unscaled.index[start_idx : start_idx + num_preds]\n\n        plt.figure(figsize=(12, 6))\n        # Plot Actuals (Black Line)\n        plt.plot(dates, y_test_eval[:num_preds], label='Actual Bloom', color='black', alpha=0.6, linewidth=1.5)\n        # Plot Probabilities (Red Line)\n        plt.plot(dates, y_pred_prob[:num_preds], label='Predicted Probability', color='#d62728', alpha=0.8, linewidth=1.5)\n        # Threshold Line\n        plt.axhline(best_thresh, color='gray', linestyle='--', label=f'Threshold ({best_thresh:.2f})')\n\n        plt.title(f'Bloom Predictions vs Actuals ({forecast_horizon}-Step Horizon)')\n        plt.ylabel('Bloom Probability')\n        plt.xlabel('Date')\n        plt.legend(frameon=True, fancybox=False, edgecolor='black', loc='upper right')\n        plt.grid(True, linestyle=':', alpha=0.5)\n        plt.tight_layout()\n\n        # SAVE FIGURE\n        save_path = f\"output/timeseries_preds_{model_name}.png\"\n        plt.savefig(save_path, bbox_inches='tight')\n        print(f\"Saved: {save_path}\")\n        plt.show()\n    else:\n        print(\"Warning: Could not align dates for plotting.\")\n\n    # 5. Bloom Onset Analysis (Custom Logic)\n    print(f\"\\n4. Bloom Onset Analysis (Did we catch the start?)\")\n    y_test_flat = y_test_eval.flatten()\n    y_pred_flat = y_pred_class.flatten()\n\n    onsets = np.where((y_test_flat[:-1] == 0) & (y_test_flat[1:] == 1))[0] + 1\n    total_onsets = len(onsets)\n\n    hits = 0\n    late_hits = 0\n\n    if total_onsets > 0:\n        for idx in onsets:\n            # Check window: [Week Before, Week Of, Week After]\n            pred_before = y_pred_flat[idx-1] if idx > 0 else 0\n            pred_on     = y_pred_flat[idx]\n            pred_after  = y_pred_flat[idx+1] if idx+1 < len(y_pred_flat) else 0\n\n            if pred_before == 1 or pred_on == 1:\n                hits += 1 # Success (Early or On Time)\n            elif pred_after == 1:\n                late_hits += 1 # Late by 1 week\n\n        print(f\"  Total Onsets: {total_onsets}\")\n        print(f\"  Caught Early/On-Time: {hits} ({hits/total_onsets:.1%})\")\n        print(f\"  Caught 1-Week Late:   {late_hits}\")\n        print(f\"  Missed Completely:    {total_onsets - hits - late_hits}\")\n    else:\n        print(\"  No bloom onsets found in Test Data.\")\n\n# --- Main Execution ---\nif __name__ == \"__main__\":\n    if 'trained_model' in locals() and 'X_test' in locals():\n        # Ensure data is numpy float32\n        X_test = np.array(X_test, dtype='float32')\n        y_test = np.array(y_test, dtype='float32')\n\n        evaluate_model(\n            model=trained_model,\n            x_test=X_test,\n            y_test=y_test,\n            test_df_unscaled=test_df, # Must be defined in Step 4\n            seq_len=SEQUENCE_LENGTH,\n            forecast_horizon=FORECAST_HORIZON,\n            model_name=\"LSTM_Baseline\"\n        )\n    else:\n        print(\"Error: Prerequisites (trained_model, X_test) not found.\")\n\n\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/lstm-enkf#part-10-baseline-evaluation","position":23},{"hierarchy":{"lvl1":"Outline","lvl3":"Part 11: EnKF Setup","lvl2":"Library installation"},"type":"lvl3","url":"/lstm-enkf#part-11-enkf-setup","position":24},{"hierarchy":{"lvl1":"Outline","lvl3":"Part 11: EnKF Setup","lvl2":"Library installation"},"content":"This cell defines the EnsembleKalmanFilter class (assuming it’s not in a separate enkf.py file for simplicity here, but importing is better practice) and a function setup_enkf to initialize the filter for the test period. It calculates the initial state, noise matrices (based on the ENKF_NOISE_ESTIMATION setting from Part 1), and identifies the necessary column indices.\n\nExplanation:\n\nEnKF Class: Includes the EnsembleKalmanFilter class definition directly in the cell for convenience (assuming it’s not imported from enkf.py).\n\nsetup_enkf Function:\n\nTakes necessary configurations and dataframes as input.\n\nInitial State/Covariance: Extracts the state vector values (x_initial) from the unscaled test_df at the correct time index (end of the first sequence’s lookback period). Calculates an initial covariance matrix P_initial based on relative uncertainty assumptions (these ratios are tunable).\n\nNoise Matrices (R, Q): Calculates the observation noise R and process noise Q based on the noise_estimation_method.\n\nIf ‘manual’, it uses MANUAL_R_DIAG and MANUAL_Q_DIAG (defined in Part 1).\n\nIf ‘basic_stats’, it calculates variances based on the mean (for R) and standard deviation of weekly changes (for Q) from the unscaled train_df, scaled by tunable percentage factors. Includes fallback logic for constant columns.\n\nState Indices: Finds the numerical index corresponding to each ENKF_STATE_VARS within the final_feature_columns_used list (which defines the order in the scaled data and LSTM input). This is crucial for correctly updating the LSTM input later.\n\nInitialization: Creates an instance of the EnsembleKalmanFilter class using the calculated x_initial, P_initial, and other parameters. Stores the calculated R and Q matrices as attributes of the instance for easy access in the next step.\n\nReturns: The initialized enkf object and the list of enkf_state_indices.\n\nMain Execution:\n\nChecks for prerequisite variables from previous steps.\n\nCalls setup_enkf with the appropriate arguments from the configuration (Part 1) and dataframes (Parts 4, 5).\n\nStores the returned enkf_instance and enkf_indices for use in the forecasting loop (Part 12).\n\nAfter running this cell, the enkf_instance is ready, configured with initial conditions and noise characteristics appropriate for the start of the test period. The enkf_indices list tells us which columns in the LSTM input sequence correspond to the variables being updated by the EnKF.\n\nclass EnsembleKalmanFilter:\n    \"\"\"\n    Stochastic Ensemble Kalman Filter (Burgers et al., 1998).\n    Includes observation perturbation to maintain ensemble variance.\n    \"\"\"\n    def __init__(self, x_init, P_init, dim_z, N):\n        \"\"\"\n        Args:\n            x_init: Initial state mean (dim_x,)\n            P_init: Initial state covariance (dim_x, dim_x)\n            dim_z:  Dimension of observations\n            N:      Number of ensemble members\n        \"\"\"\n        self.dim_x = len(x_init)\n        self.dim_z = dim_z\n        self.N = N\n        self.x = None # Current state mean\n        self.P = None # Current state covariance\n\n        # Initialize Ensemble\n        # We enforce symmetry and positive-definiteness on P_init\n        P_init = (P_init + P_init.T) / 2\n        self.ensemble = self._multivariate_normal(x_init, P_init, N)\n\n        # Calculate initial stats\n        self.x = np.mean(self.ensemble, axis=0)\n        self.P = np.cov(self.ensemble.T)\n\n        print(f\"EnKF Initialized. Members: {N}, State Dim: {self.dim_x}\")\n\n    def predict(self, F_func, Q, dt=1):\n        \"\"\"\n        Propagate state forward: x = F(x) + Noise\n        \"\"\"\n        # 1. Apply Model Dynamics (The \"Physics\" or \"AI\")\n        # We apply F_func to every member of the ensemble\n        self.ensemble = np.apply_along_axis(F_func, 1, self.ensemble)\n\n        # 2. Add Process Noise (System Error)\n        if Q is not None:\n            Q = (Q + Q.T) / 2\n            noise = self._multivariate_normal(np.zeros(self.dim_x), Q, self.N)\n            self.ensemble += noise\n\n        # Update stats\n        self.x = np.mean(self.ensemble, axis=0)\n        self.P = np.cov(self.ensemble.T)\n\n    def update(self, z, R, H=None):\n        \"\"\"\n        Assimilate observation: x = x + K(z - Hx)\n        \"\"\"\n        if H is None: H = np.eye(self.dim_z, self.dim_x)\n\n        # 1. Project Ensemble to Observation Space\n        # Shape: (N, dim_z)\n        Hx = self.ensemble @ H.T\n\n        # 2. Perturb Observations (CRITICAL for Stochastic EnKF)\n        # We treat the observation as a random variable, not a single truth\n        R = (R + R.T) / 2\n        obs_noise = self._multivariate_normal(np.zeros(self.dim_z), R, self.N)\n        z_perturbed = z + obs_noise\n\n        # 3. Calculate Innovation (Residual)\n        # Difference between \"noisy measurement\" and \"predicted measurement\"\n        D = z_perturbed - Hx\n\n        # 4. Calculate Kalman Gain (K)\n        # P_zz = H P H' + R\n        # K = P H' (P_zz)^-1\n        P_prior = np.cov(self.ensemble.T)\n        P_zz = H @ P_prior @ H.T + R\n\n        # Use Pseudo-Inverse for stability\n        K = P_prior @ H.T @ np.linalg.pinv(P_zz)\n\n        # 5. Update Ensemble\n        # x_new = x_old + K * Innovation\n        self.ensemble = self.ensemble + (D @ K.T)\n\n        # Update stats\n        self.x = np.mean(self.ensemble, axis=0)\n        self.P = np.cov(self.ensemble.T)\n\n        return self.x, self.P\n\n    def _multivariate_normal(self, mean, cov, size):\n        \"\"\"Helper to sample safely, adding jitter if matrix is singular.\"\"\"\n        try:\n            return np.random.multivariate_normal(mean, cov, size)\n        except np.linalg.LinAlgError:\n            # Add small \"jitter\" to diagonal to fix numerical instability\n            print(\"Warning: Matrix not positive definite. Adding jitter.\")\n            epsilon = 1e-6 * np.eye(len(mean))\n            return np.random.multivariate_normal(mean, cov + epsilon, size)\n\n\n# --- 2. Setup Function (Robust with Manual Overrides) ---\ndef setup_enkf(enkf_state_vars, n_enkf, train_df, test_df_unscaled,\n               seq_len, feature_columns, manual_r_diag=None, manual_q_diag=None):\n    \"\"\"\n    Configures the EnKF.\n    Allows manual override of Q (Process) and R (Observation) noise diagonals.\n    \"\"\"\n    print(\"\\n--- Setting up EnKF ---\")\n\n    # Validation\n    if not all(col in test_df_unscaled.columns for col in enkf_state_vars):\n        print(f\"Error: Missing state variables in test data.\")\n        return None, None, None, None\n\n    dim_x = len(enkf_state_vars)\n\n    # A. Initial State (x0)\n    start_idx = seq_len - 1\n    x_init = test_df_unscaled[enkf_state_vars].iloc[start_idx].values.astype(float)\n    print(f\"Initial State Vector (t=0): {x_init}\")\n\n    # B. Initial Covariance (P0)\n    P_init = np.diag((np.abs(x_init) * 0.30) ** 2) + 1e-6 * np.eye(dim_x)\n\n    # C. Noise Matrices (Q and R)\n\n    # --- R MATRIX (Observation Noise) ---\n    if manual_r_diag is not None:\n        if len(manual_r_diag) != dim_x:\n            print(f\"Error: Manual R diag length ({len(manual_r_diag)}) does not match state vars ({dim_x})\")\n            return None, None, None, None\n        print(\"-> Using MANUAL R Matrix values.\")\n        R_matrix = np.diag(manual_r_diag)\n    else:\n        print(\"-> Calculating Automatic R Matrix (1% of mean).\")\n        r_diags = []\n        for col in enkf_state_vars:\n            mean_val = train_df[col].mean()\n            r_diags.append((abs(mean_val) * 0.01) ** 2)\n        R_matrix = np.diag(r_diags)\n\n    # --- Q MATRIX (Process Noise) ---\n    if manual_q_diag is not None:\n        if len(manual_q_diag) != dim_x:\n            print(f\"Error: Manual Q diag length ({len(manual_q_diag)}) does not match state vars ({dim_x})\")\n            return None, None, None, None\n        print(\"-> Using MANUAL Q Matrix values.\")\n        Q_matrix = np.diag(manual_q_diag)\n    else:\n        print(\"-> Calculating Automatic Q Matrix (based on volatility).\")\n        q_diags = []\n        for col in enkf_state_vars:\n            volatility = train_df[col].diff().std()\n            if pd.isna(volatility) or volatility == 0: volatility = 1e-3\n            q_diags.append((volatility * 2.0) ** 2)\n        Q_matrix = np.diag(q_diags)\n\n    print(\"Noise Matrices Finalized:\")\n    print(f\"  R (Obs Noise) diag: {np.diag(R_matrix)}\")\n    print(f\"  Q (Proc Noise) diag: {np.diag(Q_matrix)}\")\n\n    # D. Feature Indices\n    try:\n        enkf_indices = [feature_columns.index(var) for var in enkf_state_vars]\n    except ValueError as e:\n        print(f\"Error mapping variables to features: {e}\")\n        return None, None, None, None\n\n    # E. Create Instance\n    enkf = EnsembleKalmanFilter(x_init, P_init, dim_z=dim_x, N=n_enkf)\n    enkf.R = R_matrix\n    enkf.Q = Q_matrix\n\n    return enkf, enkf_indices, R_matrix, Q_matrix\n\n# --- 3. Execute ---\nif __name__ == \"__main__\":\n    try:\n        # Configuration check\n        REQUIRED = ['ENKF_STATE_VARS', 'N_ENKF', 'train_df', 'test_df',\n                    'SEQUENCE_LENGTH', 'final_feature_columns_used']\n\n        for var in REQUIRED:\n            if var not in locals(): raise NameError(f\"{var} is missing.\")\n\n\n        if (ENKF_NOISE_ESTIMATION == 'manual'):\n          MANUAL_Q = MANUAL_Q_DIAG\n          MANUAL_R = MANUAL_R_DIAG\n        else:\n          MANUAL_Q = None\n          MANUAL_R = None\n\n        enkf_instance, enkf_indices, R_matrix_enkf, Q_matrix_enkf = setup_enkf(\n            enkf_state_vars=ENKF_STATE_VARS,\n            n_enkf=N_ENKF,\n            train_df=train_df,\n            test_df_unscaled=test_df,\n            seq_len=SEQUENCE_LENGTH,\n            feature_columns=final_feature_columns_used,\n            manual_r_diag=MANUAL_R,\n            manual_q_diag=MANUAL_Q\n        )\n\n        if enkf_instance:\n            print(\"\\nEnKF Setup Complete.\")\n\n    except Exception as e:\n        print(f\"EnKF Setup Failed: {e}\")\n\nif 'Q_matrix_enkf' in locals() and 'R_matrix_enkf' in locals() and 'ENKF_STATE_VARS' in locals():\n\n    print(\"\\nGenerating publication-quality heatmaps for Q and R matrices...\")\n\n    # --- STYLE SETTINGS (Academic Standard) ---\n    plt.rcdefaults()\n    params = {\n        'font.family': 'serif',\n        'font.serif': ['Times New Roman', 'Times', 'DejaVu Serif'],\n        'axes.titlesize': 18,\n        'axes.labelsize': 14,\n        'xtick.labelsize': 12,\n        'ytick.labelsize': 12,\n        'figure.dpi': 300\n    }\n    plt.rcParams.update(params)\n\n    # Helper function to plot and save\n    def plot_covariance_matrix(matrix, title, filename, labels, cmap='Blues'):\n        plt.figure(figsize=(8, 7))\n\n        sns.heatmap(matrix, annot=True, fmt='.2e', cmap=cmap,\n                    xticklabels=labels, yticklabels=labels,\n                    square=True, cbar_kws={'label': 'Variance / Covariance'},\n                    linewidths=1, linecolor='black')\n\n        plt.title(title, pad=20)\n        plt.xticks(rotation=45, ha='right')\n        plt.yticks(rotation=0)\n        plt.tight_layout()\n\n        save_path = f\"output/{filename}\"\n        plt.savefig(save_path, bbox_inches='tight')\n        print(f\"Saved: {save_path}\")\n        plt.show()\n\n    # --- 1. Plot Q Matrix (Process Noise) ---\n    plot_covariance_matrix(\n        Q_matrix_enkf,\n        title=\"Process Noise Covariance ($Q$)\",\n        filename=\"matrix_Q_heatmap.png\",\n        labels=ENKF_STATE_VARS,\n        cmap=\"Reds\"\n    )\n\n    # --- 2. Plot R Matrix (Observation Noise) ---\n    plot_covariance_matrix(\n        R_matrix_enkf,\n        title=\"Observation Noise Covariance ($R$)\",\n        filename=\"matrix_R_heatmap.png\",\n        labels=ENKF_STATE_VARS,\n        cmap=\"Blues\"\n    )\n\nelse:\n    print(\"Error: Q and R matrices (or state vars) not found. Run Step 11 first.\")\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/lstm-enkf#part-11-enkf-setup","position":25},{"hierarchy":{"lvl1":"Outline","lvl3":"Part 12: EnKF + MC Dropout Forecasting Loop","lvl2":"Library installation"},"type":"lvl3","url":"/lstm-enkf#part-12-enkf-mc-dropout-forecasting-loop","position":26},{"hierarchy":{"lvl1":"Outline","lvl3":"Part 12: EnKF + MC Dropout Forecasting Loop","lvl2":"Library installation"},"content":"This section implements the core forecasting engine, which synthesizes the data assimilation capabilities of the Ensemble Kalman Filter (EnKF) with the predictive power and uncertainty estimation of the LSTM. The loop iterates through the test period, sequentially updating environmental states and generating probabilistic forecasts.","type":"content","url":"/lstm-enkf#part-12-enkf-mc-dropout-forecasting-loop","position":27},{"hierarchy":{"lvl1":"Outline","lvl4":"Functionality: run_enkf_mc_forecast","lvl3":"Part 12: EnKF + MC Dropout Forecasting Loop","lvl2":"Library installation"},"type":"lvl4","url":"/lstm-enkf#functionality-run-enkf-mc-forecast","position":28},{"hierarchy":{"lvl1":"Outline","lvl4":"Functionality: run_enkf_mc_forecast","lvl3":"Part 12: EnKF + MC Dropout Forecasting Loop","lvl2":"Library installation"},"content":"The forecasting simulation is encapsulated in the run_enkf_mc_forecast function, which manages the interaction between real-time observations and the deep learning model.\n\nState Assimilation (EnKF): For each time step, the function retrieves the current observations from the unscaled test data. If perform_enkf is enabled, the EnKF instance performs a prediction and update step. This process corrects the hydrological state variables (such as river discharge and nutrient levels) by balancing the model’s internal state with actual observed data, effectively reducing the “drift” often found in long-term time-series forecasting.\n\nInput Sequence Modification: Once the EnKF provides an updated state, these values are passed through the fitted scaler. The function then takes the standard input sequence from X_test and replaces the state variables in the most recent time step with these assimilated values. This ensures the LSTM makes its next prediction based on the most accurate, ground-truth-adjusted data available.\n\nProbabilistic Prediction (MC Dropout):\n\nUncertainty Quantification: If perform_mc is active, the model performs multiple forward passes (defined by n_mc_samples) for the same input sequence. By keeping dropout layers active during inference (training=True), each pass produces a slightly different prediction, allowing us to capture the model’s predictive uncertainty.\n\nStandard Inference: If MC Dropout is disabled, the model performs a standard predict() call, providing a single deterministic output.","type":"content","url":"/lstm-enkf#functionality-run-enkf-mc-forecast","position":29},{"hierarchy":{"lvl1":"Outline","lvl4":"Execution and Robustness","lvl3":"Part 12: EnKF + MC Dropout Forecasting Loop","lvl2":"Library installation"},"type":"lvl4","url":"/lstm-enkf#execution-and-robustness","position":30},{"hierarchy":{"lvl1":"Outline","lvl4":"Execution and Robustness","lvl3":"Part 12: EnKF + MC Dropout Forecasting Loop","lvl2":"Library installation"},"content":"The execution block verifies that all prerequisites—including the trained model, initialized EnKF, and noise matrices—are present before starting the loop.\n\nError Handling: The loop is wrapped in try-except blocks to manage potential issues during matrix inversions in the EnKF or scaling errors. If a critical failure occurs, the loop captures the error and returns the results generated up to that point.\n\nResult Trimming: To maintain data integrity, both the prediction results and the corresponding ground truth labels (y_test) are trimmed to the exact number of successfully completed iterations.\n\nUpon completion, the results are stored in mc_predictions_results. These raw probability outputs (either as single points or ensembles) serve as the foundation for the performance metrics and uncertainty analysis performed in Part 13: EnKF + MC Dropout Evaluation.\n\ndef run_enkf_mc_forecast(model, enkf_instance, enkf_indices, R_matrix, Q_matrix,\n                         x_test, y_test, test_df_unscaled, scaler,\n                         seq_len, perform_enkf=True, perform_mc=True, n_mc_samples=50):\n    \"\"\"\n    Optimized Forecasting Loop with Vectorized MC Dropout.\n    \"\"\"\n    # 1. Validation & Setup\n    if model is None or x_test is None: return None, None\n    print(f\"\\n--- Starting Forecast (EnKF={perform_enkf}, MC={perform_mc}) ---\")\n\n    num_steps = len(y_test)\n    n_features = x_test.shape[2]\n\n    # Pre-allocate output array\n    # If MC is off, we still use shape (N, 1) for consistency\n    samples_col = n_mc_samples if perform_mc else 1\n    predictions = np.zeros((num_steps, samples_col))\n\n    # 2. Pre-fetch Scaler Parameters (Speed Optimization)\n    # Accessing scaler attributes inside the loop is slow. Do it once here.\n    if perform_enkf:\n        try:\n            # Handle StandardScaler vs RobustScaler\n            if hasattr(scaler, 'mean_'):\n                mu, sigma = scaler.mean_, scaler.scale_\n            elif hasattr(scaler, 'center_'):\n                mu, sigma = scaler.center_, scaler.scale_\n            else:\n                print(\"Error: Scaler not fitted.\"); return None, None\n\n            # Keep only the stats for the EnKF variables to avoid indexing in loop\n            enkf_mu = mu[enkf_indices]\n            enkf_sigma = sigma[enkf_indices]\n        except Exception as e:\n            print(f\"Scaler Error: {e}\"); return None, None\n\n    # 3. Main Loop\n    # EnKF must be sequential (step t depends on step t-1)\n    for i in tqdm(range(num_steps), desc=\"Forecasting\"):\n\n        # A. Current Timestamp Index\n        # The test_df index corresponding to the *end* of the current sequence\n        df_idx = i + seq_len - 1\n        if df_idx >= len(test_df_unscaled): break\n\n        # B. EnKF Update Step\n        updated_state = None\n        if perform_enkf:\n            # Get Observation (z)\n            z = test_df_unscaled[ENKF_STATE_VARS].iloc[df_idx].values\n\n            # 1. Predict (Move Ensemble Forward)\n            # Note: In a full Hybrid EnKF, the 'F' function would run the LSTM here.\n            # Here we use the persistence assumption for the state transition.\n            enkf_instance.predict(lambda x: x, Q=Q_matrix)\n\n            # 2. Update (Correct with Observation)\n            updated_state, _ = enkf_instance.update(z, R_matrix)\n\n        # C. Update LSTM Input\n        # We copy the sequence so we don't overwrite the original data\n        current_seq = x_test[i].copy() # Shape: (Seq_Len, Features)\n\n        if perform_enkf and updated_state is not None:\n            # Scale the updated state (Vectorized Math)\n            # (Raw - Mean) / Scale\n            scaled_state = (updated_state - enkf_mu) / enkf_sigma\n\n            # Inject into the LAST time step of the sequence\n            # We replace only the columns corresponding to EnKF variables\n            current_seq[-1, enkf_indices] = scaled_state\n\n        # D. LSTM Prediction (Vectorized MC Dropout)\n        # Prepare Tensor\n        # Shape: (1, Seq_Len, Features)\n        input_tensor = tf.convert_to_tensor([current_seq], dtype=tf.float32)\n\n        if perform_mc:\n            # OPTIMIZATION: Tile the input N times to create a batch\n            # New Shape: (n_mc_samples, Seq_Len, Features)\n            batch = tf.tile(input_tensor, [n_mc_samples, 1, 1])\n\n            # ONE single call to the model for all samples\n            # training=True enables Dropout\n            preds = model(batch, training=True)\n\n            # Store results (flatten to 1D array of probabilities)\n            predictions[i, :] = preds[:, 0].numpy()\n        else:\n            # Standard Inference (No Dropout)\n            pred = model(input_tensor, training=False)\n            predictions[i, 0] = pred[0, 0].numpy()\n\n    return predictions, y_test[:len(predictions)]\n\n# --- Execute ---\nif __name__ == \"__main__\":\n    if 'trained_model' in locals() and 'enkf_instance' in locals():\n        if PERFORM_ENKF and enkf_instance is None:\n            print(\"Warning: PERFORM_ENKF is True, but 'enkf_instance' not found.\")\n            print(\"Running in pure LSTM mode (EnKF disabled for this run).\")\n            # Temporarily disable for this function call to prevent crash\n            current_perform_enkf = False\n        else:\n            current_perform_enkf = PERFORM_ENKF\n\n        mc_preds, y_test_trimmed = run_enkf_mc_forecast(\n            model=trained_model,\n            enkf_instance=enkf_instance,\n            enkf_indices=enkf_indices,\n            R_matrix=R_matrix_enkf,\n            Q_matrix=Q_matrix_enkf,\n            x_test=X_test,\n            y_test=y_test,\n            test_df_unscaled=test_df,\n            scaler=scaler, # Ensure this matches your scaler variable name\n            seq_len=SEQUENCE_LENGTH,\n            perform_enkf=True,         # Set to False to test Baseline again\n            perform_mc=True,           # Set to True for Uncertainty\n            n_mc_samples=50            # 50 is a good balance for speed/accuracy\n        )\n\n        if mc_preds is not None:\n            print(f\"✅ Forecast Complete. Prediction Shape: {mc_preds.shape}\")\n    else:\n        print(\"Error: Prerequisites (Part 9 Model or Part 11 EnKF) missing.\")\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/lstm-enkf#execution-and-robustness","position":31},{"hierarchy":{"lvl1":"Outline","lvl3":"Part 13: EnKF + MC Dropout Evaluation","lvl2":"Library installation"},"type":"lvl3","url":"/lstm-enkf#part-13-enkf-mc-dropout-evaluation","position":32},{"hierarchy":{"lvl1":"Outline","lvl3":"Part 13: EnKF + MC Dropout Evaluation","lvl2":"Library installation"},"content":"This final analytical section focuses on quantifying the performance of the integrated forecasting system. By evaluating the predictions generated in Part 12, we assess how the combination of Ensemble Kalman Filter (EnKF) state updates and Monte Carlo (MC) Dropout uncertainty estimation improves the model’s ability to predict red tide events.","type":"content","url":"/lstm-enkf#part-13-enkf-mc-dropout-evaluation","position":33},{"hierarchy":{"lvl1":"Outline","lvl4":"Core Evaluation Components","lvl3":"Part 13: EnKF + MC Dropout Evaluation","lvl2":"Library installation"},"type":"lvl4","url":"/lstm-enkf#core-evaluation-components","position":34},{"hierarchy":{"lvl1":"Outline","lvl4":"Core Evaluation Components","lvl3":"Part 13: EnKF + MC Dropout Evaluation","lvl2":"Library installation"},"content":"The evaluation framework is designed to move beyond simple point-accuracy and instead provide a comprehensive view of model reliability and confidence.\n\nEnsemble Statistics and Uncertainty:\nFor sessions where PERFORM_MC_DROPOUT was enabled, the evaluation calculates the mean and standard deviation across all MC samples for each time step. The mean serves as the primary probabilistic forecast, while the standard deviation provides a quantifiable measure of model uncertainty. This allows us to identify periods where the model is highly confident versus periods where environmental volatility leads to wider prediction intervals.\n\nBinary Classification Performance:\nSince the ultimate goal is to predict bloom occurrences, the probabilistic outputs are converted into binary classifications using an optimized threshold. Key performance metrics include:\n\nPrecision and Recall: Determining the model’s ability to correctly identify blooms while minimizing false alarms.\n\nF1-Score: Providing a balanced metric that accounts for the inherent class imbalance in red tide data.\n\nConfusion Matrix: Visualizing the distribution of True Positives, True Negatives, False Positives, and False Negatives.\n\nProbabilistic Analysis:\nThe evaluation includes the calculation of the Brier Score to measure the accuracy of the probability forecasts and the Area Under the Precision-Recall Curve (AUPRC). These metrics are particularly useful for evaluating models where the event of interest (the bloom) is relatively rare compared to non-bloom periods.","type":"content","url":"/lstm-enkf#core-evaluation-components","position":35},{"hierarchy":{"lvl1":"Outline","lvl4":"Visualizing the Results","lvl3":"Part 13: EnKF + MC Dropout Evaluation","lvl2":"Library installation"},"type":"lvl4","url":"/lstm-enkf#visualizing-the-results","position":36},{"hierarchy":{"lvl1":"Outline","lvl4":"Visualizing the Results","lvl3":"Part 13: EnKF + MC Dropout Evaluation","lvl2":"Library installation"},"content":"The section generates a series of diagnostic plots to illustrate the system’s effectiveness:\n\nForecast vs. Actuals: A time-series plot comparing the predicted probabilities (with uncertainty bands) against the actual observed bloom markers.\n\nEnKF Impact Analysis: Comparative visualizations showing how the state assimilation influenced the trajectory of the predictions compared to a baseline model without EnKF updates.\n\nBy the end of this evaluation, we can determine the specific “value-add” of the EnKF + MC Dropout approach, providing a clear picture of how well the hybrid system handles the complexities of ecological forecasting.\n\nfrom sklearn.calibration import calibration_curve\ndef evaluate_mc_predictions(mc_preds, y_test, test_df_unscaled, seq_len,\n                            forecast_horizon, model_name=\"EnKF_MC\",\n                            output_dir=\"output\", manual_threshold=None):\n    \"\"\"\n    Evaluates probabilistic predictions (Mean, Uncertainty, Calibration).\n\n    Args:\n        mc_preds: Array of shape (n_samples, n_mc_iterations) or (n_samples, 1)\n        y_test: True labels (n_samples,)\n        test_df_unscaled: DataFrame with timestamps\n        seq_len: Input sequence length (for aligning dates)\n        forecast_horizon: Forecast horizon (for aligning dates)\n        model_name: String for labeling files/plots\n        output_dir: Folder to save results\n        manual_threshold: Float (0.0-1.0) to force a specific decision boundary.\n                          If None, calculates optimal F1 threshold.\n    \"\"\"\n    print(f\"\\n=== Evaluating Uncertainty: {model_name} ===\")\n\n    # 1. Setup Output Directory\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Validation\n    if mc_preds is None or y_test is None: return None\n\n    # 2. Statistics Calculation\n    # -------------------------------------------------------\n    # Mean: The \"Best Guess\"\n    y_mean = np.mean(mc_preds, axis=1)\n\n    # Std Dev: The \"Disagreement\" (Model Uncertainty)\n    y_std = np.std(mc_preds, axis=1)\n\n    # 95% Confidence Interval\n    ci_lower = np.percentile(mc_preds, 2.5, axis=1)\n    ci_upper = np.percentile(mc_preds, 97.5, axis=1)\n\n    # Entropy (Uncertainty)\n    epsilon = 1e-10\n    y_mean_clipped = np.clip(y_mean, epsilon, 1-epsilon)\n    entropy = -(y_mean_clipped * np.log(y_mean_clipped) + (1-y_mean_clipped) * np.log(1-y_mean_clipped))\n\n    # 3. Threshold Logic\n    # -------------------------------------------------------\n    if manual_threshold is not None:\n        best_thresh = manual_threshold\n        print(f\"Using Manual Decision Threshold: {best_thresh:.4f}\")\n    else:\n        # Calculate Optimal F1 Threshold\n        precision, recall, thresholds = precision_recall_curve(y_test, y_mean)\n        numerator = 2 * precision * recall\n        denominator = precision + recall\n        with np.errstate(divide='ignore', invalid='ignore'):\n            fscore = np.nan_to_num(np.divide(numerator, denominator))\n\n        ix = np.argmax(fscore)\n        # Safety check if index is out of bounds\n        best_thresh = thresholds[ix] if ix < len(thresholds) else 0.5\n        print(f\"Optimal Threshold (Auto-F1): {best_thresh:.4f} (Max F1: {fscore[ix]:.4f})\")\n\n    # Apply threshold\n    y_pred_class = (y_mean >= best_thresh).astype(int)\n\n    # 4. Calculate Metrics\n    # -------------------------------------------------------\n    try:\n        metrics = {\n            'accuracy': float(accuracy_score(y_test, y_pred_class)),\n            'auc': float(roc_auc_score(y_test, y_mean)),\n            'brier': float(brier_score_loss(y_test, y_mean)),\n            'precision': float(precision_score(y_test, y_pred_class, zero_division=0)),\n            'recall': float(recall_score(y_test, y_pred_class, zero_division=0)),\n            'f1_score': float(f1_score(y_test, y_pred_class, zero_division=0)),\n            'avg_entropy': float(np.mean(entropy)),\n            'threshold_used': float(best_thresh)\n        }\n\n        print(f\"\\nPerformance Metrics:\")\n        print(f\"  Accuracy:    {metrics['accuracy']:.4f}\")\n        print(f\"  AUC-ROC:     {metrics['auc']:.4f}\")\n        print(f\"  Recall:      {metrics['recall']:.4f} (Caught {metrics['recall']*100:.1f}% of blooms)\")\n        print(f\"  Precision:   {metrics['precision']:.4f}\")\n        print(f\"  Brier Score: {metrics['brier']:.4f}\")\n    except Exception as e:\n        print(f\"Error calculating metrics: {e}\")\n        metrics = {}\n\n    # 5. PLOTTING (Publication Quality)\n    # -------------------------------------------------------\n    # Apply Style\n    plt.rcdefaults()\n    params = {'font.family': 'serif', 'figure.figsize': (10, 6), 'figure.dpi': 300}\n    plt.rcParams.update(params)\n\n    # Plot A: Forecast Time Series\n    start_idx = seq_len + forecast_horizon - 1\n    num_preds = len(y_mean)\n\n    if start_idx + num_preds <= len(test_df_unscaled):\n        dates = test_df_unscaled.index[start_idx : start_idx + num_preds]\n\n        plt.figure()\n        # Plot Actuals\n        plt.plot(dates, y_test[:num_preds], color='black', alpha=0.6, label='Actual Bloom', linewidth=1.5)\n        # Plot Mean Prediction\n        plt.plot(dates, y_mean[:num_preds], color='#d62728', label='Ensemble Mean', linewidth=2)\n        # Plot Confidence Interval\n        plt.fill_between(dates, ci_lower[:num_preds], ci_upper[:num_preds], color='#d62728', alpha=0.2, label='95% CI')\n        # Plot Threshold\n        plt.axhline(best_thresh, color='gray', linestyle='--', label=f'Threshold ({best_thresh:.2f})')\n\n        plt.title(f'Probabilistic Forecast ({model_name})')\n        plt.ylabel('Bloom Probability')\n        plt.legend(loc='upper right', frameon=True)\n        plt.grid(True, linestyle=':', alpha=0.5)\n        plt.tight_layout()\n\n        save_path = os.path.join(output_dir, f\"forecast_{model_name}.png\")\n        plt.savefig(save_path)\n        print(f\"Saved Plot: {save_path}\")\n        plt.show()\n\n    # Plot B: Calibration Curve\n    try:\n        prob_true, prob_pred = calibration_curve(y_test, y_mean, n_bins=10)\n        plt.figure(figsize=(6, 6))\n        plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly Calibrated')\n        plt.plot(prob_pred, prob_true, marker='o', color='blue', label=model_name)\n        plt.title('Calibration Curve (Reliability)')\n        plt.xlabel('Mean Predicted Probability')\n        plt.ylabel('Fraction of Positives (Actual)')\n        plt.legend()\n        plt.grid(True, linestyle=':', alpha=0.5)\n        plt.tight_layout()\n\n        save_path = os.path.join(output_dir, f\"calibration_{model_name}.png\")\n        plt.savefig(save_path)\n        print(f\"Saved Plot: {save_path}\")\n        plt.show()\n    except Exception as e:\n        print(f\"Skipping Calibration Plot: {e}\")\n\n    # Plot C: Entropy Distribution\n    try:\n        correct = (y_pred_class == y_test)\n        plt.figure(figsize=(8, 5))\n        sns.kdeplot(entropy[correct], fill=True, color='green', label='Correct Predictions')\n        sns.kdeplot(entropy[~correct], fill=True, color='red', label='Wrong Predictions')\n        plt.title('Predictive Entropy (Uncertainty Distribution)')\n        plt.xlabel('Entropy (Higher = More Uncertain)')\n        plt.legend()\n        plt.tight_layout()\n\n        save_path = os.path.join(output_dir, f\"entropy_{model_name}.png\")\n        plt.savefig(save_path)\n        print(f\"Saved Plot: {save_path}\")\n        plt.show()\n    except Exception as e:\n        print(f\"Skipping Entropy Plot: {e}\")\n\n    # 6. Return structured data\n    return {\n        'metrics': metrics,  # Clean dictionary for JSON\n        'arrays': {          # Arrays for CSV/Debug\n            'y_mean': y_mean,\n            'y_std': y_std,\n            'entropy': entropy,\n            'y_actual': y_test,\n            'ci_lower': ci_lower,\n            'ci_upper': ci_upper\n        }\n    }\n\n# --- Execute ---\nif __name__ == \"__main__\":\n    if 'mc_preds' in locals() and mc_preds is not None:\n\n        # 1. Run Evaluation (Try manual_threshold=0.3 if you want to fix lag)\n        results = evaluate_mc_predictions(\n            mc_preds=mc_preds,\n            y_test=y_test_trimmed,\n            test_df_unscaled=test_df,\n            seq_len=SEQUENCE_LENGTH,\n            forecast_horizon=FORECAST_HORIZON,\n            model_name=\"EnKF_LSTM_MC\",\n            output_dir=\"output\",\n            manual_threshold=None  # <--- Change to 0.3 to reduce lag!\n        )\n\n        # 2. Save Metrics to JSON\n        if results:\n            json_path = os.path.join(\"output\", \"metrics_enkf_mc.json\")\n            try:\n                with open(json_path, 'w') as f:\n                    json.dump(results['metrics'], f, indent=4)\n                print(f\"✅ Saved Metrics to: {json_path}\")\n            except Exception as e:\n                print(f\"Error saving JSON: {e}\")\n\n            # 3. Save Raw Data to CSV\n            csv_path = os.path.join(\"output\", \"predictions_enkf_mc.csv\")\n            try:\n                df_results = pd.DataFrame(results['arrays'])\n                # Add timestamp index if possible\n                start_idx = SEQUENCE_LENGTH + FORECAST_HORIZON - 1\n                if start_idx + len(df_results) <= len(test_df):\n                    df_results.index = test_df.index[start_idx : start_idx + len(df_results)]\n\n                df_results.to_csv(csv_path)\n                print(f\"✅ Saved Raw Data to: {csv_path}\")\n            except Exception as e:\n                print(f\"Error saving CSV: {e}\")\n\n    else:\n        print(\"Error: 'mc_preds' not found. Run Step 12 first.\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/lstm-enkf#visualizing-the-results","position":37},{"hierarchy":{"lvl1":"Outline","lvl3":"Part 14: SHAP Analysis","lvl2":"Library installation"},"type":"lvl3","url":"/lstm-enkf#part-14-shap-analysis","position":38},{"hierarchy":{"lvl1":"Outline","lvl3":"Part 14: SHAP Analysis","lvl2":"Library installation"},"content":"This section utilizes SHAP (SHapley Additive exPlanations) to interpret the LSTM model’s decision-making process. While deep learning models are often viewed as “black boxes,” SHAP values provide transparency by quantifying the contribution of each environmental feature to the final prediction.","type":"content","url":"/lstm-enkf#part-14-shap-analysis","position":39},{"hierarchy":{"lvl1":"Outline","lvl4":"Core Interpretability Components","lvl3":"Part 14: SHAP Analysis","lvl2":"Library installation"},"type":"lvl4","url":"/lstm-enkf#core-interpretability-components","position":40},{"hierarchy":{"lvl1":"Outline","lvl4":"Core Interpretability Components","lvl3":"Part 14: SHAP Analysis","lvl2":"Library installation"},"content":"The analysis focuses on local and global feature importance to understand which drivers—such as nutrient concentrations, river discharge, or lagged bloom states—are most influential in triggering a red tide forecast.\n\nDeepExplainer for LSTM:\nWe employ shap.DeepExplainer to handle the complex, 3-dimensional temporal data characteristic of LSTMs. This approach approximates SHAP values by comparing the model’s output for specific test sequences against a “background” dataset (typically a representative subset of the training data). This reveals how specific fluctuations in environmental conditions shift the probability of a bloom.\n\nTemporal Feature Contribution:\nSince the LSTM processes sequences over time, SHAP analysis allows us to visualize which time steps within the input window are most critical. We can identify whether the model is reacting to immediate spikes in river discharge or if it is recognizing long-term patterns in nutrient accumulation across the entire sequence length.\n\nGlobal Feature Importance:\nBy aggregating the absolute SHAP values across the entire test set, we generate a global ranking of features. This is vital for ecological validation, as it allows us to verify if the model’s “reasoning” aligns with known biological drivers of Karenia brevis, such as specific salinity levels or nitrogen-to-phosphorus ratios.","type":"content","url":"/lstm-enkf#core-interpretability-components","position":41},{"hierarchy":{"lvl1":"Outline","lvl4":"Visualization and Insights","lvl3":"Part 14: SHAP Analysis","lvl2":"Library installation"},"type":"lvl4","url":"/lstm-enkf#visualization-and-insights","position":42},{"hierarchy":{"lvl1":"Outline","lvl4":"Visualization and Insights","lvl3":"Part 14: SHAP Analysis","lvl2":"Library installation"},"content":"The section produces high-impact visualizations to communicate these findings:\n\nSHAP Summary Plots: A comprehensive view showing the magnitude and direction of each feature’s impact. For instance, it might reveal that high values of a specific lagged nutrient feature consistently push the model toward a “bloom” prediction.\n\nForce Plots / Waterfall Plots: Detailed breakdowns of individual prediction samples, showing how different features “pushed” the model’s output from the base value to the final predicted probability.\n\nBy integrating SHAP analysis, we transition from simply knowing what the model predicted to understanding why it made that prediction, providing essential context for environmental managers and researchers.\n\ntry:\n    # 1. Check & Fix Data Dimensions\n    # ---------------------------------------------------------\n    if 'X_test' not in locals() or 'trained_model' not in locals():\n        raise NameError(\"Data or Model missing. Please restart kernel and run Steps 1-9.\")\n\n    true_n_steps = X_test.shape[1]    # e.g., 12\n    true_n_features = X_test.shape[2] # e.g., 26\n\n    print(f\"Data Shape: {true_n_steps} steps, {true_n_features} features\")\n\n    # Fix the feature name list if it doesn't match the data\n    current_names = final_feature_columns_used if 'final_feature_columns_used' in locals() else []\n\n    if len(current_names) != true_n_features:\n        print(f\"Mismatch: {len(current_names)} names vs {true_n_features} data features.\")\n        diff = true_n_features - len(current_names)\n        # We assume the extra columns are usually appended at the end\n        plotting_feature_names = current_names + [f\"Extra_Feat_{i+1}\" for i in range(diff)]\n    else:\n        plotting_feature_names = current_names\n\n    # 2. Define Wrapper (The Fix)\n    # ---------------------------------------------------------\n    def model_wrapper(flat_data):\n        \"\"\"\n        Converts flat SHAP input back to 3D and queries the model\n        using Direct Call (not .predict) to avoid TF Graph errors.\n        \"\"\"\n        # Convert Numpy -> Tensor\n        # Reshape to (Samples, Time, Features)\n        reshaped = flat_data.reshape(-1, true_n_steps, true_n_features)\n        tensor_input = tf.convert_to_tensor(reshaped, dtype=tf.float32)\n\n        # Direct call (Eager Mode) - Faster and safer for SHAP\n        probs = trained_model(tensor_input, training=False)\n\n        # Return as Numpy flattened array\n        return probs.numpy().flatten()\n\n    # 3. Optimize Background (K-Means)\n    # ---------------------------------------------------------\n    print(\"Summarizing background data...\")\n    # Flatten train data: (Samples, Time*Feats)\n    X_train_flat = X_train.reshape(X_train.shape[0], -1)\n\n    # Summarize training data into 10 weighted points (Centroids)\n    background_summary = shap.kmeans(X_train_flat, 10)\n\n    # 4. Initialize KernelExplainer\n    # ---------------------------------------------------------\n    print(\"Initializing KernelExplainer...\")\n    explainer = shap.KernelExplainer(model_wrapper, background_summary)\n\n    # 5. Calculate SHAP Values\n    # ---------------------------------------------------------\n    explain_size = 20  # Keep small for speed\n    X_test_flat = X_test.reshape(X_test.shape[0], -1)\n\n    # Pick random samples to explain\n    idxs = np.random.choice(X_test_flat.shape[0], explain_size, replace=False)\n    X_test_sample = X_test_flat[idxs]\n\n    print(f\"Calculating SHAP values for {explain_size} instances...\")\n    # nsamples=auto usually works best, or try 500 if slow\n    shap_values = explainer.shap_values(X_test_sample, nsamples=500)\n\n    print(f\"SHAP calculation complete.\")\n\n    # 6. Reshape for Visualization\n    # ---------------------------------------------------------\n    # (Samples, Flat_Features) -> (Samples, Time, Features)\n    shap_values_3d = np.array(shap_values).reshape(-1, true_n_steps, true_n_features)\n    X_test_sample_3d = X_test_sample.reshape(-1, true_n_steps, true_n_features)\n\n    # 7. PLOT A: Global Feature Importance\n    # ---------------------------------------------------------\n    # Sum SHAP impact across all time steps\n    shap_sum_over_time = np.sum(shap_values_3d, axis=1) # (Samples, Features)\n    features_mean = np.mean(X_test_sample_3d, axis=1)   # (Samples, Features)\n\n    plt.figure(figsize=(10, 8), dpi=300)\n    shap.summary_plot(shap_sum_over_time, features_mean,\n                      feature_names=plotting_feature_names, show=False)\n    plt.title(\"Global Feature Importance\")\n    plt.tight_layout()\n    plt.savefig(\"output/shap_summary_global.png\")\n    plt.show()\n\n    # 8. PLOT B: Temporal Heatmap\n    # ---------------------------------------------------------\n    # Average absolute impact per time step\n    temporal_importance = np.mean(np.abs(shap_values_3d), axis=0) # (Time, Feats)\n\n    plt.figure(figsize=(12, 6), dpi=300)\n    sns.heatmap(temporal_importance.T, cmap='viridis',\n                yticklabels=plotting_feature_names,\n                xticklabels=range(true_n_steps))\n\n    plt.title(\"Temporal Feature Importance\\n(Brighter = Higher Impact at that Lag)\")\n    plt.xlabel(\"Time Lag (Steps into Sequence)\")\n    plt.ylabel(\"Feature\")\n    plt.tight_layout()\n    plt.savefig(\"output/shap_temporal_heatmap.png\")\n    plt.show()\n\n    print(\"✅ SHAP Analysis Completed Successfully.\")\n\nexcept Exception as e:\n    print(f\"❌ SHAP Failed: {e}\")\n    import traceback\n    traceback.print_exc()\n\n\n\n\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/lstm-enkf#visualization-and-insights","position":43},{"hierarchy":{"lvl1":"Outline","lvl3":"Part 15: Time-Lagged Cross-Correlation (TLCC) Analysis","lvl2":"Library installation"},"type":"lvl3","url":"/lstm-enkf#part-15-time-lagged-cross-correlation-tlcc-analysis","position":44},{"hierarchy":{"lvl1":"Outline","lvl3":"Part 15: Time-Lagged Cross-Correlation (TLCC) Analysis","lvl2":"Library installation"},"content":"This section introduces Time-Lagged Cross-Correlation (TLCC) to investigate the dynamic relationships between environmental drivers and red tide concentrations. Unlike standard correlation, TLCC identifies leads and lags in the data, revealing how long it takes for a change in an environmental factor (like a nutrient spike) to manifest as a biological response (a bloom).","type":"content","url":"/lstm-enkf#part-15-time-lagged-cross-correlation-tlcc-analysis","position":45},{"hierarchy":{"lvl1":"Outline","lvl4":"Core Analytical Components","lvl3":"Part 15: Time-Lagged Cross-Correlation (TLCC) Analysis","lvl2":"Library installation"},"type":"lvl4","url":"/lstm-enkf#core-analytical-components","position":46},{"hierarchy":{"lvl1":"Outline","lvl4":"Core Analytical Components","lvl3":"Part 15: Time-Lagged Cross-Correlation (TLCC) Analysis","lvl2":"Library installation"},"content":"The analysis provides a temporal roadmap of the ecosystem’s behavior, helping to validate the choice of lookback windows used in the LSTM model.\n\nIdentifying Temporal Offsets:\nTLCC calculates the correlation between two time series—such as river discharge and Karenia brevis abundance—at various time shifts. By finding the “peak” correlation, we can determine the specific latency of the system. For example, if the peak correlation with Phosphorus occurs at a lag of 14 days, it suggests a two-week window for biological uptake and population growth.\n\nDirectionality of Influence:\nBy observing whether the peak correlation occurs at a positive or negative lag, we can confirm the causal direction. A positive lag (where the environmental variable precedes the bloom) confirms the variable as a leading indicator or “driver,” whereas a zero lag suggests a simultaneous response.\n\nStability and Seasonality:\nThe analysis can be extended to Windowed TLCC, which evaluates how these correlations change over time. This is particularly relevant for Florida’s coastlines, where the relationship between rainfall and red tide may shift between the wet and dry seasons.","type":"content","url":"/lstm-enkf#core-analytical-components","position":47},{"hierarchy":{"lvl1":"Outline","lvl4":"Insights and Model Refinement","lvl3":"Part 15: Time-Lagged Cross-Correlation (TLCC) Analysis","lvl2":"Library installation"},"type":"lvl4","url":"/lstm-enkf#insights-and-model-refinement","position":48},{"hierarchy":{"lvl1":"Outline","lvl4":"Insights and Model Refinement","lvl3":"Part 15: Time-Lagged Cross-Correlation (TLCC) Analysis","lvl2":"Library installation"},"content":"The results from this section serve as a critical bridge between data science and marine biology:\n\nFeature Engineering Validation: We use these findings to refine the lag_days parameters in Part 4, ensuring the model focuses on the most biologically relevant timeframes.\n\nSystem Memory: TLCC helps quantify the “memory” of the ecosystem, indicating how long an environmental disturbance continues to influence bloom dynamics.\n\nPredictive Confidence: Identifying features with strong, stable leads increases our confidence in the LSTM’s ability to provide early warnings before a bloom is visible in satellite or field data.\n\nBy incorporating TLCC, we move beyond static snapshots to a fluid, temporal understanding of the drivers behind red tide events.\n\ndef analyze_onset_lag(y_true, y_pred_prob, threshold=0.3, tolerance=4):\n    \"\"\"pynb i\n    Calculates exactly how many weeks late (or early) the model predicts blooms.\n\n    Args:\n        y_true: Binary actuals (0/1)\n        y_pred_prob: Continuous probabilities (0.0-1.0)\n        threshold: Decision threshold\n        tolerance: Max weeks to look for a matching predicted onset\n    \"\"\"\n    print(f\"\\n--- Onset Latency Analysis (Threshold: {threshold}) ---\")\n\n    y_pred_class = (y_pred_prob >= threshold).astype(int)\n\n    # 1. Identify \"Start\" events (0 -> 1 transitions)\n    # We use diff() to find where value changes from 0 to 1\n    actual_onsets = np.where(np.diff(y_true, prepend=0) == 1)[0]\n    pred_onsets = np.where(np.diff(y_pred_class, prepend=0) == 1)[0]\n\n    print(f\"Actual Bloom Starts found: {len(actual_onsets)}\")\n    print(f\"Predicted Bloom Starts found: {len(pred_onsets)}\")\n\n    if len(actual_onsets) == 0:\n        print(\"No blooms in test set to analyze.\")\n        return\n\n    # 2. Match Actual Starts to Nearest Predicted Start\n    lags = []\n\n    for t_actual in actual_onsets:\n        # Find predicted onsets within 'tolerance' weeks window\n        # We look for the closest prediction around the actual event\n        nearby_preds = pred_onsets[np.abs(pred_onsets - t_actual) <= tolerance]\n\n        if len(nearby_preds) > 0:\n            # Find the closest one\n            closest_pred = nearby_preds[np.argmin(np.abs(nearby_preds - t_actual))]\n\n            # Lag = Predicted Time - Actual Time\n            # Positive = Late (Lag)\n            # Negative = Early Warning (Lead)\n            lag = closest_pred - t_actual\n            lags.append(lag)\n\n            status = \"LATE\" if lag > 0 else \"EARLY\" if lag < 0 else \"PERFECT\"\n            print(f\"  Event at Week {t_actual}: Model is {status} by {abs(lag)} weeks.\")\n        else:\n            print(f\"  Event at Week {t_actual}: MISSED COMPLETELY (No prediction within {tolerance} weeks)\")\n            lags.append(np.nan) # Missed event\n\n    # 3. Summary Stats\n    lags_clean = [l for l in lags if not np.isnan(l)]\n    if lags_clean:\n        avg_lag = np.mean(lags_clean)\n        print(f\"\\n>>> AVERAGE LATENCY: {avg_lag:.2f} Weeks\")\n        if avg_lag > 0:\n            print(\"    (Positive = The model is LAGGING)\")\n        else:\n            print(\"    (Negative = The model gives EARLY WARNING)\")\n    else:\n        print(\"\\n>>> No matched events found.\")\n\n    # 4. Visualizing the Lag\n    # Cross-Correlation Plot (Statistical Proof)\n    # Shift predictions back/forward and see where correlation is highest\n    shifts = range(-5, 6) # Shift -5 to +5 weeks\n    correlations = []\n    for s in shifts:\n        # Shift predicted probabilities\n        if s < 0:\n            p_shifted = y_pred_prob[-s:]\n            y_shifted = y_true[:s]\n        elif s > 0:\n            p_shifted = y_pred_prob[:-s]\n            y_shifted = y_true[s:]\n        else:\n            p_shifted = y_pred_prob\n            y_shifted = y_true\n\n        # Calculate correlation\n        if len(y_shifted) > 0:\n            corr = np.corrcoef(y_shifted, p_shifted)[0, 1]\n            correlations.append(corr)\n        else:\n            correlations.append(0)\n\n    plt.figure(figsize=(8, 5), dpi=300)\n    plt.bar(shifts, correlations, color='skyblue', edgecolor='black')\n\n    # Highlight the max\n    max_idx = np.argmax(correlations)\n    best_lag = shifts[max_idx]\n    plt.bar(best_lag, correlations[max_idx], color='red', label=f'Peak Correlation (Lag={best_lag})')\n\n    plt.title(\"Time-Lagged Cross-Correlation (TLCC)\")\n    plt.xlabel(\"Lag (Weeks)\\n<-- Model Leads (Good) | Model Lags (Bad) -->\")\n    plt.ylabel(\"Correlation with Actuals\")\n    plt.axvline(0, color='black', linestyle='--')\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(\"output/lag_analysis.png\")\n    plt.show()\n\n# --- Execute ---\nif __name__ == \"__main__\":\n    if 'mc_preds' in locals() and 'y_test_trimmed' in locals():\n        # Get mean probs\n        probs = np.mean(mc_preds, axis=1)\n\n        # Run with your chosen threshold (e.g., 0.3)\n        analyze_onset_lag(y_test_trimmed.flatten(), probs.flatten(), threshold=0.3)\n\n\n\n","type":"content","url":"/lstm-enkf#insights-and-model-refinement","position":49}]}