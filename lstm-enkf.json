{"version":3,"kind":"Notebook","sha256":"76c286d5dc253bae14b24f9694bdd23e1c3ee36934db17e5846451d04204fb24","slug":"lstm-enkf","location":"/LSTM+EnKF.ipynb","dependencies":[],"frontmatter":{"title":"Outline","content_includes_title":false,"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"authors":[{"nameParsed":{"literal":"Victor M. Blanco","given":"Victor M.","family":"Blanco"},"name":"Victor M. Blanco","id":"contributors-Users\\victo\\PyCharmMiscProject\\myst-generated-uid-0"},{"nameParsed":{"literal":"Daniel Krutky","given":"Daniel","family":"Krutky"},"name":"Daniel Krutky","id":"contributors-Users\\victo\\PyCharmMiscProject\\myst-generated-uid-1"},{"nameParsed":{"literal":"Peter Nguyen","given":"Peter","family":"Nguyen"},"name":"Peter Nguyen","id":"contributors-Users\\victo\\PyCharmMiscProject\\myst-generated-uid-2"},{"nameParsed":{"literal":"Ahmed S. Elshall","given":"Ahmed S.","family":"Elshall"},"name":"Ahmed S. Elshall","id":"contributors-Users\\victo\\PyCharmMiscProject\\myst-generated-uid-3"},{"nameParsed":{"literal":"Ming Ye","given":"Ming","family":"Ye"},"name":"Ming Ye","id":"contributors-Users\\victo\\PyCharmMiscProject\\myst-generated-uid-4"},{"nameParsed":{"literal":"Michael L. Parsons","given":"Michael L.","family":"Parsons"},"name":"Michael L. Parsons","id":"contributors-Users\\victo\\PyCharmMiscProject\\myst-generated-uid-5"}],"keywords":["harmful algae blooms","red tides","machine learning","deep learning","ensemble Kalman filter","LSTM","uncertainty analysis","data assimilation","Florida"],"exports":[{"format":"ipynb","filename":"LSTM+EnKF.ipynb","url":"/build/LSTM+EnKF-f375f434b088e097355cdcd5881a5701.ipynb"}]},"widgets":{"application/vnd.jupyter.widget-state+json":{"0eef66dc7b3b45f581fc334c6d33db34":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_03534a03d35d49f983c66f8bc1572b9c","IPY_MODEL_eff0a5292d484968ba7e7bf3cef090c3","IPY_MODEL_eecb5d87e890498487275b62e90adcd4"],"layout":"IPY_MODEL_8b1224ba29d34b84b1981718c4b70ab0"}},"03534a03d35d49f983c66f8bc1572b9c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9dff54d4d6f4fde817049a8bc267cd1","placeholder":"​","style":"IPY_MODEL_f7a5380eec4d426ab054cfbfe967b776","value":"100%"}},"eff0a5292d484968ba7e7bf3cef090c3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b27cac4bb2d64bd090640e6a725c0057","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_486fa3ac2e2e4b2681e63ff0ba064f8e","value":20}},"eecb5d87e890498487275b62e90adcd4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac29d24173054a929fc194828a9f1e60","placeholder":"​","style":"IPY_MODEL_e5f7e2ea050041ed9cc2891b4d3c3eea","value":" 20/20 [00:02&lt;00:00,  9.39it/s]"}},"8b1224ba29d34b84b1981718c4b70ab0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9dff54d4d6f4fde817049a8bc267cd1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7a5380eec4d426ab054cfbfe967b776":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b27cac4bb2d64bd090640e6a725c0057":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"486fa3ac2e2e4b2681e63ff0ba064f8e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ac29d24173054a929fc194828a9f1e60":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e5f7e2ea050041ed9cc2891b4d3c3eea":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"mdast":{"type":"root","children":[{"type":"block","kind":"notebook-content","data":{"id":"dc526dc0"},"children":[{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"This project presents a novel integration of Deep Learning and Sequential Data Assimilation to forecast Karenia brevis (red tide) blooms. While LSTMs are powerful at capturing non-linear temporal dependencies, they often suffer from “drift” when applied to long-term forecasting without mid-course corrections. Our framework solves this by treating the LSTM as the “transition model” within an Ensemble Kalman Filter (EnKF) loop.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"K0nB8iPq9I"}],"key":"hVAhPwe5Tt"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"Research Objectives\nThe primary goal of this research is to develop a robust, self-correcting forecasting system for harmful algal blooms along the Florida coast.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"fIzkUd4eHG"}],"key":"KlFjLEIo6J"},{"type":"paragraph","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"Architecture Design: Construct an LSTM neural network optimized for high-dimensional environmental time-series and extreme class imbalance.","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"UgKnLY5HQs"}],"key":"E3RcjKtrON"},{"type":"paragraph","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Hybrid Integration: Develop a mathematical bridge that allows LSTM state outputs to be assimilated and corrected by an Ensemble Kalman Filter (EnKF) in real-time.","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"uQULXHkQRY"}],"key":"ciJaA3i5gc"},{"type":"paragraph","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"Uncertainty Quantification: Implement Monte Carlo (MC) Dropout to shift from deterministic “point-forecasts” to probabilistic risk assessments.","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"IKScvRuXTK"}],"key":"tzE9ZCLasy"},{"type":"paragraph","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"Ecological Interpretability: Quantify the sensitivity of the model to specific environmental drivers like nutrient loading and river discharge using SHAP values.","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"zvbRpvkGXz"}],"key":"i81UfWl1Hz"},{"type":"paragraph","position":{"start":{"line":16,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"text","value":"Research Questions\nTo evaluate the effectiveness of this hybrid framework, this project seeks to answer:","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"c1BvsJ9doM"}],"key":"cV8U7CgGAy"},{"type":"paragraph","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"text","value":"Can data assimilation mitigate LSTM drift? To what extent does the periodic injection of physical observations via EnKF improve the long-term stability of K. brevis forecasts?","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"RnOnKMURre"}],"key":"EM1pacfjrf"},{"type":"paragraph","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"How does ecological memory impact accuracy? Does the inclusion of long-term rolling aggregates and multi-day lags significantly outperform models relying only on immediate environmental snapshots?","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"rKZhIvRkB9"}],"key":"gRDPJrcAD1"},{"type":"paragraph","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"What is the “Reliability-Resolution” trade-off? How well does the MC Dropout ensemble capture the actual variance of bloom occurrences?","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"JmYLf3QLpt"}],"key":"BAdJD5Buh6"},{"type":"paragraph","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"text","value":"Which environmental drivers dominate the model’s decision-making? Does the model prioritize nutrient concentrations, physical transport (discharge), or biological persistence when predicting a bloom?","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"aqaK9mC9IK"}],"key":"tloWXhVifL"},{"type":"paragraph","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"text","value":"Technical Roadmap","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"VBKrpHv9lo"}],"key":"otPuclPpYE"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":28,"column":1},"end":{"line":30,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":28,"column":1},"end":{"line":30,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Data Foundation & Enhanced Engineering\nThe predictive power of this framework rests on capturing the “ecological memory” of the Florida coast.","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"key":"szFFS3328K"}],"key":"ObU1KxCLNI"}],"key":"len2IE932D"}],"key":"VHXpj1cevj"},{"type":"paragraph","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"children":[{"type":"text","value":"Feature Engineering (Parts 1-4): We move beyond raw concentrations to create Rolling Aggregates (7, 14, and 30-day windows) and Time-Lagged Variables. This allows the model to “see” nutrient accumulation trends and delayed river discharge impacts.","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"UPjLubNHL1"}],"key":"B8jxPkyaXf"},{"type":"paragraph","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"children":[{"type":"text","value":"Robust Preprocessing: Using RobustScaler, we ensure that extreme outliers—common in nutrient data during hurricane or high-flow events—do not skew the model’s weight distribution.","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"key":"av68KXJmID"}],"key":"RePQp6FV3N"},{"type":"list","ordered":true,"start":2,"spread":false,"position":{"start":{"line":35,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":35,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"The LSTM Predictive Engine\nArchitecture & Tuning (Parts 5-9): We implement a stacked LSTM architecture designed to process 3D temporal sequences. To combat extreme class imbalance, we utilize Custom Class Weighting in the loss function to penalize the misclassification of rare bloom events more heavily.","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"key":"C0JliQjEej"}],"key":"XwDyqlCR3l"}],"key":"LLKr97eihE"}],"key":"sAv7d37tSV"},{"type":"paragraph","position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"children":[{"type":"text","value":"Hyperparameter Optimization: We utilize Bayesian optimization to determine the ideal hidden layer depth, dropout rates, and optimal sequence length (lookback).","position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"key":"aB9dFdXGQk"}],"key":"Y2SIyyGCFf"},{"type":"list","ordered":true,"start":3,"spread":false,"position":{"start":{"line":40,"column":1},"end":{"line":42,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":40,"column":1},"end":{"line":42,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"The Hybrid Loop: Data Assimilation (EnKF)\nThis is the core innovation: anchoring deep learning predictions to physical reality.","position":{"start":{"line":40,"column":1},"end":{"line":40,"column":1}},"key":"CkYla8W30e"}],"key":"bid0xkXKaG"}],"key":"tG1TVIL0u5"}],"key":"feHDh2hNEr"},{"type":"paragraph","position":{"start":{"line":43,"column":1},"end":{"line":43,"column":1}},"children":[{"type":"text","value":"The Forecast Step: The LSTM generates a state estimate for the next time step.","position":{"start":{"line":43,"column":1},"end":{"line":43,"column":1}},"key":"Xztrnf5ueO"}],"key":"gyHL2cllhb"},{"type":"paragraph","position":{"start":{"line":45,"column":1},"end":{"line":45,"column":1}},"children":[{"type":"text","value":"The Assimilation Step (Parts 10-12): When a new physical observation (e.g., a nitrogen measurement) becomes available, the EnKF calculates the Kalman Gain. This corrects the model’s internal state variables before the next recursive forecast, effectively resetting the “drift.”","position":{"start":{"line":45,"column":1},"end":{"line":45,"column":1}},"key":"lYhJHTOo6g"}],"key":"rpIPtMmC3A"},{"type":"paragraph","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"children":[{"type":"text","value":"MC Dropout: By keeping dropout active during inference, we generate an ensemble of predictions to quantify Epistemic Uncertainty.","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"UtL7Q5dGai"}],"key":"ZWzbrm4l2Q"},{"type":"list","ordered":true,"start":4,"spread":false,"position":{"start":{"line":49,"column":1},"end":{"line":51,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":49,"column":1},"end":{"line":51,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Advanced Evaluation & Explainability\nProbabilistic Evaluation (Part 13): We evaluate using Brier Skill Scores to measure the calibration of our probability forecasts and Precision-Recall Curves to assess bloom detection reliability.","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"key":"dxV1II0Hni"}],"key":"CCRPBSTTtK"}],"key":"iRMpgdhzO2"}],"key":"XCdwq2Ztf7"},{"type":"paragraph","position":{"start":{"line":52,"column":1},"end":{"line":52,"column":1}},"children":[{"type":"text","value":"Opening the Black Box (Parts 14-15): * SHAP Analysis: Identifying which specific nutrients are driving “high probability” forecasts.","position":{"start":{"line":52,"column":1},"end":{"line":52,"column":1}},"key":"qeyS7B8rUd"}],"key":"WeeeZqrlm3"},{"type":"paragraph","position":{"start":{"line":54,"column":1},"end":{"line":54,"column":1}},"children":[{"type":"text","value":"TLCC Analysis: Validating the lead-time of features to ensure the LSTM is learning biologically plausible causal relationships.","position":{"start":{"line":54,"column":1},"end":{"line":54,"column":1}},"key":"t5Rkvz5LLX"}],"key":"CvsLDNcMqP"}],"identifier":"dc526dc0","label":"dc526dc0","html_id":"dc526dc0","key":"PW7DI5jQWg"},{"type":"block","kind":"notebook-content","data":{"id":"vtGtg_lX_K5T"},"children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Library installation","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"e2z1Js54vl"}],"identifier":"library-installation","label":"Library installation","html_id":"library-installation","implicit":true,"key":"VyEJEC45aV"}],"identifier":"vtgtg_lx_k5t","label":"vtGtg_lX_K5T","html_id":"vtgtg-lx-k5t","key":"Fdz26L6tOi"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iuwFYPopie1_","outputId":"1fec5d1b-1541-445e-9c4b-3849ead10d6d","jupyter":{"is_executing":true}},"children":[{"type":"code","lang":"python","executable":true,"value":"!pip install tensorflow.keras\n!pip install keras_tuner\nimport json\nimport os\n\nimport joblib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport shap\nimport tensorflow as tf\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score,\n                             brier_score_loss, accuracy_score, precision_score,\n                             recall_score, f1_score)\nfrom sklearn.metrics import (precision_recall_curve)\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, Input\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.metrics import BinaryAccuracy, AUC, Recall, Precision\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\ntry:\n    from tqdm.notebook import tqdm\n    print(\"Using tqdm.notebook for progress bars.\")\nexcept ImportError:\n    try:\n        from tqdm import tqdm\n        print(\"Using standard tqdm for progress bars.\")\n    except ImportError:\n        print(\"Warning: tqdm not installed. Progress bars will not be shown.\")\n        # Define a dummy tqdm function if not installed\n        def tqdm(iterable=None, *args, **kwargs):\n            if iterable is None:\n                # Handle case where tqdm is called without an iterable\n                class DummyTqdm:\n                    def __enter__(self): return self\n                    def __exit__(self, *args): pass\n                    def update(self, n=1): pass\n                    def close(self): pass\n                    def set_description(self, desc): pass\n                return DummyTqdm()\n            else:\n                return iterable\n\nprint(\"Imported libraries.\")","identifier":"iuwfypopie1_-code","enumerator":"1","html_id":"iuwfypopie1-code","key":"mGLTu9vwOd"},{"type":"outputs","id":"1_aOAX0An0kWmPeo5j0Y9","children":[],"identifier":"iuwfypopie1_-outputs","html_id":"iuwfypopie1-outputs","key":"ytANnshNim"}],"identifier":"iuwfypopie1_","label":"iuwFYPopie1_","html_id":"iuwfypopie1","key":"fvqKLxI5Zy"},{"type":"block","kind":"notebook-content","data":{"id":"2ngbXkSqi50I"},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Part 1: Configuration & Setup","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lgaO4YDCxt"}],"identifier":"part-1-configuration-setup","label":"Part 1: Configuration & Setup","html_id":"part-1-configuration-setup","implicit":true,"key":"HAZdsKYiee"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Define all file paths (input data, saved components like scaler/model).","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"RiEUalFyy2"}],"key":"s2jOEVOMOa"}],"key":"YvkIndXCIN"},{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Set core parameters: ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"JmjWNWTPzc"},{"type":"inlineCode","value":"SEQUENCE_LENGTH","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"xWZ5I4mj3L"},{"type":"text","value":", ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"l5aVkdHOKN"},{"type":"inlineCode","value":"FORECAST_HORIZON","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"uCsnrpbYKX"},{"type":"text","value":", ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Hy2udeHxJJ"},{"type":"inlineCode","value":"BLOOM_THRESHOLD","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"XwWPvLjYwa"},{"type":"text","value":".","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"oQ3MxYhSo3"}],"key":"U0mg6jcdMv"}],"key":"SLDoFjML6i"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Define feature lists: ","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"y67nJj6YsZ"},{"type":"inlineCode","value":"BASE_FEATURES","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"CvAPCxitQB"},{"type":"text","value":", ","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"tOFFP09KGK"},{"type":"inlineCode","value":"ENKF_STATE_VARS","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"V9jVmK9WNI"},{"type":"text","value":", ","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"zVxoVRgxk4"},{"type":"inlineCode","value":"ENHANCED_KB_LAGS","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"HzCOBhxcRJ"},{"type":"text","value":" (optional).","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"OUZpGWf3hK"}],"key":"CZDtsIAazw"}],"key":"DpdRDLjk3D"},{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Control Flags/Options:","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"LX4q5qLHVL"}],"key":"aUVobTYckc"}],"key":"P5n1zhNetI"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":6,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"paragraph","children":[{"type":"inlineCode","value":"USE_ENHANCED_FEATURES","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"SqHrTbC2gl"},{"type":"text","value":": Boolean (True/False) to switch between basic and enhanced feature engineering.","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"LBFSrDVt60"}],"key":"J89k13mmTi"}],"key":"Vf7eHMtdkQ"},{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"paragraph","children":[{"type":"inlineCode","value":"SCALER_TYPE","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"ow2FbUi5wZ"},{"type":"text","value":": String (‘Standard’ or ‘Robust’) to choose the scaler.","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"KV2xU4rnLp"}],"key":"RWb4a95UiJ"}],"key":"YbrnozIEZ5"},{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"inlineCode","value":"USE_CLASS_WEIGHT","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"LtVjq6J4Uu"},{"type":"text","value":": Boolean (True/False) to enable/disable class weighting during training.","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"DWFrNNRJPM"}],"key":"Ki9lo1wVjE"}],"key":"Zmz8tfrLlB"},{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"paragraph","children":[{"type":"inlineCode","value":"PERFORM_TUNING","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"jlvnksrkHm"},{"type":"text","value":": Boolean (True/False) to run hyperparameter tuning or use defaults.","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"Jzj4abIYsS"}],"key":"P5tWdbmcXR"}],"key":"p6BDkW0pyA"}],"key":"EySlK2DNos"}],"key":"WZxlgJ5vgb"},{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Set tuning parameters: ","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"tSVs3kAhEK"},{"type":"inlineCode","value":"MAX_TRIALS","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"pmdXo5vyQZ"},{"type":"text","value":", ","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"OtZe3ppjx3"},{"type":"inlineCode","value":"TUNER_EPOCHS","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"lOSFvwNoZJ"},{"type":"text","value":".","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"sVhEP0aXdv"}],"key":"nNAiBoLTUu"}],"key":"TNudEIxnmj"},{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Set training parameters: ","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"gwic7OuOUf"},{"type":"inlineCode","value":"EPOCHS","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"Wq1xTPoBOO"},{"type":"text","value":", ","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"sDCZCeQRek"},{"type":"inlineCode","value":"BATCH_SIZE","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"JHP4YQDWwW"},{"type":"text","value":", ","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"ihZ8Pj3GUK"},{"type":"inlineCode","value":"PATIENCE","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"CiNkfjxJ0r"},{"type":"text","value":".","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"ozaxUaJudq"}],"key":"yRaktNehUM"}],"key":"AcagZ6hYvd"},{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Set EnKF parameters: ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"E1VO0795Nw"},{"type":"inlineCode","value":"N_ENKF","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"zc1q22VWMU"},{"type":"text","value":" (ensemble size), define noise matrix estimation approach (e.g., ‘basic_stats’ or ‘manual’).","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"wmdcH7BcPd"}],"key":"fP77jKwleh"}],"key":"YN5YOzoNas"},{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Set MC Dropout parameters: ","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"IMgpPCd5LJ"},{"type":"inlineCode","value":"N_MC_SAMPLES","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"Mw7W7Q7fmu"},{"type":"text","value":".","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"XE02TtPjQb"}],"key":"fyICx3YpNe"}],"key":"H3OSCzRzkc"}],"key":"r0BG5jeGa6"}],"identifier":"2ngbxksqi50i","label":"2ngbXkSqi50I","html_id":"id-2ngbxksqi50i","key":"WWBAHNUuEl"},{"type":"block","kind":"notebook-code","data":{"id":"vP1-KGorslgp","ExecuteTime":{"end_time":"2026-01-07T03:44:43.360958Z","start_time":"2026-01-07T03:37:16.343344Z"}},"children":[{"type":"code","lang":"python","executable":true,"value":"# Set these flags to control the workflow execution\n# === Feature Engineering ===\n# Set to True to use detailed lags (like RF paper), False: Use basic lags\nUSE_ENHANCED_FEATURES = True\n\n# === Preprocessing ===\n# Options: 'Robust' or 'Standard'\nSCALER_TYPE = 'Robust'\n\n# === Training ===\n# Set to True to apply class weighting during LSTM training\nUSE_CLASS_WEIGHT = True\n# Set to True to run KerasTuner hyperparameter search, False to use default HPs\nPERFORM_TUNING = True\n\n# === Advanced Steps ===\n# Set to True to run EnKF data assimilation during testing/forecasting\nPERFORM_ENKF = True","identifier":"vp1-kgorslgp-code","enumerator":"2","html_id":"vp1-kgorslgp-code","key":"IQOG6xuz4v"},{"type":"outputs","id":"emTEtKpTbR_USCuOSaoO0","children":[],"identifier":"vp1-kgorslgp-outputs","html_id":"vp1-kgorslgp-outputs","key":"hYaOR4W0T8"}],"identifier":"vp1-kgorslgp","label":"vP1-KGorslgp","html_id":"vp1-kgorslgp","key":"IGSzjsbK7d"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b5eUaHJksrFS","outputId":"a49d3d56-ded0-45e3-941b-e581a2233584","ExecuteTime":{"end_time":"2026-01-07T03:44:43.360958Z","start_time":"2026-01-07T03:34:10.647980Z"}},"children":[{"type":"code","lang":"python","executable":true,"value":"# --- File Paths ---\nINPUT_DATA_PATH = 'data_weekly_interpolated.csv'\n# Directory for optional external files (e.g., Caloosahatchee) - Ensure this exists if used\nEXTERNAL_DATA_DIR = 'external_data/'\n# Directory to save results, models, scalers (use a distinct name)\nOUTPUT_DIR = 'output_refactored/'\n# Construct filenames dynamically based on config where appropriate\nscaler_suffix = SCALER_TYPE.lower()\nfeature_suffix = 'enhanced' if USE_ENHANCED_FEATURES else 'basic'\nSCALER_FILENAME = os.path.join(OUTPUT_DIR, f'red_tide_scaler_{scaler_suffix}.joblib')\nFEATURE_LIST_FILENAME = os.path.join(OUTPUT_DIR, f'red_tide_feature_list_{feature_suffix}.joblib')\n# Template for sequence files: {horizon}{feature_suffix}\nSEQUENCES_FILENAME_TEMPLATE = os.path.join(OUTPUT_DIR, 'sequences_horizon{}wk_{}.npz')\n# Template for model checkpoint files: {model_type} e.g., baseline_weighted\n# Using .keras extension for saving the full model (architecture + weights + optimizer state)\nMODEL_CHECKPOINT_TEMPLATE = os.path.join(OUTPUT_DIR, 'best_lstm_model_{}.keras')\n# Use .weights.h5 if saving only weights (e.g., for subclassed models like physics-informed)\nMODEL_WEIGHTS_CHECKPOINT_TEMPLATE = os.path.join(OUTPUT_DIR, 'best_lstm_model_{}.weights.h5')\n\nTUNER_PROJECT_DIR = 'keras_tuner_dir_refactored' # Use a new name\nTUNER_PROJECT_NAME = 'red_tide_lstm_tuning'\n\n# Create output directory if it doesn't exist\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nprint(f\"Output directory: {OUTPUT_DIR}\")","identifier":"b5euahjksrfs-code","enumerator":"3","html_id":"b5euahjksrfs-code","key":"nf2tvA8OrE"},{"type":"outputs","id":"-oHTVcgCfVq9EC5XyJ5f3","children":[{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"Output directory: output_refactored/\n"},"children":[],"identifier":"b5euahjksrfs-outputs-0","html_id":"b5euahjksrfs-outputs-0","key":"H2Q2SX0CZy"}],"identifier":"b5euahjksrfs-outputs","html_id":"b5euahjksrfs-outputs","key":"i8ee5eayPe"}],"identifier":"b5euahjksrfs","label":"b5eUaHJksrFS","html_id":"b5euahjksrfs","key":"kjqxhCe9qB"},{"type":"block","kind":"notebook-code","data":{"id":"6wGE-LHas0R_","ExecuteTime":{"end_time":"2026-01-07T03:44:43.368602700Z","start_time":"2026-01-07T03:34:10.669998Z"}},"children":[{"type":"code","lang":"python","executable":true,"value":"# --- Core Parameters ---\nDATETIME_COL = 'time'\nTARGET_COL = 'kb' # Original cell count column\nTARGET_BINARY_COL = 'bloom_target' # Binary target column name\nBLOOM_THRESHOLD = 1e5 # Cells/L\nSEQUENCE_LENGTH = 12  # Default sequence length (can be tuned)\nFORECAST_HORIZON = 1   # Default forecast horizon (1-week or 4-week)","identifier":"6wge-lhas0r_-code","enumerator":"4","html_id":"id-6wge-lhas0r-code","key":"qT460rsxw3"},{"type":"outputs","id":"P-M4WA2IYw9rmYKfmM4zP","children":[],"identifier":"6wge-lhas0r_-outputs","html_id":"id-6wge-lhas0r-outputs","key":"Acoul1LhQD"}],"identifier":"6wge-lhas0r_","label":"6wGE-LHas0R_","html_id":"id-6wge-lhas0r","key":"rA05OO0vvi"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BqoFhfOQiRYw","outputId":"30f18e42-46d3-4574-e0e2-aa90d4332569","ExecuteTime":{"end_time":"2026-01-07T03:44:43.372952600Z","start_time":"2026-01-07T03:35:26.298847Z"}},"children":[{"type":"code","lang":"python","executable":true,"value":"# --- Feature Engineering Configuration ---\nBASE_FEATURES = [ # Features from the core dataset to consider initially\n    'zos', 'water_temp',\n    'peace_discharge', 'peace_TN', 'peace_TP',\n    'wind_u', 'wind_v'\n]\n# Define basic lag configuration\nBASIC_LAG_CONFIG = {\n    'kb_lags': [1, 2],\n    'env_lags': list(range(1, 7)) # e.g., 1 to 6 weeks\n}\n# Define enhanced lag configuration (matching RF paper more closely)\nENHANCED_LAG_CONFIG = {\n    # 'kb_lags': [1, 2],\n    'kb_rolling_windows': [(1, 4), (5, 8), (9, 12)], # (start_lag, end_lag) for mean & prop\n    'discharge_rolling_window': 4, # e.g., 4-week avg discharge lag 1\n    'env_lags': [1] # Lags for other env vars in enhanced mode\n}\n\n# --- Data Splitting ---\nTRAIN_SPLIT_RATIO = 0.70\nVALIDATION_SPLIT_RATIO = 0.15\n# Test split is the remainder\n\n# --- LSTM Model Default Hyperparameters (Used if PERFORM_TUNING is False) ---\nDEFAULT_LSTM_UNITS = 64\nDEFAULT_DROPOUT_RATE = 0.3\nDEFAULT_LEARNING_RATE = 0.001\n\n# --- KerasTuner Configuration (Used if PERFORM_TUNING is True) ---\nTUNER_MAX_TRIALS = 10\nTUNER_EPOCHS = 30\nTUNER_BATCH_SIZE = 32\n\n# --- Training Parameters ---\nTRAIN_EPOCHS = 50\nTRAIN_BATCH_SIZE = 32\nTRAIN_PATIENCE = 10 # For EarlyStopping\n\n# --- EnKF Configuration ---\nENKF_STATE_VARS = [\n    'peace_discharge', 'peace_TN', 'peace_TP', 'kb','wind_u', 'wind_v'\n]\n# Variables to assimilate\nN_ENKF = 50 # Ensemble size\n# Noise Estimation Approach ('basic_stats' uses train set stats, 'manual' requires defining R_diag, Q_diag below)\nENKF_NOISE_ESTIMATION = 'manual'\n# Manual noise variances (used only if ENKF_NOISE_ESTIMATION = 'manual') - Define placeholder values\nMANUAL_R_DIAG = [\n    (100.0 * 0.01)**2, # Discharge (Trust the gauge)\n    (0.2 * 0.01)**2,   # TN (Trust the sample)\n    (0.02 * 0.01)**2,  # TP\n    (5.0 * 0.05)**2,   # Wind U (Wind is noisy, maybe 5% error)\n    (5.0 * 0.05)**2,   # Wind V\n    (0.1 * 0.01)**2    # ZOS (Trust the satellite) # Example observation noise variances\n    ]\nMANUAL_Q_DIAG = [\n    (150.0 * 2.0)**2,  # Discharge can spike massively\n    (0.3 * 2.0)**2,    # TN spikes\n    (0.03 * 2.0)**2,   # TP spikes\n    (10.0 * 1.5)**2,   # Wind changes direction rapidly\n    (10.0 * 1.5)**2,   # Wind V\n    (0.1 * 1.0)**2     # ZOS changes\n]\n\n# --- MC Dropout Configuration ---\nN_MC_SAMPLES = 50\n\n\n# --- Print Setup Summary ---\nprint(\"\\n--- Workflow Configuration Summary ---\")\nprint(f\"Enhanced Features Enabled: {USE_ENHANCED_FEATURES}\")\nprint(f\"Scaler Type Selected: {SCALER_TYPE}\")\nprint(f\"Class Weighting Enabled: {USE_CLASS_WEIGHT}\")\nprint(f\"Hyperparameter Tuning Enabled: {PERFORM_TUNING}\")\nprint(f\"EnKF Enabled: {PERFORM_ENKF}\")\nprint(f\"Sequence Length: {SEQUENCE_LENGTH}\")\nprint(f\"Forecast Horizon: {FORECAST_HORIZON} week(s)\")\nif PERFORM_TUNING:\n    print(f\"Tuner Max Trials: {TUNER_MAX_TRIALS}, Epochs per Trial: {TUNER_EPOCHS}\")\nelse:\n    print(f\"Using Default LSTM HPs: Units={DEFAULT_LSTM_UNITS}, Dropout={DEFAULT_DROPOUT_RATE}, LR={DEFAULT_LEARNING_RATE}\")\nif PERFORM_ENKF:\n    print(f\"EnKF Ensemble Size: {N_ENKF}, Noise Estimation: {ENKF_NOISE_ESTIMATION}\")\nprint(\"------------------------------------\")\n\n# Check if optional modules were imported if flags are set\nif PERFORM_TUNING and kt is None:\n    print(\"\\nWarning: KerasTuner (kt) not imported/installed, but PERFORM_TUNING is True. Tuning will be skipped.\")\n    PERFORM_TUNING = False # Disable tuning if library not available\nif PERFORM_ENKF and 'EnsembleKalmanFilter' not in locals():\n     # We will define EnKF class later, but good to note dependency\n     print(\"\\nNote: EnKF flag is True, ensure EnsembleKalmanFilter class is defined/imported later.\")","identifier":"bqofhfoqiryw-code","enumerator":"5","html_id":"bqofhfoqiryw-code","key":"BdPQjvxakk"},{"type":"outputs","id":"bLf060VsCNI1xzOfUVV-B","children":[{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"\n--- Workflow Configuration Summary ---\nEnhanced Features Enabled: True\nScaler Type Selected: Robust\nClass Weighting Enabled: True\nHyperparameter Tuning Enabled: True\nEnKF Enabled: True\nSequence Length: 12\nForecast Horizon: 1 week(s)\nTuner Max Trials: 10, Epochs per Trial: 30\nEnKF Ensemble Size: 50, Noise Estimation: manual\n------------------------------------\n\nNote: EnKF flag is True, ensure EnsembleKalmanFilter class is defined/imported later.\n"},"children":[],"identifier":"bqofhfoqiryw-outputs-0","html_id":"bqofhfoqiryw-outputs-0","key":"sGrqy6cfAS"}],"identifier":"bqofhfoqiryw-outputs","html_id":"bqofhfoqiryw-outputs","key":"CIaMmTEkaQ"}],"identifier":"bqofhfoqiryw","label":"BqoFhfOQiRYw","html_id":"bqofhfoqiryw","key":"Bieup73cks"},{"type":"block","kind":"notebook-content","data":{"id":"HKujscl3jRj2"},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Part 2: Data Loading & Initial Processing","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rFjqvpuOJl"}],"identifier":"part-2-data-loading-initial-processing","label":"Part 2: Data Loading & Initial Processing","html_id":"part-2-data-loading-initial-processing","implicit":true,"key":"ihvyiyS7gP"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"This cell defines and executes a function to load the raw weekly data, handle the datetime index, calculate U/V wind components, and create the binary target variable based on the bloom threshold. It uses the configuration parameters defined in Part 1.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"hoy3cEDhD3"}],"key":"NfGSE56LOT"},{"type":"paragraph","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"Explanation:","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"IyozQn5TP7"}],"key":"jy80amDHf3"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":8,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"calculate_wind_components(df): Takes a DataFrame, calculates U and V wind components if ‘wind_direction’ and ‘wind_speed’ exist, drops the original columns, and returns the modified DataFrame and a list of the newly added column names. Includes basic NaN handling for the calculation.","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"QRFGotO7r5"}],"key":"hWM4R5hkW8"}],"key":"fxADKaF0rA"},{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"create_target(...): Takes the DataFrame and target configuration, ensures the original target column is numeric, drops rows where the target is NaN (important for supervised learning), creates the binary bloom_target column, prints the class distribution, and returns the DataFrame.","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"P9br0UTViU"}],"key":"XPdphEaKK7"}],"key":"ARXLk8TtOS"},{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"load_and_prepare_data(...): Orchestrates the initial steps: loads the CSV, sets the datetime index, calls calculate_wind_components, calls create_target, and returns the resulting DataFrame (df_initial). Includes error handling for file not found or missing columns.","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"De0dOsO2SI"}],"key":"n6Jc5fBpWQ"}],"key":"X9yhkwliTH"},{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"merge_external_data(...) (Optional Placeholder): Provides a structure for loading and merging additional datasets based on a dictionary of file paths. It performs a left merge to keep all original data points. Note: This function needs actual file paths and assumes external CSVs have a compatible datetime index.","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"fyOz4gMplR"}],"key":"ncHn0zWBNC"}],"key":"cOqVyiGAAY"},{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Main Execution Block (if ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"PBpwVXzjmU"},{"type":"strong","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"name","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"vvUBSUZ2Hn"}],"key":"eZVMLb3pbf"},{"type":"text","value":" == “","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"G607SVcoIo"},{"type":"strong","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"main","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"NPnCpr57rC"}],"key":"ibR3PXv5hD"},{"type":"text","value":"”:): Calls load_and_prepare_data using the configuration variables. Includes a commented-out section showing how merge_external_data would be called if needed. Prints final info about the df_initial DataFrame. Includes basic NameError handling in case Part 1 wasn’t run.","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"r6VDmJodq5"}],"key":"EX1A88nQGo"}],"key":"TvlVNlyIEc"}],"key":"yt67u17WIZ"},{"type":"paragraph","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"After running this cell, the df_initial DataFrame should contain the core data with wind components calculated and the binary bloom target created, ready for the feature engineering steps in Part 3.","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"MsJDmv5dFX"}],"key":"FXyyCDo7sF"}],"identifier":"hkujscl3jrj2","label":"HKujscl3jRj2","html_id":"hkujscl3jrj2","key":"QORlHuFstW"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jaMZelqJjS5m","outputId":"223d2c0c-8aa0-4249-f7d2-3bf492b3cff4","ExecuteTime":{"end_time":"2026-01-07T03:44:43.373949800Z","start_time":"2026-01-07T03:35:26.321856Z"}},"children":[{"type":"code","lang":"python","executable":true,"value":"# Ensure configuration variables from Part 1 are accessible in the environment\n# Example: INPUT_DATA_PATH, DATETIME_COL, TARGET_COL, BLOOM_THRESHOLD, TARGET_BINARY_COL\n\ndef calculate_wind_components(df):\n    \"\"\"Calculates U and V wind components if columns exist.\"\"\"\n    if 'wind_direction' in df.columns and 'wind_speed' in df.columns:\n        print(\"Calculating wind U/V components...\")\n        # Ensure numeric types, handle potential NaNs before calculation\n        df['wind_direction'] = pd.to_numeric(df['wind_direction'], errors='coerce')\n        df['wind_speed'] = pd.to_numeric(df['wind_speed'], errors='coerce')\n\n        # Temporarily fill NaNs with 0 for calculation if they exist\n        wind_cols_to_check = ['wind_direction', 'wind_speed']\n        if df[wind_cols_to_check].isnull().any().any():\n            print(\"Warning: NaNs found in wind direction/speed. Temporarily filling with 0 for component calculation.\")\n            df[wind_cols_to_check] = df[wind_cols_to_check].fillna(0)\n\n        wind_dir_rad = np.deg2rad(df['wind_direction'])\n        wind_speed = df['wind_speed']\n        # Meteorological convention: wind direction 'coming from'\n        df['wind_u'] = -wind_speed * np.sin(wind_dir_rad)\n        df['wind_v'] = -wind_speed * np.cos(wind_dir_rad)\n        # Drop original wind columns\n        df = df.drop(columns=['wind_direction', 'wind_speed'])\n        print(\"Calculated wind U/V components and dropped original columns.\")\n        added_cols = ['wind_u', 'wind_v']\n    else:\n        print(\"Warning: 'wind_direction' or 'wind_speed' not found. Skipping component calculation.\")\n        added_cols = []\n    return df, added_cols\n\ndef create_target(df, target_col, bloom_threshold, target_binary_col):\n    \"\"\"Creates the binary target variable.\"\"\"\n    if target_col not in df.columns:\n        print(f\"Error: Target column '{target_col}' not found.\")\n        return None # Return None if target column is missing\n\n    print(f\"Creating binary target '{target_binary_col}' using threshold {bloom_threshold:.0f} cells/L...\")\n    df[target_col] = pd.to_numeric(df[target_col], errors='coerce')\n\n    # Handle potential NaNs in target before comparison by dropping rows\n    initial_rows = len(df)\n    if df[target_col].isnull().any():\n        print(f\"Warning: NaNs found in target column '{target_col}'. Dropping rows with NaN target.\")\n        df.dropna(subset=[target_col], inplace=True)\n        print(f\"Dropped {initial_rows - len(df)} rows with NaN in '{target_col}'. New shape: {df.shape}\")\n\n    df[target_binary_col] = (df[target_col] >= bloom_threshold).astype(int)\n    print(f\"Target distribution (%):\\n{df[target_binary_col].value_counts(normalize=True) * 100}\")\n    return df\n\ndef load_and_prepare_data(filepath, datetime_col, target_col, bloom_threshold, target_binary_col):\n    \"\"\"Loads data, handles datetime, calculates wind components, creates target.\"\"\"\n    print(f\"Loading data from: {filepath}\")\n    try:\n        df = pd.read_csv(filepath)\n        # Handle datetime index\n        if datetime_col not in df.columns:\n            raise ValueError(f\"Datetime column '{datetime_col}' not found.\")\n        df[datetime_col] = pd.to_datetime(df[datetime_col])\n        df = df.sort_values(datetime_col).set_index(datetime_col)\n        print(f\"Data loaded successfully. Shape: {df.shape}, Time range: {df.index.min()} to {df.index.max()}\")\n        print(f\"Initial NaN counts:\\n{df.isnull().sum()}\")\n\n        # Calculate wind components\n        df, wind_cols = calculate_wind_components(df)\n\n        # Create binary target\n        df = create_target(df, target_col, bloom_threshold, target_binary_col)\n\n        if df is not None:\n            print(\"\\n--- Initial Data Preparation Complete ---\")\n            print(f\"DataFrame shape after initial processing: {df.shape}\")\n            print(\"Columns:\", df.columns.tolist())\n            print(\"\\nFirst 5 rows:\")\n            print(df.head())\n        return df\n\n    except FileNotFoundError:\n        print(f\"Error: Input data file not found at {filepath}\")\n        return None\n    except ValueError as ve:\n        print(f\"ValueError during data preparation: {ve}\")\n        return None\n    except Exception as e:\n        print(f\"An unexpected error occurred during data loading/preparation: {e}\")\n        return None\n\n# --- Optional: Function Placeholder for Merging External Data ---\ndef merge_external_data(df, external_files_dict):\n    \"\"\"\n    Loads and merges external data (e.g., Caloosahatchee Q/NOx) into the main dataframe.\n    Args:\n        df (pd.DataFrame): The main dataframe with datetime index.\n        external_files_dict (dict): Dictionary where keys are column names (e.g., 'caloos_Q')\n                                     and values are file paths to the external CSV data.\n                                     External CSVs must have a compatible datetime index.\n    Returns:\n        pd.DataFrame: DataFrame with external data merged (left join).\n    \"\"\"\n    print(\"\\n--- Merging External Data (Placeholder) ---\")\n    if not external_files_dict:\n        print(\"No external data files specified.\")\n        return df\n\n    df_merged = df.copy()\n    for col_name, file_path in external_files_dict.items():\n        try:\n            print(f\"Loading external data for '{col_name}' from '{file_path}'...\")\n            # Assuming external CSV has datetime index named same as DATETIME_COL or is the index\n            df_ext = pd.read_csv(file_path, index_col=DATETIME_COL, parse_dates=True) # Adjust index_col if needed\n            df_ext = df_ext[[col_name]] # Keep only the specified column\n            # Perform left merge\n            df_merged = df_merged.merge(df_ext, left_index=True, right_index=True, how='left')\n            print(f\"Merged '{col_name}'. NaN count: {df_merged[col_name].isnull().sum()}\")\n        except FileNotFoundError:\n            print(f\"Warning: External data file not found: {file_path}. Skipping '{col_name}'.\")\n        except KeyError:\n             print(f\"Warning: Column '{col_name}' not found in file {file_path}. Skipping.\")\n        except Exception as e:\n            print(f\"Error merging external file {file_path}: {e}\")\n\n    print(\"External data merging complete.\")\n    return df_merged\n\n\n# --- Execute Data Loading and Preparation ---\n# Ensure config variables from Part 1 are defined before running this\nif __name__ == \"__main__\":\n    try:\n        df_initial = load_and_prepare_data(\n            INPUT_DATA_PATH,\n            DATETIME_COL,\n            TARGET_COL,\n            BLOOM_THRESHOLD,\n            TARGET_BINARY_COL\n        )\n\n        if df_initial is not None:\n            # Display basic info about the prepared dataframe\n            print(\"\\n--- Dataframe after initial processing (df_initial) ---\")\n            df_initial.info()\n        else:\n            print(\"\\nData loading/preparation failed.\")\n\n    except NameError as ne:\n         print(f\"NameError: A configuration variable might be missing. Did Part 1 run correctly? Error: {ne}\")\n    except Exception as e:\n         print(f\"An unexpected error occurred in main execution block: {e}\")\n\n","identifier":"jamzelqjjs5m-code","enumerator":"6","html_id":"jamzelqjjs5m-code","key":"ROb2dTpTsd"},{"type":"outputs","id":"U73Qsum23i0vcmqxawUVI","children":[{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"Loading data from: data_weekly_intepolated.csv\nData loaded successfully. Shape: (1617, 9), Time range: 1993-01-04 00:00:00 to 2023-12-25 00:00:00\nInitial NaN counts:\nkb                 0\nzos                0\nsalinity           0\nwater_temp         0\nwind_direction     0\nwind_speed         0\npeace_discharge    0\npeace_TN           0\npeace_TP           0\ndtype: int64\nCalculating wind U/V components...\nCalculated wind U/V components and dropped original columns.\nCreating binary target 'bloom_target' using threshold 100000 cells/L...\nTarget distribution (%):\nbloom_target\n0    73.09833\n1    26.90167\nName: proportion, dtype: float64\n\n--- Initial Data Preparation Complete ---\nDataFrame shape after initial processing: (1617, 10)\nColumns: ['kb', 'zos', 'salinity', 'water_temp', 'peace_discharge', 'peace_TN', 'peace_TP', 'wind_u', 'wind_v', 'bloom_target']\n\nFirst 5 rows:\n               kb       zos   salinity  water_temp  peace_discharge  peace_TN  \\\ntime                                                                            \n1993-01-04  333.0  0.012906  33.043478        26.8            202.0    8.2000   \n1993-01-11  667.0  0.015614  33.065217        27.0            423.0   10.1000   \n1993-01-18  667.0  0.021702  33.086957        27.1           1470.0   12.0000   \n1993-01-25    0.0  0.015950  33.108696        26.8           1450.0   10.0475   \n1993-02-01    0.0  0.008977  33.130435        26.5           1490.0    8.0950   \n\n            peace_TP     wind_u     wind_v  bloom_target  \ntime                                                      \n1993-01-04  1.999091  -8.170215 -11.245336             0  \n1993-01-11  1.934545 -14.303751   7.605439             0  \n1993-01-18  1.870000 -15.407116   5.006075             0  \n1993-01-25  1.870500 -11.840127   4.309454             0  \n1993-02-01  1.871000  -4.233633 -16.980175             0  \n\n--- Dataframe after initial processing (df_initial) ---\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 1617 entries, 1993-01-04 to 2023-12-25\nData columns (total 10 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   kb               1617 non-null   float64\n 1   zos              1617 non-null   float64\n 2   salinity         1617 non-null   float64\n 3   water_temp       1617 non-null   float64\n 4   peace_discharge  1617 non-null   float64\n 5   peace_TN         1617 non-null   float64\n 6   peace_TP         1617 non-null   float64\n 7   wind_u           1617 non-null   float64\n 8   wind_v           1617 non-null   float64\n 9   bloom_target     1617 non-null   int64  \ndtypes: float64(9), int64(1)\nmemory usage: 139.0 KB\n"},"children":[],"identifier":"jamzelqjjs5m-outputs-0","html_id":"jamzelqjjs5m-outputs-0","key":"Mto9FctYrW"}],"identifier":"jamzelqjjs5m-outputs","html_id":"jamzelqjjs5m-outputs","key":"Mqf16nWN71"}],"identifier":"jamzelqjjs5m","label":"jaMZelqJjS5m","html_id":"jamzelqjjs5m","key":"uJmp5K6qLO"},{"type":"block","kind":"notebook-content","data":{"id":"Fdv8RABhjnY3"},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Part 3: Feature Engineering","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qkm6mVQ99m"}],"identifier":"part-3-feature-engineering","label":"Part 3: Feature Engineering","html_id":"part-3-feature-engineering","implicit":true,"key":"XMlyN08RCK"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"This part defines and executes a function create_features that adds lagged features to the initially processed DataFrame (df_initial from Part 2). It uses the configuration flags set in Part 1 (USE_ENHANCED_FEATURES) to determine whether to create basic lags or the more detailed lags inspired by the Random Forest paper.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Drg65Se57d"}],"key":"UQk16jzsKI"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"This cell defines the feature engineering function and applies it to the df_initial DataFrame. It creates lagged versions of environmental variables and, optionally, more detailed lagged and rolling aggregate features for K. brevis counts.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"vO99s7sQdo"}],"key":"HrDJg54atC"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Explanation:","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"aGvQnQUcsZ"}],"key":"YxMq8xFcwm"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":9,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"Configuration: It first defines or ensures access to the necessary configuration variables from Part 1 (like USE_ENHANCED_FEATURES, lag definitions, feature lists).","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"UyFpKZFosn"}],"key":"WFnPe1Sten"}],"key":"tnqOyJAODB"},{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"create_features(...) Function:","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"bGS7NYI7yc"}],"key":"ggI8XNgsfs"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":11,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Takes the initial DataFrame (df_initial) and configuration details as input.","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"zhUCWRP12u"}],"key":"znggU8orxX"}],"key":"LQ5ZcrXMo7"},{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Selects either BASIC_LAG_CONFIG or ENHANCED_LAG_CONFIG based on the USE_ENHANCED_FEATURES flag.","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"XoStYYivkS"}],"key":"H0Hf7VK3QY"}],"key":"uTKBIjqGBz"},{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"KB Lags: Creates simple weekly lags (_L1, _L2) for the raw kb column.","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"oqyFHTFVLH"}],"key":"f60vqb09qN"}],"key":"IX8viFBAOD"},{"type":"listItem","spread":true,"position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"KB Rolling Aggregates (Enhanced Only): Calculates rolling mean (_M1_mean, _M2_mean, etc.) and rolling proportion of bloom weeks (_M1_prop, _M2_prop, etc.) over specified windows (e.g., 4, 8, 12 weeks), shifted back appropriately.","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"wiSuNiHcL0"}],"key":"dQ5WWijlfO"}],"key":"zq5bhbNary"},{"type":"listItem","spread":true,"position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Discharge Rolling Average (Enhanced Only): Calculates a rolling average (e.g., 4-week) for the primary discharge column, lagged by 1 week.","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"GlJFspLX4N"}],"key":"LFKY1yAJDU"}],"key":"laz9kdyu6d"},{"type":"listItem","spread":true,"position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Environmental Lags: Creates simple weekly lags for all other specified environmental/hydrological variables (BASE_FEATURES). The number of lags depends on whether basic or enhanced mode is selected.","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"hJIFvkOoKY"}],"key":"jPH1AML8Bx"}],"key":"kxDKxugoiI"},{"type":"listItem","spread":true,"position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"NaN Handling: Tracks the maximum lag introduced by any operation and drops the corresponding number of initial rows from the DataFrame to ensure sequences are complete.","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"txGil2LOjm"}],"key":"sx4QVC5C4I"}],"key":"tSwlJXiGSd"},{"type":"listItem","spread":true,"position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"All-NaN Column Check: Includes a check to identify and optionally drop columns that might become entirely NaN after lagging (important for sparse data).","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"gLtcuOU6pc"}],"key":"BGwAXr5MCB"}],"key":"sKx3Aj18yi"},{"type":"listItem","spread":true,"position":{"start":{"line":19,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Returns the final DataFrame (df_processed) with all engineered features.","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"iV5U77cGoQ"}],"key":"rMoQV5eXmB"}],"key":"WsIiQODLhk"}],"key":"KePZ3QzBG7"}],"key":"Pmtp9grDeM"},{"type":"listItem","spread":true,"position":{"start":{"line":21,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"Main Execution: Calls the create_features function with the appropriate arguments based on the configuration flags. Prints the head and tail of the resulting df_processed DataFrame.","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"DTQtnPy1On"}],"key":"T2UKuE7bnB"}],"key":"gaDlhiAt13"}],"key":"IoVA7lcwBb"},{"type":"paragraph","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"After running this cell, df_processed will contain the data ready for splitting (Part 4) and subsequent preprocessing/modeling steps. The number of columns will vary depending on whether basic or enhanced features were generated.","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"mKFh9uCwZA"}],"key":"hhPCWOf36p"}],"identifier":"fdv8rabhjny3","label":"Fdv8RABhjnY3","html_id":"fdv8rabhjny3","key":"TRNAyU20W5"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"DswtKiJRjo8j","outputId":"472733a1-fd16-4a85-c5ff-3283a3109454","ExecuteTime":{"end_time":"2026-01-07T03:44:43.373949800Z","start_time":"2026-01-07T03:37:30.134123Z"}},"children":[{"type":"code","lang":"python","executable":true,"value":"# --- Configuration (Ensure these are defined from Part 1 or redefined here) ---\nTARGET_COL = 'kb'\nTARGET_BINARY_COL = 'bloom_target'\nUSE_ENHANCED_FEATURES = True # Set based on Part 1 config\n\n# Define lag configurations (can be pulled from Part 1 config)\nBASIC_LAG_CONFIG = {\n    'kb_lags': [1, 2],\n    'env_lags': list(range(1, 7))\n}\nENHANCED_LAG_CONFIG = {\n    'kb_lags': [1, 2],\n    'kb_rolling_windows': [(1, 4), (5, 8), (9, 12)], # (start_lag, end_lag)\n    'discharge_rolling_window': 4, # Window size for rolling avg discharge\n    'env_lags': [1] # Lags for other env vars in enhanced mode\n}\n# Features to lag (environmental/hydrological) - should include wind components if created\nBASE_FEATURES = [\n    'zos', 'water_temp',\n    'peace_discharge', 'peace_TN', 'peace_TP',\n    'wind_u', 'wind_v'\n]\n\n# --- Function Definition ---\ndef create_features(df, target_col, target_binary_col, base_features, use_enhanced=False, basic_cfg=None, enhanced_cfg=None):\n    \"\"\"Creates lagged and rolling features based on configuration.\"\"\"\n    if df is None:\n        print(\"Error in create_features: Input DataFrame is None.\")\n        return None\n\n    print(f\"\\n--- Creating Features (Enhanced Mode: {use_enhanced}) ---\")\n    df_featured = df.copy()\n    all_created_feature_cols = list(df.columns) # Start with existing columns\n\n    # Select config based on flag\n    if use_enhanced:\n        cfg = enhanced_cfg if enhanced_cfg else ENHANCED_LAG_CONFIG\n        env_lags = cfg.get('env_lags', [1])\n        kb_lags = cfg.get('kb_lags', [])\n        kb_rolling_windows = cfg.get('kb_rolling_windows', [])\n        discharge_rolling_window = cfg.get('discharge_rolling_window', None)\n        print(\"Using ENHANCED feature configuration.\")\n    else:\n        cfg = basic_cfg if basic_cfg else BASIC_LAG_CONFIG\n        env_lags = cfg.get('env_lags', [])\n        kb_lags = cfg.get('kb_lags', [])\n        kb_rolling_windows = [] # No rolling features in basic mode\n        discharge_rolling_window = None\n        print(\"Using BASIC feature configuration.\")\n\n    max_lag_needed = 0 # Track the maximum lag introduced\n\n    # --- Lagged K. brevis Features (Raw Counts) ---\n    if target_col in df.columns and kb_lags:\n        print(f\"Creating lagged features for target: {target_col}...\")\n        df[target_col] = pd.to_numeric(df[target_col], errors='coerce')\n        for lag in kb_lags:\n            col_name = f'{target_col}_L{lag}'\n            df_featured[col_name] = df[target_col].shift(lag)\n            all_created_feature_cols.append(col_name)\n            max_lag_needed = max(max_lag_needed, lag)\n        print(f\"  Created KB weekly lags: L{', L'.join(map(str, kb_lags))}\")\n    elif kb_lags:\n        print(f\"Warning: Target column '{target_col}' not found for lagging.\")\n\n    # --- Rolling K. brevis Features (Enhanced Mode Only) ---\n    if use_enhanced and kb_rolling_windows and target_col in df.columns and target_binary_col in df.columns:\n        print(\"Creating rolling aggregate features for KB...\")\n        target_binary_1e5 = df[target_binary_col]\n        monthly_lags_created = []\n        max_roll_lag = 0\n        for i, (start_lag, end_lag) in enumerate(kb_rolling_windows):\n            window_size = end_lag - start_lag + 1\n            month_lag_id = f'M{i+1}'\n            monthly_lags_created.append(month_lag_id)\n            max_roll_lag = max(max_roll_lag, end_lag) # Max lag needed for rolling window\n\n            # Mean of raw kb over the window, shifted back\n            col_name_mean = f'{target_col}_{month_lag_id}_mean'\n            df_featured[col_name_mean] = df[target_col].rolling(window=window_size, min_periods=1).mean().shift(start_lag)\n            all_created_feature_cols.append(col_name_mean)\n\n            # Proportion of bloom weeks (target=1) over the window, shifted back\n            col_name_prop = f'{target_binary_col}_{month_lag_id}_prop'\n            df_featured[col_name_prop] = target_binary_1e5.rolling(window=window_size, min_periods=1).mean().shift(start_lag)\n            all_created_feature_cols.append(col_name_prop)\n        print(f\"  Created approx KB monthly lags (mean, prop_bloom): {', '.join(monthly_lags_created)}\")\n        max_lag_needed = max(max_lag_needed, max_roll_lag) # Rolling needs lookback up to end_lag\n\n    # --- Rolling Discharge Feature (Enhanced Mode Only) ---\n    discharge_col = 'peace_discharge' # Or potentially 'caloos_Q' if prioritized/available\n    if use_enhanced and discharge_rolling_window and discharge_col in df.columns:\n         print(f\"Creating rolling average feature for {discharge_col}...\")\n         window_size = discharge_rolling_window\n         # Typically want average over past month, lagged by 1 week\n         start_lag = 1\n         col_name_discharge_roll = f'{discharge_col}_{window_size}w_avg_L{start_lag}'\n         df_featured[col_name_discharge_roll] = df[discharge_col].rolling(window=window_size, min_periods=1).mean().shift(start_lag)\n         all_created_feature_cols.append(col_name_discharge_roll)\n         max_lag_needed = max(max_lag_needed, window_size + start_lag -1) # Max lookback needed\n         print(f\"  Created {col_name_discharge_roll}\")\n\n\n    # --- Lagged Environmental/Hydrological Features ---\n    if env_lags:\n        print(\"\\nCreating lagged environmental/hydrological features...\")\n        lagged_env_cols_added_names = []\n        max_env_lag = 0\n        for feature in base_features:\n             if feature in df.columns:\n                df[feature] = pd.to_numeric(df[feature], errors='coerce') # Ensure numeric\n                for lag in env_lags:\n                    col_name = f'{feature}_L{lag}'\n                    df_featured[col_name] = df[feature].shift(lag)\n                    all_created_feature_cols.append(col_name)\n                    max_env_lag = max(max_env_lag, lag)\n                lagged_env_cols_added_names.append(feature)\n             else:\n                print(f\"  Warning: Feature '{feature}' not found for lagging.\")\n        if lagged_env_cols_added_names:\n            print(f\"  Lagged features created for: {', '.join(lagged_env_cols_added_names)} using lags L{', L'.join(map(str, env_lags))}\")\n        max_lag_needed = max(max_lag_needed, max_env_lag)\n\n    # --- Drop Rows with NaNs from Lagging/Rolling ---\n    print(f\"\\nMaximum lag/window introduced: {max_lag_needed} weeks.\")\n    initial_rows = len(df_featured)\n    if max_lag_needed > 0:\n        df_processed = df_featured.iloc[max_lag_needed:].copy()\n        print(f\"Dropped first {max_lag_needed} rows due to NaNs generated by feature engineering.\")\n        print(f\"Shape after dropping initial NaNs: {df_processed.shape}\")\n    else:\n        df_processed = df_featured.copy()\n        print(\"No lagging applied or max lag was 0, no initial rows dropped.\")\n\n    # Final check for all-NaN columns (can happen with rolling/shifting if data is sparse)\n    all_nan_cols = df_processed.columns[df_processed.isnull().all()].tolist()\n    if all_nan_cols:\n        print(f\"\\nWarning: The following columns consist entirely of NaNs and will be dropped: {all_nan_cols}\")\n        df_processed = df_processed.dropna(axis=1, how='all')\n        print(f\"Shape after dropping all-NaN columns: {df_processed.shape}\")\n\n    # Ensure no duplicate columns (though unlikely with this structure)\n    df_processed = df_processed.loc[:,~df_processed.columns.duplicated()]\n\n    print(\"\\n--- Feature Engineering Complete ---\")\n    print(f\"Final number of columns (features + targets): {len(df_processed.columns)}\")\n    return df_processed\n\n# --- Execute Feature Engineering ---\n# Ensure df_initial exists from Part 2 and config variables from Part 1\nif __name__ == \"__main__\":\n    try:\n        if 'df_initial' in locals() and df_initial is not None:\n            # Determine which config to use based on the flag from Part 1\n            lag_config_to_use = ENHANCED_LAG_CONFIG if USE_ENHANCED_FEATURES else BASIC_LAG_CONFIG\n            features_to_lag_list = BASE_FEATURES # Modify if external data was added\n\n            df_processed = create_features(\n                df_initial,\n                TARGET_COL,\n                TARGET_BINARY_COL,\n                features_to_lag_list,\n                use_enhanced=USE_ENHANCED_FEATURES,\n                basic_cfg=BASIC_LAG_CONFIG,\n                enhanced_cfg=ENHANCED_LAG_CONFIG\n            )\n\n            if df_processed is not None:\n                print(\"\\nFirst 5 rows of processed data (df_processed):\")\n                display(df_processed.head())\n                print(\"\\nLast 5 rows:\")\n                display(df_processed.tail())\n                print(f\"\\nFinal shape of df_processed: {df_processed.shape}\")\n            else:\n                print(\"\\nFeature engineering failed.\")\n        else:\n            print(\"\\nError: df_initial not found or is None. Please run Part 2 first.\")\n\n    except NameError as ne:\n         print(f\"NameError: A configuration variable might be missing. Did Part 1 run correctly? Error: {ne}\")\n    except Exception as e:\n         print(f\"An unexpected error occurred in main execution block: {e}\")\n\n","identifier":"dswtkijrjo8j-code","enumerator":"7","html_id":"dswtkijrjo8j-code","key":"ir0bYTe2ul"},{"type":"outputs","id":"8jUjCNFolzO_XTOBImV63","children":[{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"\n--- Creating Features (Enhanced Mode: True) ---\nUsing ENHANCED feature configuration.\nCreating lagged features for target: kb...\n  Created KB weekly lags: L1, L2\nCreating rolling aggregate features for KB...\n  Created approx KB monthly lags (mean, prop_bloom): M1, M2, M3\nCreating rolling average feature for peace_discharge...\n  Created peace_discharge_4w_avg_L1\n\nCreating lagged environmental/hydrological features...\n  Lagged features created for: zos, water_temp, peace_discharge, peace_TN, peace_TP, wind_u, wind_v using lags L1\n\nMaximum lag/window introduced: 12 weeks.\nDropped first 12 rows due to NaNs generated by feature engineering.\nShape after dropping initial NaNs: (1605, 26)\n\n--- Feature Engineering Complete ---\nFinal number of columns (features + targets): 26\n\nFirst 5 rows of processed data (df_processed):\n"},"children":[],"identifier":"dswtkijrjo8j-outputs-0","html_id":"dswtkijrjo8j-outputs-0","key":"swe0BztNt2"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"             kb       zos   salinity  water_temp  peace_discharge  peace_TN  \\\ntime                                                                          \n1993-03-29  0.0 -0.060247  33.304348        26.2           2000.0      2.02   \n1993-04-05  0.0 -0.037195  33.326087        26.4           3290.0      5.68   \n1993-04-12  0.0 -0.008214  33.347826        26.3           3140.0      9.34   \n1993-04-19  0.0 -0.012612  33.369565        26.5           2040.0     13.00   \n1993-04-26  0.0 -0.034968  33.391304        26.9           1800.0      9.30   \n\n            peace_TP     wind_u     wind_v  bloom_target  ...  kb_M3_mean  \\\ntime                                                      ...               \n1993-03-29  1.504000 -11.346331   9.188078             0  ...      416.75   \n1993-04-05  1.442667 -17.383315  -2.753247             0  ...      333.50   \n1993-04-12  1.381333 -12.056565   9.419621             0  ...      166.75   \n1993-04-19  1.320000  -9.191863 -10.954436             0  ...        0.00   \n1993-04-26  1.440000 -12.943406   3.468175             0  ...        0.00   \n\n            bloom_target_M3_prop  peace_discharge_4w_avg_L1    zos_L1  \\\ntime                                                                    \n1993-03-29                   0.0                     1822.5 -0.061417   \n1993-04-05                   0.0                     2050.0 -0.060247   \n1993-04-12                   0.0                     2600.0 -0.037195   \n1993-04-19                   0.0                     2745.0 -0.008214   \n1993-04-26                   0.0                     2617.5 -0.012612   \n\n            water_temp_L1  peace_discharge_L1  peace_TN_L1  peace_TP_L1  \\\ntime                                                                      \n1993-03-29           25.5              2550.0        1.948     1.565333   \n1993-04-05           26.2              2000.0        2.020     1.504000   \n1993-04-12           26.4              3290.0        5.680     1.442667   \n1993-04-19           26.3              3140.0        9.340     1.381333   \n1993-04-26           26.5              2040.0       13.000     1.320000   \n\n            wind_u_L1  wind_v_L1  \ntime                              \n1993-03-29 -17.499440   5.685913  \n1993-04-05 -11.346331   9.188078  \n1993-04-12 -17.383315  -2.753247  \n1993-04-19 -12.056565   9.419621  \n1993-04-26  -9.191863 -10.954436  \n\n[5 rows x 26 columns]","content_type":"text/plain"},"text/html":{"content":"\n  <div id=\"df-3a899256-df90-46c0-87fb-836f1b7e7467\" class=\"colab-df-container\">\n    <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>kb</th>\n      <th>zos</th>\n      <th>salinity</th>\n      <th>water_temp</th>\n      <th>peace_discharge</th>\n      <th>peace_TN</th>\n      <th>peace_TP</th>\n      <th>wind_u</th>\n      <th>wind_v</th>\n      <th>bloom_target</th>\n      <th>...</th>\n      <th>kb_M3_mean</th>\n      <th>bloom_target_M3_prop</th>\n      <th>peace_discharge_4w_avg_L1</th>\n      <th>zos_L1</th>\n      <th>water_temp_L1</th>\n      <th>peace_discharge_L1</th>\n      <th>peace_TN_L1</th>\n      <th>peace_TP_L1</th>\n      <th>wind_u_L1</th>\n      <th>wind_v_L1</th>\n    </tr>\n    <tr>\n      <th>time</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1993-03-29</th>\n      <td>0.0</td>\n      <td>-0.060247</td>\n      <td>33.304348</td>\n      <td>26.2</td>\n      <td>2000.0</td>\n      <td>2.02</td>\n      <td>1.504000</td>\n      <td>-11.346331</td>\n      <td>9.188078</td>\n      <td>0</td>\n      <td>...</td>\n      <td>416.75</td>\n      <td>0.0</td>\n      <td>1822.5</td>\n      <td>-0.061417</td>\n      <td>25.5</td>\n      <td>2550.0</td>\n      <td>1.948</td>\n      <td>1.565333</td>\n      <td>-17.499440</td>\n      <td>5.685913</td>\n    </tr>\n    <tr>\n      <th>1993-04-05</th>\n      <td>0.0</td>\n      <td>-0.037195</td>\n      <td>33.326087</td>\n      <td>26.4</td>\n      <td>3290.0</td>\n      <td>5.68</td>\n      <td>1.442667</td>\n      <td>-17.383315</td>\n      <td>-2.753247</td>\n      <td>0</td>\n      <td>...</td>\n      <td>333.50</td>\n      <td>0.0</td>\n      <td>2050.0</td>\n      <td>-0.060247</td>\n      <td>26.2</td>\n      <td>2000.0</td>\n      <td>2.020</td>\n      <td>1.504000</td>\n      <td>-11.346331</td>\n      <td>9.188078</td>\n    </tr>\n    <tr>\n      <th>1993-04-12</th>\n      <td>0.0</td>\n      <td>-0.008214</td>\n      <td>33.347826</td>\n      <td>26.3</td>\n      <td>3140.0</td>\n      <td>9.34</td>\n      <td>1.381333</td>\n      <td>-12.056565</td>\n      <td>9.419621</td>\n      <td>0</td>\n      <td>...</td>\n      <td>166.75</td>\n      <td>0.0</td>\n      <td>2600.0</td>\n      <td>-0.037195</td>\n      <td>26.4</td>\n      <td>3290.0</td>\n      <td>5.680</td>\n      <td>1.442667</td>\n      <td>-17.383315</td>\n      <td>-2.753247</td>\n    </tr>\n    <tr>\n      <th>1993-04-19</th>\n      <td>0.0</td>\n      <td>-0.012612</td>\n      <td>33.369565</td>\n      <td>26.5</td>\n      <td>2040.0</td>\n      <td>13.00</td>\n      <td>1.320000</td>\n      <td>-9.191863</td>\n      <td>-10.954436</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>2745.0</td>\n      <td>-0.008214</td>\n      <td>26.3</td>\n      <td>3140.0</td>\n      <td>9.340</td>\n      <td>1.381333</td>\n      <td>-12.056565</td>\n      <td>9.419621</td>\n    </tr>\n    <tr>\n      <th>1993-04-26</th>\n      <td>0.0</td>\n      <td>-0.034968</td>\n      <td>33.391304</td>\n      <td>26.9</td>\n      <td>1800.0</td>\n      <td>9.30</td>\n      <td>1.440000</td>\n      <td>-12.943406</td>\n      <td>3.468175</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>2617.5</td>\n      <td>-0.012612</td>\n      <td>26.5</td>\n      <td>2040.0</td>\n      <td>13.000</td>\n      <td>1.320000</td>\n      <td>-9.191863</td>\n      <td>-10.954436</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 26 columns</p>\n</div>\n    <div class=\"colab-df-buttons\">\n\n  <div class=\"colab-df-container\">\n    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3a899256-df90-46c0-87fb-836f1b7e7467')\"\n            title=\"Convert this dataframe to an interactive table.\"\n            style=\"display:none;\">\n\n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n  </svg>\n    </button>\n\n  <style>\n    .colab-df-container {\n      display:flex;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    .colab-df-buttons div {\n      margin-bottom: 4px;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n    <script>\n      const buttonEl =\n        document.querySelector('#df-3a899256-df90-46c0-87fb-836f1b7e7467 button.colab-df-convert');\n      buttonEl.style.display =\n        google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n      async function convertToInteractive(key) {\n        const element = document.querySelector('#df-3a899256-df90-46c0-87fb-836f1b7e7467');\n        const dataTable =\n          await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                    [key], {});\n        if (!dataTable) return;\n\n        const docLinkHtml = 'Like what you see? Visit the ' +\n          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n          + ' to learn more about interactive tables.';\n        element.innerHTML = '';\n        dataTable['output_type'] = 'display_data';\n        await google.colab.output.renderOutput(dataTable, element);\n        const docLink = document.createElement('div');\n        docLink.innerHTML = docLinkHtml;\n        element.appendChild(docLink);\n      }\n    </script>\n  </div>\n\n\n    <div id=\"df-ddfb3322-816a-4f7d-b66d-f681ce2773d3\">\n      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ddfb3322-816a-4f7d-b66d-f681ce2773d3')\"\n                title=\"Suggest charts\"\n                style=\"display:none;\">\n\n<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n     width=\"24px\">\n    <g>\n        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n    </g>\n</svg>\n      </button>\n\n<style>\n  .colab-df-quickchart {\n      --bg-color: #E8F0FE;\n      --fill-color: #1967D2;\n      --hover-bg-color: #E2EBFA;\n      --hover-fill-color: #174EA6;\n      --disabled-fill-color: #AAA;\n      --disabled-bg-color: #DDD;\n  }\n\n  [theme=dark] .colab-df-quickchart {\n      --bg-color: #3B4455;\n      --fill-color: #D2E3FC;\n      --hover-bg-color: #434B5C;\n      --hover-fill-color: #FFFFFF;\n      --disabled-bg-color: #3B4455;\n      --disabled-fill-color: #666;\n  }\n\n  .colab-df-quickchart {\n    background-color: var(--bg-color);\n    border: none;\n    border-radius: 50%;\n    cursor: pointer;\n    display: none;\n    fill: var(--fill-color);\n    height: 32px;\n    padding: 0;\n    width: 32px;\n  }\n\n  .colab-df-quickchart:hover {\n    background-color: var(--hover-bg-color);\n    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n    fill: var(--button-hover-fill-color);\n  }\n\n  .colab-df-quickchart-complete:disabled,\n  .colab-df-quickchart-complete:disabled:hover {\n    background-color: var(--disabled-bg-color);\n    fill: var(--disabled-fill-color);\n    box-shadow: none;\n  }\n\n  .colab-df-spinner {\n    border: 2px solid var(--fill-color);\n    border-color: transparent;\n    border-bottom-color: var(--fill-color);\n    animation:\n      spin 1s steps(1) infinite;\n  }\n\n  @keyframes spin {\n    0% {\n      border-color: transparent;\n      border-bottom-color: var(--fill-color);\n      border-left-color: var(--fill-color);\n    }\n    20% {\n      border-color: transparent;\n      border-left-color: var(--fill-color);\n      border-top-color: var(--fill-color);\n    }\n    30% {\n      border-color: transparent;\n      border-left-color: var(--fill-color);\n      border-top-color: var(--fill-color);\n      border-right-color: var(--fill-color);\n    }\n    40% {\n      border-color: transparent;\n      border-right-color: var(--fill-color);\n      border-top-color: var(--fill-color);\n    }\n    60% {\n      border-color: transparent;\n      border-right-color: var(--fill-color);\n    }\n    80% {\n      border-color: transparent;\n      border-right-color: var(--fill-color);\n      border-bottom-color: var(--fill-color);\n    }\n    90% {\n      border-color: transparent;\n      border-bottom-color: var(--fill-color);\n    }\n  }\n</style>\n\n      <script>\n        async function quickchart(key) {\n          const quickchartButtonEl =\n            document.querySelector('#' + key + ' button');\n          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n          quickchartButtonEl.classList.add('colab-df-spinner');\n          try {\n            const charts = await google.colab.kernel.invokeFunction(\n                'suggestCharts', [key], {});\n          } catch (error) {\n            console.error('Error during call to suggestCharts:', error);\n          }\n          quickchartButtonEl.classList.remove('colab-df-spinner');\n          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n        }\n        (() => {\n          let quickchartButtonEl =\n            document.querySelector('#df-ddfb3322-816a-4f7d-b66d-f681ce2773d3 button');\n          quickchartButtonEl.style.display =\n            google.colab.kernel.accessAllowed ? 'block' : 'none';\n        })();\n      </script>\n    </div>\n\n    </div>\n  </div>\n","content_type":"text/html"},"application/vnd.google.colaboratory.intrinsic+json":{"content":"{\"type\":\"dataframe\"}","content_type":"application/vnd.google.colaboratory.intrinsic+json"}}},"children":[],"identifier":"dswtkijrjo8j-outputs-1","html_id":"dswtkijrjo8j-outputs-1","key":"Qph80aWLYs"},{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"\nLast 5 rows:\n"},"children":[],"identifier":"dswtkijrjo8j-outputs-2","html_id":"dswtkijrjo8j-outputs-2","key":"JucZSdWSSy"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"             kb       zos  salinity  water_temp  peace_discharge  peace_TN  \\\ntime                                                                         \n2023-11-27  0.0 -0.005114      36.0   27.110714            326.0    2.0675   \n2023-12-04  0.0 -0.003981      36.0   26.992857            231.0    2.1800   \n2023-12-11  0.0 -0.009013      36.0   26.528571            201.0    1.8400   \n2023-12-18  0.0  0.005557      36.0   26.207143            460.0    2.2080   \n2023-12-25  0.0  0.001243      36.0   26.053571            470.0    2.2080   \n\n            peace_TP     wind_u    wind_v  bloom_target  ...  kb_M3_mean  \\\ntime                                                     ...               \n2023-11-27    0.8365 -11.482282 -2.065989             0  ...         0.0   \n2023-12-04    0.8000 -12.302635  0.314994             0  ...         0.0   \n2023-12-11    0.7900 -12.016729 -1.298361             0  ...         0.0   \n2023-12-18    0.9700 -11.410499  4.251085             0  ...         0.0   \n2023-12-25    0.9700 -13.387089  0.506480             0  ...         0.0   \n\n            bloom_target_M3_prop  peace_discharge_4w_avg_L1    zos_L1  \\\ntime                                                                    \n2023-11-27                   0.0                     496.25  0.022788   \n2023-12-04                   0.0                     392.25 -0.005114   \n2023-12-11                   0.0                     323.25 -0.003981   \n2023-12-18                   0.0                     287.50 -0.009013   \n2023-12-25                   0.0                     304.50  0.005557   \n\n            water_temp_L1  peace_discharge_L1  peace_TN_L1  peace_TP_L1  \\\ntime                                                                      \n2023-11-27      27.271429               392.0       1.9550       0.8730   \n2023-12-04      27.110714               326.0       2.0675       0.8365   \n2023-12-11      26.992857               231.0       2.1800       0.8000   \n2023-12-18      26.528571               201.0       1.8400       0.7900   \n2023-12-25      26.207143               460.0       2.2080       0.9700   \n\n            wind_u_L1  wind_v_L1  \ntime                              \n2023-11-27 -11.853374   4.605556  \n2023-12-04 -11.482282  -2.065989  \n2023-12-11 -12.302635   0.314994  \n2023-12-18 -12.016729  -1.298361  \n2023-12-25 -11.410499   4.251085  \n\n[5 rows x 26 columns]","content_type":"text/plain"},"text/html":{"content":"\n  <div id=\"df-6262d544-c340-4b6b-9503-d9465d36cb77\" class=\"colab-df-container\">\n    <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>kb</th>\n      <th>zos</th>\n      <th>salinity</th>\n      <th>water_temp</th>\n      <th>peace_discharge</th>\n      <th>peace_TN</th>\n      <th>peace_TP</th>\n      <th>wind_u</th>\n      <th>wind_v</th>\n      <th>bloom_target</th>\n      <th>...</th>\n      <th>kb_M3_mean</th>\n      <th>bloom_target_M3_prop</th>\n      <th>peace_discharge_4w_avg_L1</th>\n      <th>zos_L1</th>\n      <th>water_temp_L1</th>\n      <th>peace_discharge_L1</th>\n      <th>peace_TN_L1</th>\n      <th>peace_TP_L1</th>\n      <th>wind_u_L1</th>\n      <th>wind_v_L1</th>\n    </tr>\n    <tr>\n      <th>time</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2023-11-27</th>\n      <td>0.0</td>\n      <td>-0.005114</td>\n      <td>36.0</td>\n      <td>27.110714</td>\n      <td>326.0</td>\n      <td>2.0675</td>\n      <td>0.8365</td>\n      <td>-11.482282</td>\n      <td>-2.065989</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>496.25</td>\n      <td>0.022788</td>\n      <td>27.271429</td>\n      <td>392.0</td>\n      <td>1.9550</td>\n      <td>0.8730</td>\n      <td>-11.853374</td>\n      <td>4.605556</td>\n    </tr>\n    <tr>\n      <th>2023-12-04</th>\n      <td>0.0</td>\n      <td>-0.003981</td>\n      <td>36.0</td>\n      <td>26.992857</td>\n      <td>231.0</td>\n      <td>2.1800</td>\n      <td>0.8000</td>\n      <td>-12.302635</td>\n      <td>0.314994</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>392.25</td>\n      <td>-0.005114</td>\n      <td>27.110714</td>\n      <td>326.0</td>\n      <td>2.0675</td>\n      <td>0.8365</td>\n      <td>-11.482282</td>\n      <td>-2.065989</td>\n    </tr>\n    <tr>\n      <th>2023-12-11</th>\n      <td>0.0</td>\n      <td>-0.009013</td>\n      <td>36.0</td>\n      <td>26.528571</td>\n      <td>201.0</td>\n      <td>1.8400</td>\n      <td>0.7900</td>\n      <td>-12.016729</td>\n      <td>-1.298361</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>323.25</td>\n      <td>-0.003981</td>\n      <td>26.992857</td>\n      <td>231.0</td>\n      <td>2.1800</td>\n      <td>0.8000</td>\n      <td>-12.302635</td>\n      <td>0.314994</td>\n    </tr>\n    <tr>\n      <th>2023-12-18</th>\n      <td>0.0</td>\n      <td>0.005557</td>\n      <td>36.0</td>\n      <td>26.207143</td>\n      <td>460.0</td>\n      <td>2.2080</td>\n      <td>0.9700</td>\n      <td>-11.410499</td>\n      <td>4.251085</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>287.50</td>\n      <td>-0.009013</td>\n      <td>26.528571</td>\n      <td>201.0</td>\n      <td>1.8400</td>\n      <td>0.7900</td>\n      <td>-12.016729</td>\n      <td>-1.298361</td>\n    </tr>\n    <tr>\n      <th>2023-12-25</th>\n      <td>0.0</td>\n      <td>0.001243</td>\n      <td>36.0</td>\n      <td>26.053571</td>\n      <td>470.0</td>\n      <td>2.2080</td>\n      <td>0.9700</td>\n      <td>-13.387089</td>\n      <td>0.506480</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>304.50</td>\n      <td>0.005557</td>\n      <td>26.207143</td>\n      <td>460.0</td>\n      <td>2.2080</td>\n      <td>0.9700</td>\n      <td>-11.410499</td>\n      <td>4.251085</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 26 columns</p>\n</div>\n    <div class=\"colab-df-buttons\">\n\n  <div class=\"colab-df-container\">\n    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6262d544-c340-4b6b-9503-d9465d36cb77')\"\n            title=\"Convert this dataframe to an interactive table.\"\n            style=\"display:none;\">\n\n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n  </svg>\n    </button>\n\n  <style>\n    .colab-df-container {\n      display:flex;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    .colab-df-buttons div {\n      margin-bottom: 4px;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n    <script>\n      const buttonEl =\n        document.querySelector('#df-6262d544-c340-4b6b-9503-d9465d36cb77 button.colab-df-convert');\n      buttonEl.style.display =\n        google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n      async function convertToInteractive(key) {\n        const element = document.querySelector('#df-6262d544-c340-4b6b-9503-d9465d36cb77');\n        const dataTable =\n          await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                    [key], {});\n        if (!dataTable) return;\n\n        const docLinkHtml = 'Like what you see? Visit the ' +\n          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n          + ' to learn more about interactive tables.';\n        element.innerHTML = '';\n        dataTable['output_type'] = 'display_data';\n        await google.colab.output.renderOutput(dataTable, element);\n        const docLink = document.createElement('div');\n        docLink.innerHTML = docLinkHtml;\n        element.appendChild(docLink);\n      }\n    </script>\n  </div>\n\n\n    <div id=\"df-8c5c49e3-2b72-43f8-b42a-d958cf65b92c\">\n      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8c5c49e3-2b72-43f8-b42a-d958cf65b92c')\"\n                title=\"Suggest charts\"\n                style=\"display:none;\">\n\n<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n     width=\"24px\">\n    <g>\n        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n    </g>\n</svg>\n      </button>\n\n<style>\n  .colab-df-quickchart {\n      --bg-color: #E8F0FE;\n      --fill-color: #1967D2;\n      --hover-bg-color: #E2EBFA;\n      --hover-fill-color: #174EA6;\n      --disabled-fill-color: #AAA;\n      --disabled-bg-color: #DDD;\n  }\n\n  [theme=dark] .colab-df-quickchart {\n      --bg-color: #3B4455;\n      --fill-color: #D2E3FC;\n      --hover-bg-color: #434B5C;\n      --hover-fill-color: #FFFFFF;\n      --disabled-bg-color: #3B4455;\n      --disabled-fill-color: #666;\n  }\n\n  .colab-df-quickchart {\n    background-color: var(--bg-color);\n    border: none;\n    border-radius: 50%;\n    cursor: pointer;\n    display: none;\n    fill: var(--fill-color);\n    height: 32px;\n    padding: 0;\n    width: 32px;\n  }\n\n  .colab-df-quickchart:hover {\n    background-color: var(--hover-bg-color);\n    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n    fill: var(--button-hover-fill-color);\n  }\n\n  .colab-df-quickchart-complete:disabled,\n  .colab-df-quickchart-complete:disabled:hover {\n    background-color: var(--disabled-bg-color);\n    fill: var(--disabled-fill-color);\n    box-shadow: none;\n  }\n\n  .colab-df-spinner {\n    border: 2px solid var(--fill-color);\n    border-color: transparent;\n    border-bottom-color: var(--fill-color);\n    animation:\n      spin 1s steps(1) infinite;\n  }\n\n  @keyframes spin {\n    0% {\n      border-color: transparent;\n      border-bottom-color: var(--fill-color);\n      border-left-color: var(--fill-color);\n    }\n    20% {\n      border-color: transparent;\n      border-left-color: var(--fill-color);\n      border-top-color: var(--fill-color);\n    }\n    30% {\n      border-color: transparent;\n      border-left-color: var(--fill-color);\n      border-top-color: var(--fill-color);\n      border-right-color: var(--fill-color);\n    }\n    40% {\n      border-color: transparent;\n      border-right-color: var(--fill-color);\n      border-top-color: var(--fill-color);\n    }\n    60% {\n      border-color: transparent;\n      border-right-color: var(--fill-color);\n    }\n    80% {\n      border-color: transparent;\n      border-right-color: var(--fill-color);\n      border-bottom-color: var(--fill-color);\n    }\n    90% {\n      border-color: transparent;\n      border-bottom-color: var(--fill-color);\n    }\n  }\n</style>\n\n      <script>\n        async function quickchart(key) {\n          const quickchartButtonEl =\n            document.querySelector('#' + key + ' button');\n          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n          quickchartButtonEl.classList.add('colab-df-spinner');\n          try {\n            const charts = await google.colab.kernel.invokeFunction(\n                'suggestCharts', [key], {});\n          } catch (error) {\n            console.error('Error during call to suggestCharts:', error);\n          }\n          quickchartButtonEl.classList.remove('colab-df-spinner');\n          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n        }\n        (() => {\n          let quickchartButtonEl =\n            document.querySelector('#df-8c5c49e3-2b72-43f8-b42a-d958cf65b92c button');\n          quickchartButtonEl.style.display =\n            google.colab.kernel.accessAllowed ? 'block' : 'none';\n        })();\n      </script>\n    </div>\n\n    </div>\n  </div>\n","content_type":"text/html"},"application/vnd.google.colaboratory.intrinsic+json":{"content":"{\"type\":\"dataframe\"}","content_type":"application/vnd.google.colaboratory.intrinsic+json"}}},"children":[],"identifier":"dswtkijrjo8j-outputs-3","html_id":"dswtkijrjo8j-outputs-3","key":"qW7xcHqZ5z"},{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"\nFinal shape of df_processed: (1605, 26)\n"},"children":[],"identifier":"dswtkijrjo8j-outputs-4","html_id":"dswtkijrjo8j-outputs-4","key":"V50Qv5qCnn"}],"identifier":"dswtkijrjo8j-outputs","html_id":"dswtkijrjo8j-outputs","key":"a8lfgHv8tl"}],"identifier":"dswtkijrjo8j","label":"DswtKiJRjo8j","html_id":"dswtkijrjo8j","key":"lMw67oMGp4"},{"type":"block","kind":"notebook-content","data":{"id":"SxrAEziyj0El"},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Part 4: Data Splitting (Chronological)","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"cQZxP6Nmzd"}],"identifier":"part-4-data-splitting-chronological","label":"Part 4: Data Splitting (Chronological)","html_id":"part-4-data-splitting-chronological","implicit":true,"key":"pOOHGPalGt"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"This part defines and executes a function to split the feature-engineered DataFrame (df_processed from Part 3) into training, validation, and testing sets based on time. A chronological split is essential for time-series data to ensure the model is trained on past data and evaluated on future data, simulating a real-world forecasting scenario.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"ThSRUOfgQD"}],"key":"TXWA1Yjio0"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"This cell defines the split_data_chronological function and applies it to df_processed. It uses the TRAIN_SPLIT_RATIO and VALIDATION_SPLIT_RATIO defined in Part 1 to divide the data.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"cCtU6iW1bE"}],"key":"eC0gEvrQks"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Explanation:","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"DfwdOosnos"}],"key":"r28W2epFgo"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":8,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"Function split_data_chronological:","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"pMgx3LKlAI"}],"key":"VdAugkqZfG"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":9,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Takes the feature-engineered DataFrame (df_processed) and the desired train/validation ratios as input.","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"bDtUTOLCIF"}],"key":"wVJQwu6qup"}],"key":"TR40rgAoV3"},{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Includes checks to ensure the input DataFrame is valid and has a datetime index.","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"IIHYA5GTlk"}],"key":"ptI5P72kUP"}],"key":"SURBKku3qQ"},{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Calculates the number of samples for each set based on the ratios.","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"mUeJ3uPLKY"}],"key":"fJpj2EIGxF"}],"key":"sz5UaWw7DE"},{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Performs the split using integer-location based indexing (iloc) which respects the chronological order. .copy() is used to avoid potential SettingWithCopyWarning later.","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"SEIsAnyrGH"}],"key":"IL4S0mTMkE"}],"key":"rm3XWxUUtX"},{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Prints the size and date range of each resulting subset.","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"kCUEilDuDA"}],"key":"UX2nQHkGcf"}],"key":"S4rQSy7wsl"},{"type":"listItem","spread":true,"position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Includes an assertion to double-check that the splits don’t overlap.","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"NGh2lK4hpJ"}],"key":"IynFN5J2Qj"}],"key":"q0kA3NoprR"},{"type":"listItem","spread":true,"position":{"start":{"line":15,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Returns the three DataFrames: train_df, validation_df, test_df.","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"OeDnRfGqh5"}],"key":"D5kBk7LT7E"}],"key":"jxax0ooDN4"}],"key":"Q8GVeXCc5l"}],"key":"s0E63O7t9l"},{"type":"listItem","spread":true,"position":{"start":{"line":17,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"text","value":"Main Execution:","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"kaEDLN4Fbf"}],"key":"CYlaKuJMcS"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":18,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Calls the function using df_processed (output of Part 3) and the ratios defined in the configuration (Part 1).","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"qrVBpWVgyE"}],"key":"Lle64OdEwL"}],"key":"gnUkZrnSHK"},{"type":"listItem","spread":true,"position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Stores the results in train_df, validation_df, and test_df. These variables will be used in subsequent steps (scaling, sequence creation, EnKF).","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"exdI6999xY"}],"key":"RmBoatnluj"}],"key":"Zwzy0oLBgn"},{"type":"listItem","spread":true,"position":{"start":{"line":20,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Prints the head of train_df for verification.","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"FJaLmvp6ZQ"}],"key":"Fhnnj7TMZO"}],"key":"GRlQRx5nrt"}],"key":"hQtXxDR7gh"}],"key":"tHtGYZIXMa"}],"key":"xPDLIjPV2o"},{"type":"paragraph","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"text","value":"After running this cell, you will have the data divided into the necessary subsets for training, validation (tuning/early stopping), and final testing, while preserving the temporal order. The next step (Part 5) will handle imputation and scaling, fitting the necessary objects only on train_df.","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"LHAqbyWYNe"}],"key":"KdrE6wM30g"}],"identifier":"sxraeziyj0el","label":"SxrAEziyj0El","html_id":"sxraeziyj0el","key":"WLYapxg1RE"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/","height":565},"id":"47ChAVmSjxe-","outputId":"54bdab40-61fe-4da3-b09b-f499bebb33f5","ExecuteTime":{"end_time":"2026-01-07T03:44:43.374949200Z","start_time":"2026-01-07T03:37:30.211789Z"}},"children":[{"type":"code","lang":"python","executable":true,"value":"# --- Configuration (Ensure these are defined from Part 1 or redefined here) ---\nTRAIN_SPLIT_RATIO = 0.70\nVALIDATION_SPLIT_RATIO = 0.15\n# Test split ratio is implicitly calculated\n\n# --- Function Definition ---\ndef split_data_chronological(df, train_ratio, val_ratio):\n    \"\"\"\n    Splits the DataFrame chronologically into train, validation, and test sets.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame with a datetime index, sorted chronologically.\n        train_ratio (float): The proportion of data to use for training (e.g., 0.7).\n        val_ratio (float): The proportion of data to use for validation (e.g., 0.15).\n\n    Returns:\n        tuple: A tuple containing train_df, validation_df, test_df (pd.DataFrames),\n               or (None, None, None) if splitting fails.\n    \"\"\"\n    if df is None or df.empty:\n        print(\"Error in split_data_chronological: Input DataFrame is None or empty.\")\n        return None, None, None\n    if not isinstance(df.index, pd.DatetimeIndex):\n         print(\"Error: DataFrame index must be a DatetimeIndex.\")\n         return None, None, None\n    if not df.index.is_monotonic_increasing:\n         print(\"Warning: DataFrame index is not sorted chronologically. Sorting now...\")\n         df = df.sort_index()\n\n    print(\"\\n--- Splitting Data into Train, Validation, and Test Sets (Chronological) ---\")\n    n_total = len(df)\n    n_train = int(n_total * train_ratio)\n    n_validation = int(n_total * val_ratio)\n    n_test = n_total - n_train - n_validation\n\n    # Ensure calculated splits are valid\n    if n_train <= 0 or n_validation <= 0 or n_test <= 0:\n         print(f\"Error: Not enough data ({n_total} rows) to create non-empty train/validation/test splits.\")\n         print(f\"Calculated splits: Train={n_train}, Val={n_validation}, Test={n_test}\")\n         return None, None, None\n    if n_train + n_validation + n_test != n_total:\n         print(\"Error: Split ratios do not sum correctly.\")\n         return None, None, None\n\n    # Perform chronological split using iloc\n    train_df = df.iloc[:n_train].copy()\n    validation_df = df.iloc[n_train : n_train + n_validation].copy()\n    test_df = df.iloc[n_train + n_validation :].copy() # Takes the rest\n\n    print(f\"Total samples: {n_total}\")\n    print(f\"Training set:   {len(train_df)} samples (Index: {train_df.index.min()} to {train_df.index.max()})\")\n    print(f\"Validation set: {len(validation_df)} samples (Index: {validation_df.index.min()} to {validation_df.index.max()})\")\n    print(f\"Test set:       {len(test_df)} samples (Index: {test_df.index.min()} to {test_df.index.max()})\")\n\n    # Basic check for overlap\n    assert train_df.index.max() < validation_df.index.min(), \"Train/Validation sets overlap!\"\n    assert validation_df.index.max() < test_df.index.min(), \"Validation/Test sets overlap!\"\n\n    print(\"Data splitting complete.\")\n    return train_df, validation_df, test_df\n\n# --- Execute Data Splitting ---\n# Ensure df_processed exists from Part 3\nif __name__ == \"__main__\":\n    try:\n        if 'df_processed' in locals() and df_processed is not None:\n            # Use ratios defined in Part 1 config\n            train_df, validation_df, test_df = split_data_chronological(\n                df_processed,\n                TRAIN_SPLIT_RATIO,\n                VALIDATION_SPLIT_RATIO\n            )\n\n            if train_df is not None:\n                # Display head of training data as confirmation\n                print(\"\\nHead of Training Data (train_df):\")\n                display(train_df.head())\n                # Keep test_df (unscaled) for later use with EnKF\n                print(\"\\nUnscaled test_df also created for EnKF observations.\")\n            else:\n                print(\"\\nData splitting failed.\")\n        else:\n            print(\"\\nError: df_processed not found or is None. Please run Part 3 first.\")\n            # Define placeholders to prevent errors if run out of order in notebook\n            train_df, validation_df, test_df = None, None, None\n\n    except NameError as ne:\n         print(f\"NameError: A configuration variable might be missing. Did Part 1 run correctly? Error: {ne}\")\n    except Exception as e:\n         print(f\"An unexpected error occurred in main execution block: {e}\")\n         train_df, validation_df, test_df = None, None, None\n\n","identifier":"47chavmsjxe--code","enumerator":"8","html_id":"id-47chavmsjxe-code","key":"D3rzRlmLtN"},{"type":"outputs","id":"RLzNgmOjO33uc2eLlMxlL","children":[{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"\n--- Splitting Data into Train, Validation, and Test Sets (Chronological) ---\nTotal samples: 1605\nTraining set:   1123 samples (Index: 1993-03-29 00:00:00 to 2014-09-29 00:00:00)\nValidation set: 240 samples (Index: 2014-10-06 00:00:00 to 2019-05-06 00:00:00)\nTest set:       242 samples (Index: 2019-05-13 00:00:00 to 2023-12-25 00:00:00)\nData splitting complete.\n\nHead of Training Data (train_df):\n"},"children":[],"identifier":"47chavmsjxe--outputs-0","html_id":"id-47chavmsjxe-outputs-0","key":"tOU8FUoAfj"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"             kb       zos   salinity  water_temp  peace_discharge  peace_TN  \\\ntime                                                                          \n1993-03-29  0.0 -0.060247  33.304348        26.2           2000.0      2.02   \n1993-04-05  0.0 -0.037195  33.326087        26.4           3290.0      5.68   \n1993-04-12  0.0 -0.008214  33.347826        26.3           3140.0      9.34   \n1993-04-19  0.0 -0.012612  33.369565        26.5           2040.0     13.00   \n1993-04-26  0.0 -0.034968  33.391304        26.9           1800.0      9.30   \n\n            peace_TP     wind_u     wind_v  bloom_target  ...  kb_M3_mean  \\\ntime                                                      ...               \n1993-03-29  1.504000 -11.346331   9.188078             0  ...      416.75   \n1993-04-05  1.442667 -17.383315  -2.753247             0  ...      333.50   \n1993-04-12  1.381333 -12.056565   9.419621             0  ...      166.75   \n1993-04-19  1.320000  -9.191863 -10.954436             0  ...        0.00   \n1993-04-26  1.440000 -12.943406   3.468175             0  ...        0.00   \n\n            bloom_target_M3_prop  peace_discharge_4w_avg_L1    zos_L1  \\\ntime                                                                    \n1993-03-29                   0.0                     1822.5 -0.061417   \n1993-04-05                   0.0                     2050.0 -0.060247   \n1993-04-12                   0.0                     2600.0 -0.037195   \n1993-04-19                   0.0                     2745.0 -0.008214   \n1993-04-26                   0.0                     2617.5 -0.012612   \n\n            water_temp_L1  peace_discharge_L1  peace_TN_L1  peace_TP_L1  \\\ntime                                                                      \n1993-03-29           25.5              2550.0        1.948     1.565333   \n1993-04-05           26.2              2000.0        2.020     1.504000   \n1993-04-12           26.4              3290.0        5.680     1.442667   \n1993-04-19           26.3              3140.0        9.340     1.381333   \n1993-04-26           26.5              2040.0       13.000     1.320000   \n\n            wind_u_L1  wind_v_L1  \ntime                              \n1993-03-29 -17.499440   5.685913  \n1993-04-05 -11.346331   9.188078  \n1993-04-12 -17.383315  -2.753247  \n1993-04-19 -12.056565   9.419621  \n1993-04-26  -9.191863 -10.954436  \n\n[5 rows x 26 columns]","content_type":"text/plain"},"text/html":{"content":"\n  <div id=\"df-ce5625cc-39f0-466e-b119-bea6494370d2\" class=\"colab-df-container\">\n    <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>kb</th>\n      <th>zos</th>\n      <th>salinity</th>\n      <th>water_temp</th>\n      <th>peace_discharge</th>\n      <th>peace_TN</th>\n      <th>peace_TP</th>\n      <th>wind_u</th>\n      <th>wind_v</th>\n      <th>bloom_target</th>\n      <th>...</th>\n      <th>kb_M3_mean</th>\n      <th>bloom_target_M3_prop</th>\n      <th>peace_discharge_4w_avg_L1</th>\n      <th>zos_L1</th>\n      <th>water_temp_L1</th>\n      <th>peace_discharge_L1</th>\n      <th>peace_TN_L1</th>\n      <th>peace_TP_L1</th>\n      <th>wind_u_L1</th>\n      <th>wind_v_L1</th>\n    </tr>\n    <tr>\n      <th>time</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1993-03-29</th>\n      <td>0.0</td>\n      <td>-0.060247</td>\n      <td>33.304348</td>\n      <td>26.2</td>\n      <td>2000.0</td>\n      <td>2.02</td>\n      <td>1.504000</td>\n      <td>-11.346331</td>\n      <td>9.188078</td>\n      <td>0</td>\n      <td>...</td>\n      <td>416.75</td>\n      <td>0.0</td>\n      <td>1822.5</td>\n      <td>-0.061417</td>\n      <td>25.5</td>\n      <td>2550.0</td>\n      <td>1.948</td>\n      <td>1.565333</td>\n      <td>-17.499440</td>\n      <td>5.685913</td>\n    </tr>\n    <tr>\n      <th>1993-04-05</th>\n      <td>0.0</td>\n      <td>-0.037195</td>\n      <td>33.326087</td>\n      <td>26.4</td>\n      <td>3290.0</td>\n      <td>5.68</td>\n      <td>1.442667</td>\n      <td>-17.383315</td>\n      <td>-2.753247</td>\n      <td>0</td>\n      <td>...</td>\n      <td>333.50</td>\n      <td>0.0</td>\n      <td>2050.0</td>\n      <td>-0.060247</td>\n      <td>26.2</td>\n      <td>2000.0</td>\n      <td>2.020</td>\n      <td>1.504000</td>\n      <td>-11.346331</td>\n      <td>9.188078</td>\n    </tr>\n    <tr>\n      <th>1993-04-12</th>\n      <td>0.0</td>\n      <td>-0.008214</td>\n      <td>33.347826</td>\n      <td>26.3</td>\n      <td>3140.0</td>\n      <td>9.34</td>\n      <td>1.381333</td>\n      <td>-12.056565</td>\n      <td>9.419621</td>\n      <td>0</td>\n      <td>...</td>\n      <td>166.75</td>\n      <td>0.0</td>\n      <td>2600.0</td>\n      <td>-0.037195</td>\n      <td>26.4</td>\n      <td>3290.0</td>\n      <td>5.680</td>\n      <td>1.442667</td>\n      <td>-17.383315</td>\n      <td>-2.753247</td>\n    </tr>\n    <tr>\n      <th>1993-04-19</th>\n      <td>0.0</td>\n      <td>-0.012612</td>\n      <td>33.369565</td>\n      <td>26.5</td>\n      <td>2040.0</td>\n      <td>13.00</td>\n      <td>1.320000</td>\n      <td>-9.191863</td>\n      <td>-10.954436</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>2745.0</td>\n      <td>-0.008214</td>\n      <td>26.3</td>\n      <td>3140.0</td>\n      <td>9.340</td>\n      <td>1.381333</td>\n      <td>-12.056565</td>\n      <td>9.419621</td>\n    </tr>\n    <tr>\n      <th>1993-04-26</th>\n      <td>0.0</td>\n      <td>-0.034968</td>\n      <td>33.391304</td>\n      <td>26.9</td>\n      <td>1800.0</td>\n      <td>9.30</td>\n      <td>1.440000</td>\n      <td>-12.943406</td>\n      <td>3.468175</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>2617.5</td>\n      <td>-0.012612</td>\n      <td>26.5</td>\n      <td>2040.0</td>\n      <td>13.000</td>\n      <td>1.320000</td>\n      <td>-9.191863</td>\n      <td>-10.954436</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 26 columns</p>\n</div>\n    <div class=\"colab-df-buttons\">\n\n  <div class=\"colab-df-container\">\n    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ce5625cc-39f0-466e-b119-bea6494370d2')\"\n            title=\"Convert this dataframe to an interactive table.\"\n            style=\"display:none;\">\n\n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n  </svg>\n    </button>\n\n  <style>\n    .colab-df-container {\n      display:flex;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    .colab-df-buttons div {\n      margin-bottom: 4px;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n    <script>\n      const buttonEl =\n        document.querySelector('#df-ce5625cc-39f0-466e-b119-bea6494370d2 button.colab-df-convert');\n      buttonEl.style.display =\n        google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n      async function convertToInteractive(key) {\n        const element = document.querySelector('#df-ce5625cc-39f0-466e-b119-bea6494370d2');\n        const dataTable =\n          await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                    [key], {});\n        if (!dataTable) return;\n\n        const docLinkHtml = 'Like what you see? Visit the ' +\n          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n          + ' to learn more about interactive tables.';\n        element.innerHTML = '';\n        dataTable['output_type'] = 'display_data';\n        await google.colab.output.renderOutput(dataTable, element);\n        const docLink = document.createElement('div');\n        docLink.innerHTML = docLinkHtml;\n        element.appendChild(docLink);\n      }\n    </script>\n  </div>\n\n\n    <div id=\"df-c8632e87-f471-42ee-b02e-c6b1b5503f80\">\n      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c8632e87-f471-42ee-b02e-c6b1b5503f80')\"\n                title=\"Suggest charts\"\n                style=\"display:none;\">\n\n<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n     width=\"24px\">\n    <g>\n        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n    </g>\n</svg>\n      </button>\n\n<style>\n  .colab-df-quickchart {\n      --bg-color: #E8F0FE;\n      --fill-color: #1967D2;\n      --hover-bg-color: #E2EBFA;\n      --hover-fill-color: #174EA6;\n      --disabled-fill-color: #AAA;\n      --disabled-bg-color: #DDD;\n  }\n\n  [theme=dark] .colab-df-quickchart {\n      --bg-color: #3B4455;\n      --fill-color: #D2E3FC;\n      --hover-bg-color: #434B5C;\n      --hover-fill-color: #FFFFFF;\n      --disabled-bg-color: #3B4455;\n      --disabled-fill-color: #666;\n  }\n\n  .colab-df-quickchart {\n    background-color: var(--bg-color);\n    border: none;\n    border-radius: 50%;\n    cursor: pointer;\n    display: none;\n    fill: var(--fill-color);\n    height: 32px;\n    padding: 0;\n    width: 32px;\n  }\n\n  .colab-df-quickchart:hover {\n    background-color: var(--hover-bg-color);\n    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n    fill: var(--button-hover-fill-color);\n  }\n\n  .colab-df-quickchart-complete:disabled,\n  .colab-df-quickchart-complete:disabled:hover {\n    background-color: var(--disabled-bg-color);\n    fill: var(--disabled-fill-color);\n    box-shadow: none;\n  }\n\n  .colab-df-spinner {\n    border: 2px solid var(--fill-color);\n    border-color: transparent;\n    border-bottom-color: var(--fill-color);\n    animation:\n      spin 1s steps(1) infinite;\n  }\n\n  @keyframes spin {\n    0% {\n      border-color: transparent;\n      border-bottom-color: var(--fill-color);\n      border-left-color: var(--fill-color);\n    }\n    20% {\n      border-color: transparent;\n      border-left-color: var(--fill-color);\n      border-top-color: var(--fill-color);\n    }\n    30% {\n      border-color: transparent;\n      border-left-color: var(--fill-color);\n      border-top-color: var(--fill-color);\n      border-right-color: var(--fill-color);\n    }\n    40% {\n      border-color: transparent;\n      border-right-color: var(--fill-color);\n      border-top-color: var(--fill-color);\n    }\n    60% {\n      border-color: transparent;\n      border-right-color: var(--fill-color);\n    }\n    80% {\n      border-color: transparent;\n      border-right-color: var(--fill-color);\n      border-bottom-color: var(--fill-color);\n    }\n    90% {\n      border-color: transparent;\n      border-bottom-color: var(--fill-color);\n    }\n  }\n</style>\n\n      <script>\n        async function quickchart(key) {\n          const quickchartButtonEl =\n            document.querySelector('#' + key + ' button');\n          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n          quickchartButtonEl.classList.add('colab-df-spinner');\n          try {\n            const charts = await google.colab.kernel.invokeFunction(\n                'suggestCharts', [key], {});\n          } catch (error) {\n            console.error('Error during call to suggestCharts:', error);\n          }\n          quickchartButtonEl.classList.remove('colab-df-spinner');\n          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n        }\n        (() => {\n          let quickchartButtonEl =\n            document.querySelector('#df-c8632e87-f471-42ee-b02e-c6b1b5503f80 button');\n          quickchartButtonEl.style.display =\n            google.colab.kernel.accessAllowed ? 'block' : 'none';\n        })();\n      </script>\n    </div>\n\n    </div>\n  </div>\n","content_type":"text/html"},"application/vnd.google.colaboratory.intrinsic+json":{"content":"{\"type\":\"dataframe\"}","content_type":"application/vnd.google.colaboratory.intrinsic+json"}}},"children":[],"identifier":"47chavmsjxe--outputs-1","html_id":"id-47chavmsjxe-outputs-1","key":"aAx8nIAd3L"},{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"\nUnscaled test_df also created for EnKF observations.\n"},"children":[],"identifier":"47chavmsjxe--outputs-2","html_id":"id-47chavmsjxe-outputs-2","key":"HR6uBI4VYe"}],"identifier":"47chavmsjxe--outputs","html_id":"id-47chavmsjxe-outputs","key":"zULP8DzlPn"}],"identifier":"47chavmsjxe-","label":"47ChAVmSjxe-","html_id":"id-47chavmsjxe","key":"FMSaKizdqU"},{"type":"block","kind":"notebook-content","data":{"id":"zJZ8x96ukDHt"},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Part 5: Preprocessing (Imputation & Scaling)","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rM0CZ4ndNT"}],"identifier":"part-5-preprocessing-imputation-scaling","label":"Part 5: Preprocessing (Imputation & Scaling)","html_id":"part-5-preprocessing-imputation-scaling","implicit":true,"key":"E80hQfM982"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"This step takes the chronologically split dataframes (train_df, validation_df, test_df from Part 4) and performs two crucial preprocessing tasks:","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"eAze2fnRI8"}],"key":"XoWpSUjH6e"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Imputation: Handles any remaining missing values (NaNs) in the feature columns. It’s vital to fit the imputer only on the training data.\nScaling: Scales the numerical features using the scaler type specified in the configuration (RobustScaler or StandardScaler). The scaler is fit only on the training data.\nThe fitted scaler and the final list of feature columns used are saved.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"FYIcW93ceq"}],"key":"AHfyWGcNqz"},{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"This cell defines the preprocess_data function to handle imputation and scaling, fitting the necessary transformers only on the training set. It then applies this function to the split dataframes generated in Part 4.","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"IbZARhQGey"}],"key":"PhtxUNlnkz"},{"type":"paragraph","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"Explanation of Fixes:","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"jx3n4GZHAC"}],"key":"HrC9B0jjJ6"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":12,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"feature_suffix Definition: The NameError occurred because feature_suffix (which depends on USE_ENHANCED_FEATURES) was used to construct FEATURE_LIST_FILENAME before the main execution block where it would be defined.","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"SFhaEIJf0M"}],"key":"BKehO5WlpG"}],"key":"KP2hYtC4av"},{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Solution: The code now defines feature_suffix inside the if ","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"XW1HSyflTr"},{"type":"strong","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"name","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"NNWWcutL3E"}],"key":"exbd1E5VGf"},{"type":"text","value":" == “","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"JRvZIJOCme"},{"type":"strong","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"main","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"wfsnaZXkGY"}],"key":"pxuLjPRBy0"},{"type":"text","value":"”: block before calling preprocess_data. Filename templates (scaler_fname_template, flist_fname_template) are passed to the function, and the final filenames are constructed inside the function using the passed feature_suffix.","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"YVqogVepQj"}],"key":"zk9PExha0t"}],"key":"Nzy1Sg7huT"},{"type":"listItem","spread":true,"position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Configuration Checks: Added more robust checks at the beginning of the main execution block to ensure all necessary configuration variables and dataframes exist before proceeding.","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"y9fCPt7YMP"}],"key":"iuQjUIkZ77"}],"key":"mQEc6Y58gw"},{"type":"listItem","spread":true,"position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Error Handling: Added more specific error messages and try-except blocks within the preprocess_data function for robustness during imputation and scaling transforms.","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"GWa8AcdZYr"}],"key":"ARx5xyQ6kf"}],"key":"gsNJVk7Iqp"},{"type":"listItem","spread":true,"position":{"start":{"line":16,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Clarity: Minor adjustments to print statements for better clarity.\nNow, when you run this corrected Part 5 cell (after ensuring Part 1 and Part 4 have run successfully in your session), it should correctly define feature_suffix, construct the filenames, perform the preprocessing, save the components, and store the results in the train_scaled_df, validation_scaled_df, etc., variables without the NameError.","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"RhQPKEggZG"}],"key":"WVkf6mHQt4"}],"key":"GO099IilNW"}],"key":"dmvarXCORP"}],"identifier":"zjz8x96ukdht","label":"zJZ8x96ukDHt","html_id":"zjz8x96ukdht","key":"d2cIq1CLLO"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tyRG0McTkHf7","outputId":"c010e4ba-752e-4057-852a-d6bc4b9fd48c","ExecuteTime":{"end_time":"2026-01-07T03:44:43.374949200Z","start_time":"2026-01-07T03:37:30.245630Z"}},"children":[{"type":"code","lang":"python","executable":true,"value":"# --- Part 5: Preprocessing (Imputation & Scaling) ---\n\ndef preprocess_data(train_df, validation_df, test_df, target_col, scaler_type='Robust',\n                    output_dir='output/', scaler_fname_template=None, flist_fname_template=None, feature_suffix='basic'):\n\n    # 1. Validate Inputs\n    if any(df is None or df.empty for df in [train_df, validation_df, test_df]):\n        print(\"Error: One or more input DataFrames are invalid.\")\n        return (None,) * 5\n\n    print(f\"\\n--- Preprocessing Data (Imputation & Scaling: {scaler_type}) ---\")\n\n    # 2. Identify Features (Exclude Target)\n    # We use training data to determine features\n    feature_candidates = [c for c in train_df.columns if c != target_col]\n    final_feature_columns = train_df[feature_candidates].select_dtypes(include=np.number).columns.tolist()\n\n    if not final_feature_columns:\n        print(\"Error: No numeric features found.\")\n        return (None,) * 5\n\n    print(f\"Identified {len(final_feature_columns)} numeric features.\")\n\n    # 3. Initialize Transformers\n    imputer = SimpleImputer(strategy='mean')\n\n    if scaler_type.lower() == 'standard':\n        scaler = StandardScaler()\n    else:\n        scaler = RobustScaler() # Default to Robust\n\n    # 4. Fit Transformers (ONLY on Training Data)\n    train_proc_df = train_df.copy()\n\n    # Imputation Fit\n    impute_needed = train_proc_df[final_feature_columns].isnull().values.any()\n    if impute_needed:\n        print(\"Fitting imputer on training data...\")\n        imputer.fit(train_proc_df[final_feature_columns])\n    else:\n        print(\"No missing values in training set. Skipping imputer fitting.\")\n\n    # Scaling Fit (Fit on imputed training data)\n    # If we imputed, we must transform the temp training data to fit the scaler correctly\n    temp_train_features = imputer.transform(train_proc_df[final_feature_columns]) if impute_needed else train_proc_df[final_feature_columns]\n\n    print(f\"Fitting {scaler_type}Scaler on training data...\")\n    scaler.fit(temp_train_features)\n\n    # 5. Transform All Sets (Loop to remove redundancy)\n    processed_dfs = []\n    datasets = [train_df, validation_df, test_df]\n    set_names = ['Train', 'Validation', 'Test']\n\n    for name, df in zip(set_names, datasets):\n        df_copy = df.copy()\n        try:\n            # Apply Imputation\n            if impute_needed:\n                df_copy[final_feature_columns] = imputer.transform(df_copy[final_feature_columns])\n\n            # Apply Scaling\n            df_copy[final_feature_columns] = scaler.transform(df_copy[final_feature_columns])\n            processed_dfs.append(df_copy)\n        except Exception as e:\n            print(f\"Error processing {name} set: {e}\")\n            return (None,) * 5\n\n    train_scaled, val_scaled, test_scaled = processed_dfs\n    print(\"Transformation complete.\")\n\n    # 6. Save Components\n    try:\n        os.makedirs(output_dir, exist_ok=True)\n        # Use provided templates or defaults\n        s_path = scaler_fname_template.format(scaler_type.lower()) if scaler_fname_template else f\"{output_dir}/scaler.joblib\"\n        f_path = flist_fname_template.format(feature_suffix) if flist_fname_template else f\"{output_dir}/features.joblib\"\n\n        joblib.dump(scaler, s_path)\n        joblib.dump(final_feature_columns, f_path)\n        print(f\"Saved scaler and feature list to {output_dir}\")\n    except Exception as e:\n        print(f\"Warning: Could not save scaler/features: {e}\")\n\n    return train_scaled, val_scaled, test_scaled, scaler, final_feature_columns\n\n# --- Execution Block ---\nif __name__ == \"__main__\":\n    # Assumes train_df, validation_df, test_df exist from Part 4\n    # Assumes config variables like SCALER_TYPE exist from Part 1\n\n    if 'train_df' in locals():\n        train_scaled_df, validation_scaled_df, test_scaled_df, scaler, final_feature_columns_used = preprocess_data(\n            train_df, validation_df, test_df,\n            TARGET_BINARY_COL,\n            scaler_type=SCALER_TYPE,\n            output_dir=OUTPUT_DIR,\n            scaler_fname_template=SCALER_FILENAME.replace(f\"_{SCALER_TYPE.lower()}\", \"_{}\"), # Dynamic template\n            flist_fname_template=FEATURE_LIST_FILENAME.replace(f\"_{feature_suffix}\", \"_{}\"),\n            feature_suffix=feature_suffix\n        )","identifier":"tyrg0mctkhf7-code","enumerator":"9","html_id":"tyrg0mctkhf7-code","key":"RJFeg3OvDU"},{"type":"outputs","id":"gK7N3OXLLu7WwptDAZQL5","children":[{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"\n--- Preprocessing Data (Imputation & Scaling: Robust) ---\nIdentified 25 numeric features.\nNo missing values in training set. Skipping imputer fitting.\nFitting RobustScaler on training data...\nTransformation complete.\nSaved scaler and feature list to output_refactored/\n"},"children":[],"identifier":"tyrg0mctkhf7-outputs-0","html_id":"tyrg0mctkhf7-outputs-0","key":"Ery9fuFp0P"}],"identifier":"tyrg0mctkhf7-outputs","html_id":"tyrg0mctkhf7-outputs","key":"JSqxpwjq9X"}],"identifier":"tyrg0mctkhf7","label":"tyRG0McTkHf7","html_id":"tyrg0mctkhf7","key":"a0ortc3sbU"},{"type":"block","kind":"notebook-content","data":{"id":"BRyqj-kJkOh2"},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Part 6: Sequence Creation","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FfJNXslC2s"}],"identifier":"part-6-sequence-creation","label":"Part 6: Sequence Creation","html_id":"part-6-sequence-creation","implicit":true,"key":"zoETVH2CnZ"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"This step takes the scaled dataframes (train_scaled_df, validation_scaled_df, test_scaled_df) and transforms them into the sequence format required by LSTM/GRU models. Each sequence (X) will contain SEQUENCE_LENGTH consecutive time steps of features, and the corresponding target (y) will be the bloom state FORECAST_HORIZON steps after the end of the sequence.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"GVyFUEHwDI"}],"key":"kThx3vvMw7"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"This cell defines the create_sequences_from_df function and applies it to the scaled training, validation, and test dataframes generated in Part 5. It uses the SEQUENCE_LENGTH and FORECAST_HORIZON parameters defined in Part 1. The resulting NumPy arrays (X_train, y_train, etc.) are saved to a file.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"moouOF2TwZ"}],"key":"WbbA40lpY7"},{"type":"paragraph","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"Explanation:","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"dQBhlgr9tN"}],"key":"pvTHNx4l0I"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":10,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Function create_sequences_from_df:","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"caa9SFia4b"}],"key":"rLWRHZT7RT"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":11,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Takes a scaled DataFrame, the list of feature columns to use, the target column name, sequence length, and forecast horizon.","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"WqK2qhpMhI"}],"key":"O9ukzecnzU"}],"key":"MlMH2bL5aA"},{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Includes checks for valid input and sufficient data length.","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"SQbHVey5FS"}],"key":"I6VWtFgOAD"}],"key":"SmGOxDiZBv"},{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Iterates through the data, creating sequences (X) of length sequence_length and corresponding targets (y) taken forecast_horizon steps after the end of each sequence.","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"rFcwTCyIF3"}],"key":"YBWxwfUzhL"}],"key":"HWXW8MoE9q"},{"type":"listItem","spread":true,"position":{"start":{"line":14,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Returns the sequences and targets as NumPy arrays.","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"rM2adxIdPE"}],"key":"MJF7n37AdV"}],"key":"hx6uDwXCJK"}],"key":"GkavOfx124"}],"key":"i5Alsssbzr"},{"type":"listItem","spread":true,"position":{"start":{"line":16,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"Main Execution:","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"FervYlPt0Z"}],"key":"YkgGIsgeDl"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":17,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Checks that the necessary scaled DataFrames (train_scaled_df, etc.) and the feature list (final_feature_columns_used) exist from Part 5.","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"nJmLa3l7BK"}],"key":"iRxLyjETud"}],"key":"MynJpEXd65"},{"type":"listItem","spread":true,"position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Calls create_sequences_from_df separately for the training, validation, and test sets.","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"o6k8GE4ZlO"}],"key":"QOE2NXL1he"}],"key":"k4nQJCqiCf"},{"type":"listItem","spread":true,"position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Prints the shapes of the resulting NumPy arrays (X_train, y_train, etc.).","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"EiEVQVNL0O"}],"key":"hXt78T4pcJ"}],"key":"unGfKAOnoB"},{"type":"listItem","spread":true,"position":{"start":{"line":20,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Saves the generated sequences and the feature list used to create them into a single .npz file for easy loading later. The filename dynamically includes the forecast horizon and feature type based on the configuration in Part 1.","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"HNOMPx5v29"}],"key":"itlX2klFXo"}],"key":"bLA5buleSI"}],"key":"PkFryYDH9X"}],"key":"VzkhOfOnvB"}],"key":"pkCYZ3uAwf"},{"type":"paragraph","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"text","value":"After running this cell, you will have the sequence arrays (X_train, y_train, X_val, y_val, X_test, y_test) ready for the LSTM model definition (Part 7) and subsequent training/tuning steps.","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"zaFYgDrE1V"}],"key":"XkGYiHgR5T"}],"identifier":"bryqj-kjkoh2","label":"BRyqj-kJkOh2","html_id":"bryqj-kjkoh2","key":"UsIu0Jr5jW"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kk0kagLqkThx","outputId":"5881a1ee-854d-4ab0-a79d-fda73305d7ea","ExecuteTime":{"end_time":"2026-01-07T03:44:43.375949700Z","start_time":"2026-01-07T03:37:30.291068Z"}},"children":[{"type":"code","lang":"python","executable":true,"value":"def create_sequences_from_df(df, seq_length, target_col_idx=0, pred_step=1, return_targets=True):\n    \"\"\"\n    Creates sequences for LSTM model from a DataFrame.\n    \"\"\"\n    xs = []\n    ys = []\n\n    # Convert dataframe to numpy if needed\n    data = df.values if hasattr(df, 'values') else df\n\n    # Loop through data\n    # We stop earlier to account for the prediction step ahead (pred_step)\n    for i in range(len(data) - seq_length - pred_step + 1):\n        x = data[i:(i + seq_length)]\n        xs.append(x)\n\n        if return_targets:\n            # Target is 'pred_step' steps after the sequence ends\n            y = data[i + seq_length + pred_step - 1, target_col_idx]\n            ys.append(y)\n\n    return np.array(xs), np.array(ys)\n\nif __name__ == \"__main__\":\n    if 'train_scaled_df' in locals() and train_scaled_df is not None:\n\n        # Dictionary to automate the loop\n        data_splits = {\n            'train': train_scaled_df,\n            'val':   validation_scaled_df,\n            'test':  test_scaled_df\n        }\n\n        sequences = {}\n        print(f\"\\n--- Creating Sequences (Horizon: {FORECAST_HORIZON}, Seq Length: {SEQUENCE_LENGTH}) ---\")\n\n        for name, df in data_splits.items():\n            print(f\"Generating {name} sequences...\")\n\n            # --- CRITICAL FIX START ---\n            # 1. Determine which columns to use.\n            # We must ensure the TARGET column is included in the data so we can extract 'y'.\n            if 'final_feature_columns_used' in locals():\n                # Make a copy of the list so we don't modify the original variable\n                cols_to_use = list(final_feature_columns_used)\n\n                # If target is missing from features, add it so we can create 'y'\n                if TARGET_BINARY_COL not in cols_to_use:\n                    print(f\"  Note: Adding '{TARGET_BINARY_COL}' back to dataframe for sequence generation.\")\n                    cols_to_use.append(TARGET_BINARY_COL)\n\n                df_subset = df[cols_to_use]\n            else:\n                df_subset = df\n            # --- CRITICAL FIX END ---\n\n            # 2. Find the integer index of the target column inside this new subset\n            try:\n                target_idx = df_subset.columns.get_loc(TARGET_BINARY_COL)\n            except KeyError:\n                raise KeyError(f\"Target column '{TARGET_BINARY_COL}' not found in the dataframe. Check spelling!\")\n\n            # 3. Call the function\n            X, y = create_sequences_from_df(\n                df_subset,              # The Data (Features + Target)\n                SEQUENCE_LENGTH,        # Window size\n                target_idx,             # Index of the target column\n                FORECAST_HORIZON,       # Prediction step\n                True                    # Return targets?\n            )\n\n            # Store in dictionary\n            sequences[f'X_{name}'] = X\n            sequences[f'y_{name}'] = y\n            print(f\"  {name}: X={X.shape}, y={y.shape}\")\n\n        # Unpack for later parts\n        X_train, y_train = sequences['X_train'], sequences['y_train']\n        X_val, y_val     = sequences['X_val'], sequences['y_val']\n        X_test, y_test   = sequences['X_test'], sequences['y_test']\n\n        # Save to file\n        if X_train.size > 0:\n            suffix = feature_suffix if 'feature_suffix' in locals() else \"\"\n            fname = SEQUENCES_FILENAME_TEMPLATE.format(FORECAST_HORIZON, suffix)\n\n            # Save arrays and the column list for reference\n            np.savez(fname, **sequences, feature_columns=df_subset.columns)\n            print(f\"\\nSaved sequences to {fname}\")\n\n    else:\n        print(\"Skipping Part 6: Scaled data not found.\")","identifier":"kk0kaglqkthx-code","enumerator":"10","html_id":"kk0kaglqkthx-code","key":"gwRxdumF4q"},{"type":"outputs","id":"JCZPCQiIo3Z24ZkndzfyN","children":[{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"\n--- Creating Sequences (Horizon: 1, Seq Length: 12) ---\nGenerating train sequences...\n  Note: Adding 'bloom_target' back to dataframe for sequence generation.\n  train: X=(1111, 12, 26), y=(1111,)\nGenerating val sequences...\n  Note: Adding 'bloom_target' back to dataframe for sequence generation.\n  val: X=(228, 12, 26), y=(228,)\nGenerating test sequences...\n  Note: Adding 'bloom_target' back to dataframe for sequence generation.\n  test: X=(230, 12, 26), y=(230,)\n\nSaved sequences to output_refactored/sequences_horizon1wk_enhanced.npz\n"},"children":[],"identifier":"kk0kaglqkthx-outputs-0","html_id":"kk0kaglqkthx-outputs-0","key":"cg5j27Zs5n"}],"identifier":"kk0kaglqkthx-outputs","html_id":"kk0kaglqkthx-outputs","key":"ohar1CUAB6"}],"identifier":"kk0kaglqkthx","label":"Kk0kagLqkThx","html_id":"kk0kaglqkthx","key":"W1sxEwRWxr"},{"type":"block","kind":"notebook-content","data":{"id":"D79eTjU0ka0M"},"children":[{"type":"heading","depth":3,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"Part 7: Model Definition (LSTM).","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"kLBNC2oc6j"}],"identifier":"part-7-model-definition-lstm","label":"Part 7: Model Definition (LSTM).","html_id":"part-7-model-definition-lstm","implicit":true,"key":"jViIHlIDbm"},{"type":"paragraph","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"This part defines the function build_lstm_model responsible for creating the LSTM network architecture using TensorFlow/Keras. The function is designed to be flexible: it can use default hyperparameters defined in Part 1, or it can accept a hp object from KerasTuner (which we’ll use later in Part 8) to create models with varying hyperparameters during the tuning process.","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"bAqhBCI2hR"}],"key":"cyqXgh3Fo7"},{"type":"paragraph","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"This cell defines the build_lstm_model function. It specifies the layers (LSTM, Dropout, Dense) and allows for hyperparameter configuration either through defaults or a KerasTuner object. It then demonstrates building the model with default hyperparameters, assuming the input shape is known from Part 6.","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"AGDZqBVQxQ"}],"key":"aWf5AzLyVB"},{"type":"paragraph","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"Explanation:","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"BHWZnpDXO1"}],"key":"KFgBbjTidW"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":10,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Function build_lstm_model:","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"ljebuH7547"}],"key":"d9iYbv2ZhU"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":11,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Takes input_shape (required) and an optional KerasTuner hp object.","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"aRW9tJqsWx"}],"key":"OdAfIz838c"}],"key":"P6pEZnlIeA"},{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Hyperparameter Handling: If hp is provided (during tuning), it defines hyperparameters using ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"gAr2eojUun"},{"type":"link","url":"http://hp.Int","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"hp.Int","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"uwE6daOC33"}],"urlSource":"http://hp.Int","key":"t88GTrtoxL"},{"type":"text","value":", hp.Float, hp.Choice. If hp is None (when building the default or final model), it uses the DEFAULT_ variables defined in Part 1 (with fallbacks just in case).","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"PtWYEe9IAx"}],"key":"S331KBKHpW"}],"key":"Y7mjCt2VKO"},{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Architecture: Defines a two-layer LSTM structure with Dropout. You can easily modify this (e.g., change to GRU, add Dense layers, use Bidirectional) by editing this function.","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"D7AXMgCqQ0"}],"key":"l7MGHiV8GY"}],"key":"wvcLWTU0MH"},{"type":"listItem","spread":true,"position":{"start":{"line":14,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Compilation: Compiles the model inside the function using Adam optimizer and binary cross-entropy loss. This is convenient for KerasTuner.","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"VMToNOZsx0"}],"key":"r9OpaCYwly"}],"key":"G2BxkzD9GY"}],"key":"LK7cjY5HNk"}],"key":"GEPLWy1WOK"},{"type":"listItem","spread":true,"position":{"start":{"line":16,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"Main Execution (if ","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"ffilj9Uzj1"},{"type":"strong","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"name","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"vQvk01wciw"}],"key":"a7fHPBznCi"},{"type":"text","value":" == “","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"nhZzbFoZm8"},{"type":"strong","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"main","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"EaZwG7XG3X"}],"key":"wUYpgDRKpp"},{"type":"text","value":"”:):","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"IbhIPNLkAS"}],"key":"pedyPu5x7Y"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":17,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Checks if X_train exists (from Part 6) to get the required input_shape.","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"qXMWwvXrKB"}],"key":"atxxbtFGhr"}],"key":"dJfm934uqO"},{"type":"listItem","spread":true,"position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Calls build_lstm_model with hp=None to create an instance using the default hyperparameters.","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"nBNdbEo18v"}],"key":"t5Q7XNkroD"}],"key":"IbMe0NwilN"},{"type":"listItem","spread":true,"position":{"start":{"line":19,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Prints the model summary.","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"oCjyCHCNFe"}],"key":"bcI6a0XGTL"}],"key":"EsJHiUJC8i"}],"key":"UURiMoO9qm"}],"key":"YGqCIpNzc1"}],"key":"Wu9kPKATgN"},{"type":"paragraph","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"After running this cell, the function build_lstm_model is defined and ready to be used either by KerasTuner (Part 8) or directly for training the default/final model (Part 9). The lstm_model_default variable holds an example compiled model instance (useful for checking).","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"ajKIjuSfh0"}],"key":"PlGA7nwCGJ"}],"identifier":"d79etju0ka0m","label":"D79eTjU0ka0M","html_id":"d79etju0ka0m","key":"LRU3LCZrxA"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/","height":375},"id":"ztwksvFhkel6","outputId":"48cea08c-deac-43b1-8cc0-d312b23f8fa5","ExecuteTime":{"end_time":"2026-01-07T03:44:43.375949700Z","start_time":"2026-01-07T03:37:30.317485Z"}},"children":[{"type":"code","lang":"python","executable":true,"value":"def build_lstm_model(input_shape, hp=None):\n    \"\"\"\n    Builds and compiles an LSTM model.\n\n    Args:\n        input_shape (tuple): The shape of the input data (time_steps, features).\n        hp (KerasTuner.HyperParameters, optional): Hyperparameters for tuning.\n                                                   If None, uses defaults.\n    Returns:\n        model: A compiled Keras model.\n    \"\"\"\n\n    # ---------------------------------------------------------\n    # 1. Hyperparameter Definition (Dual Mode)\n    # ---------------------------------------------------------\n    # If 'hp' is provided, we are in Tuning Mode.\n    # If 'hp' is None, we use the global defaults (defined in Part 1).\n\n    if hp:\n        # Tuning Mode: Ask KerasTuner to try different values\n        units_1 = hp.Int('lstm_units_1', min_value=32, max_value=128, step=32)\n        units_2 = hp.Int('lstm_units_2', min_value=16, max_value=64, step=16)\n        dropout_rate = hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1)\n        learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n    else:\n        # Default Mode: Use variables from Part 1 (with safety fallbacks)\n        units_1 = globals().get('DEFAULT_LSTM_UNITS', 64)\n        units_2 = globals().get('DEFAULT_LSTM_UNITS', 32) # Using same default or half\n        dropout_rate = globals().get('DEFAULT_DROPOUT', 0.2)\n        learning_rate = globals().get('DEFAULT_LEARNING_RATE', 0.001)\n\n    # ---------------------------------------------------------\n    # 2. Model Architecture\n    # ---------------------------------------------------------\n    model = Sequential()\n\n    # Input Layer\n    model.add(Input(shape=input_shape))\n\n    # LSTM Layer 1 (Must return sequences to feed the next LSTM layer)\n    model.add(LSTM(units=units_1, return_sequences=True))\n    model.add(Dropout(dropout_rate))\n\n    # LSTM Layer 2 (return_sequences=False because next is Dense)\n    model.add(LSTM(units=units_2, return_sequences=False))\n    model.add(Dropout(dropout_rate))\n\n    # Output Layer (1 unit for Binary Classification: Bloom vs No Bloom)\n    model.add(Dense(1, activation='sigmoid'))\n\n    # ---------------------------------------------------------\n    # 3. Compilation\n    # ---------------------------------------------------------\n    model.compile(\n        optimizer=Adam(learning_rate=learning_rate),\n        loss='binary_crossentropy',\n        metrics=[BinaryAccuracy(name='accuracy'), AUC(name='auc'), Recall(name='recall'),Precision(name='precision')]\n    )\n\n    return model\n\n# --- Main Execution (Test Block) ---\nif __name__ == \"__main__\":\n    print(\"--- Testing Model Definition (Step 7) ---\")\n\n    # We need X_train to know the input shape\n    if 'X_train' in locals() and X_train is not None:\n\n        # Determine shape: (Sequence Length, Number of Features)\n        # X_train shape is typically (Samples, Time Steps, Features)\n        # We need the last two dimensions for input_shape\n        input_shape_test = (X_train.shape[1], X_train.shape[2])\n        print(f\"Input Shape detected: {input_shape_test}\")\n\n        # Build a \"Default\" model (hp=None) to verify it works\n        try:\n            model_test = build_lstm_model(input_shape_test, hp=None)\n            print(\"\\nModel built successfully!\")\n            model_test.summary()\n        except Exception as e:\n            print(f\"Error building model: {e}\")\n\n    else:\n        print(\"Warning: X_train not found. Run Step 6 first to test this function.\")\n        print(\"Function 'build_lstm_model' is defined but not tested.\")","identifier":"ztwksvfhkel6-code","enumerator":"11","html_id":"ztwksvfhkel6-code","key":"U1fCqCd6Cm"},{"type":"outputs","id":"2UaUFy-7KTMIemAESjYdD","children":[{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"--- Testing Model Definition (Step 7) ---\nInput Shape detected: (12, 26)\n\nModel built successfully!\n"},"children":[],"identifier":"ztwksvfhkel6-outputs-0","html_id":"ztwksvfhkel6-outputs-0","key":"TW69Oy8UmW"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"\u001b[1mModel: \"sequential\"\u001b[0m\n","content_type":"text/plain"},"text/html":{"content":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n","content_type":"text/html"}}},"children":[],"identifier":"ztwksvfhkel6-outputs-1","html_id":"ztwksvfhkel6-outputs-1","key":"Hr2NnwGnfT"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m23,296\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m33,024\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","content_type":"text/plain"},"text/html":{"content":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">23,296</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n","content_type":"text/html"}}},"children":[],"identifier":"ztwksvfhkel6-outputs-2","html_id":"ztwksvfhkel6-outputs-2","key":"GOyekcDb6u"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m56,385\u001b[0m (220.25 KB)\n","content_type":"text/plain"},"text/html":{"content":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">56,385</span> (220.25 KB)\n</pre>\n","content_type":"text/html"}}},"children":[],"identifier":"ztwksvfhkel6-outputs-3","html_id":"ztwksvfhkel6-outputs-3","key":"fEUIl8Zu4v"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m56,385\u001b[0m (220.25 KB)\n","content_type":"text/plain"},"text/html":{"content":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">56,385</span> (220.25 KB)\n</pre>\n","content_type":"text/html"}}},"children":[],"identifier":"ztwksvfhkel6-outputs-4","html_id":"ztwksvfhkel6-outputs-4","key":"kCUU1c5PcF"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","content_type":"text/plain"},"text/html":{"content":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n","content_type":"text/html"}}},"children":[],"identifier":"ztwksvfhkel6-outputs-5","html_id":"ztwksvfhkel6-outputs-5","key":"qscc0lVpK0"}],"identifier":"ztwksvfhkel6-outputs","html_id":"ztwksvfhkel6-outputs","key":"thtxHQgcKv"}],"identifier":"ztwksvfhkel6","label":"ztwksvFhkel6","html_id":"ztwksvfhkel6","key":"Vz5vhqtUvE"},{"type":"block","kind":"notebook-content","data":{"id":"-ZKfYtZkkoCz"},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Part 8: Hyperparameter Tuning with KerasTuner (Conditional)","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MyxBd87jlh"}],"identifier":"part-8-hyperparameter-tuning-with-kerastuner-conditional","label":"Part 8: Hyperparameter Tuning with KerasTuner (Conditional)","html_id":"part-8-hyperparameter-tuning-with-kerastuner-conditional","implicit":true,"key":"PR2PExn1am"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"This cell sets up and runs the KerasTuner search process if the PERFORM_TUNING flag (from Part 1) is set to True. It uses the build_lstm_model function (from Part 7) as the hypermodel builder and searches for the best combination of LSTM units, dropout rate, and learning rate based on validation accuracy.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"ETcL4h5X9g"}],"key":"PEObs8hTiH"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Explanation:","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"KGkeKZdSc0"}],"key":"dtYcacZtDt"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":7,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Conditional Execution: The entire cell’s logic is wrapped in if can_tune:, which checks if PERFORM_TUNING was set to True in Part 1 and if the keras_tuner library (kt) was successfully imported. If not, it prints a message and skips tuning.","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"PasJbwXNhq"}],"key":"pgXPkmfzw3"}],"key":"wx5vywPHI1"},{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Prerequisite Checks: Inside the if block, it checks if the necessary sequence data (X_train, y_train, etc.) and the build_lstm_model function exist before proceeding.","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"jzdljCIeUh"}],"key":"g8vWFu7bp5"}],"key":"LijJXu6ObS"},{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Tuner Setup:","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"eRnBQ5p9Bn"}],"key":"Ik2QoYkSKC"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":10,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Creates a keras_tuner.RandomSearch instance (you could switch to kt.Hyperband for potentially faster convergence).","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"yMdUY8BvWf"}],"key":"cF4BAbGDPR"}],"key":"BTJbcHtqfl"},{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Passes a lambda function lambda hp: build_lstm_model(input_shape_tune, hp=hp) as the hypermodel builder.","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"DZgar4faGo"}],"key":"YVAWuXV2Ew"}],"key":"ftViGOj36B"},{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"This ensures the build_lstm_model function receives the tuner’s hp object to define the model architecture with tunable parameters.","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"ucG9Z2HYs7"}],"key":"Vzn2liuS40"}],"key":"MqPXzuDNAH"},{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Sets the objective to ‘val_accuracy’ (tune for best accuracy on the validation set).","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"HDLCY9ns2K"}],"key":"VvGov6sZBz"}],"key":"TKCDdH8Lnv"},{"type":"listItem","spread":true,"position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Configures max_trials, directory, project_name, etc.","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"unxy1OxnZO"}],"key":"nIuZ8ykXJA"}],"key":"TgBUorBH1i"}],"key":"TdZoHSsXHt"}],"key":"FcmOCBVxLX"},{"type":"listItem","spread":true,"position":{"start":{"line":15,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Run Search:","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"l4QzrHsmeG"}],"key":"HNkdynBy5Y"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":16,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Calls ","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"O2dOPHrzfg"},{"type":"link","url":"http://tuner.search","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"tuner.search","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"nkApvogw4p"}],"urlSource":"http://tuner.search","key":"KDpEM1kkT3"},{"type":"text","value":"(), passing the training and validation data.","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"yUzrU6MtnG"}],"key":"ERS0zpQJk4"}],"key":"uiOJrY7iU8"},{"type":"listItem","spread":true,"position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Uses a dedicated EarlyStopping callback with potentially shorter patience for the tuning phase itself.","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"ZNMhGo8XBz"}],"key":"xVKPe5gY5c"}],"key":"cHMKoIilNA"}],"key":"i7zrI9zQI7"}],"key":"XE4jCgoYmr"},{"type":"listItem","spread":true,"position":{"start":{"line":18,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Retrieve Best Hyperparameters:","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"i1iqnhsRDH"}],"key":"s1Yd7QEsan"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":19,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"After the search completes, tuner.get_best_hyperparameters(num_trials=1)[0] retrieves the HyperParameters object corresponding to the best trial.","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"oWCBfstPW7"}],"key":"FoCa4Y2xc0"}],"key":"QhVGWJCe0L"},{"type":"listItem","spread":true,"position":{"start":{"line":20,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"The values for the tuned hyperparameters (e.g., lstm_units_1, dropout_rate, learning_rate) are extracted and printed.","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"G67TRLqYki"}],"key":"gMAtOtazVY"}],"key":"LojddpSJbH"}],"key":"C3mz0ZQY3T"}],"key":"eJPug3WeQZ"}],"key":"WXMDr9DnwB"},{"type":"paragraph","position":{"start":{"line":22,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"text","value":"The best_hps variable stores this object for use in the next step (Part 9).\nIncludes error handling in case the tuner fails or doesn’t return results.\nIf PERFORM_TUNING is False, this cell will simply print a message and set best_hps to None. The next step (Part 9) will then know to use the default hyperparameters defined in Part 1.","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"uON4h7PuMP"}],"key":"B9KvwKZpx0"}],"identifier":"-zkfytzkkocz","label":"-ZKfYtZkkoCz","html_id":"id-zkfytzkkocz","key":"LlUAerChIw"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aSM54gA_ko4e","outputId":"b34a35a0-3cf5-4544-9c06-2bab22214f24","ExecuteTime":{"end_time":"2026-01-07T03:44:43.375949700Z","start_time":"2026-01-07T03:37:30.392565Z"}},"children":[{"type":"code","lang":"python","executable":true,"value":"# 1. Initialize Variables\nbest_hps = None\nbest_model_from_tuner = None\n\n# 2. Check Prerequisites\n# We need X_train (Data) and build_lstm_model (Function)\ncan_tune = True\n\nif 'X_train' not in locals() or X_train is None:\n    print(\"Error: X_train not found. Please run Step 6 (Data Generation) first.\")\n    can_tune = False\n\nif 'build_lstm_model' not in locals():\n    print(\"Error: build_lstm_model not found. Please run Step 7 (Model Definition) first.\")\n    can_tune = False\n\nif 'PERFORM_TUNING' not in locals() or not PERFORM_TUNING:\n    print(\"Skipping Tuning: PERFORM_TUNING flag is False or missing.\")\n    can_tune = False\n\n# 3. Run Tuner\nif can_tune:\n    print(\"\\n--- Setting up KerasTuner ---\")\n\n    # Define Input Shape for the builder\n    # Shape = (Time Steps, Features) -> (X_train.shape[1], X_train.shape[2])\n    input_shape_tune = (X_train.shape[1], X_train.shape[2])\n    print(f\"Tuning Input Shape: {input_shape_tune}\")\n\n    # Initialize the RandomSearch Tuner\n    # Note: We use objective='val_auc' to match the name='auc' in Step 7\n    tuner = kt.RandomSearch(\n        lambda hp: build_lstm_model(input_shape_tune, hp=hp),\n        objective=kt.Objective(\"val_auc\", direction=\"max\"),\n        max_trials=TUNER_MAX_TRIALS,       # Defined in Step 1 (e.g., 10 or 20)\n        executions_per_trial=2,            # Run each trial twice to reduce luck\n        directory=TUNER_PROJECT_DIR,       # \"hab_tuning\"\n        project_name=TUNER_PROJECT_NAME,   # \"bloom_prediction\"\n        overwrite=True                     # Start fresh every time\n    )\n\n    tuner.search_space_summary()\n\n    print(\"\\n--- Starting Search ---\")\n\n    # Early Stopping strictly for the tuning phase (speed things up)\n    tuner_early_stopping = EarlyStopping(monitor='val_loss', patience=4, verbose=0)\n\n    try:\n        # CRITICAL FIX: Pass Numpy arrays directly.\n        # Do not convert to tf.Tensor manually here.\n        tuner.search(\n            X_train, y_train,\n            epochs=TUNER_EPOCHS,           # Defined in Step 1 (e.g., 10 or 20)\n            batch_size=TUNER_BATCH_SIZE,   # Defined in Step 1 (e.g., 32)\n            validation_data=(X_val, y_val),\n            callbacks=[tuner_early_stopping],\n            verbose=1\n        )\n\n        print(\"\\n--- Hyperparameter Search Finished ---\")\n\n        # 4. Retrieve Best Results\n        best_hps_list = tuner.get_best_hyperparameters(num_trials=1)\n\n        if best_hps_list:\n            best_hps = best_hps_list[0]\n            print(\"Best Hyperparameters Found:\")\n            print(f\"  LSTM Units 1:  {best_hps.values.get('lstm_units_1')}\")\n            print(f\"  LSTM Units 2:  {best_hps.values.get('lstm_units_2')}\")\n            print(f\"  Dropout Rate:  {best_hps.values.get('dropout_rate')}\")\n            print(f\"  Learning Rate: {best_hps.values.get('learning_rate')}\")\n\n            # (Optional) You can build the best model immediately if you want\n            # best_model = tuner.hypermodel.build(best_hps)\n        else:\n            print(\"Warning: Tuner finished but returned no hyperparameters.\")\n            best_hps = None\n\n    except Exception as e:\n        print(f\"CRITICAL ERROR during tuning: {e}\")\n        best_hps = None\n\nelse:\n    print(\"Tuner skipped.\")","identifier":"asm54ga_ko4e-code","enumerator":"12","html_id":"asm54ga-ko4e-code","key":"jr0BC48BfM"},{"type":"outputs","id":"06Era_An_U07LAF1azNWk","children":[{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"Trial 10 Complete [00h 00m 12s]\nval_auc: 0.9256471693515778\n\nBest val_auc So Far: 0.9290720522403717\nTotal elapsed time: 00h 02m 41s\n\n--- Hyperparameter Search Finished ---\nBest Hyperparameters Found:\n  LSTM Units 1:  128\n  LSTM Units 2:  32\n  Dropout Rate:  0.1\n  Learning Rate: 0.01\n"},"children":[],"identifier":"asm54ga_ko4e-outputs-0","html_id":"asm54ga-ko4e-outputs-0","key":"QbCy17wKvo"}],"identifier":"asm54ga_ko4e-outputs","html_id":"asm54ga-ko4e-outputs","key":"LMIf95yxTn"}],"identifier":"asm54ga_ko4e","label":"aSM54gA_ko4e","html_id":"asm54ga-ko4e","key":"aMZ5KTMthO"},{"type":"block","kind":"notebook-content","data":{"id":"N5nvBjk-leEt"},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Part 9: Model Training","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NrmBdyW4m5"}],"identifier":"part-9-model-training","label":"Part 9: Model Training","html_id":"part-9-model-training","implicit":true,"key":"BzyDYUAjDT"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"This cell defines the main training function and executes it. It uses the configuration flags from Part 1 to determine whether to use class weights and whether to use default or tuned hyperparameters (from best_hps variable potentially created in Part 8). It saves the best model weights found during training.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Dwxwd0noht"}],"key":"qHROyFIhgO"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Explanation:","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"tM0c0QjZ1C"}],"key":"RsAjCQ8H85"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":7,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Function train_model:","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"SIUwuTqDW3"}],"key":"GsZlJ4mojU"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":8,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Takes training/validation data, input shape, configuration flags (use_class_weight), optional tuned hyperparameters (best_hps), training parameters (epochs, batch_size, patience), and paths/names for saving.","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"MB2pXV6TLc"}],"key":"lPEhyVqCRc"}],"key":"qcZ0mE2in4"},{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Builds Model: Calls build_lstm_model (from Part 7), passing best_hps if it’s available (meaning tuning was done in Part 8), otherwise hp=None is passed and the function uses defaults.","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"rqikUH3xgt"}],"key":"J7DSMo6DL8"}],"key":"e26l21xHyG"},{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Class Weights: If use_class_weight is True, it calculates the weights using compute_class_weight on y_train.","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"vjg1YB8tbH"}],"key":"Lq1x16VP0F"}],"key":"C1feiZUYMY"},{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Compiles: Ensures the model is compiled with the correct learning rate (either default or from best_hps).","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"BlNbNcVFQv"}],"key":"qZWTyE6784"}],"key":"ZyD3Y9WHL0"},{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Callbacks: Sets up EarlyStopping and ModelCheckpoint. Note that ModelCheckpoint now saves the entire model in the .keras format (recommended over .h5), as we are not using the custom subclassed model in this part. The filename includes the model_type_name.","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"Ylz69koDRl"}],"key":"BDxb5l7CfN"}],"key":"Mny5M6xE9U"},{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Fits Model: Calls ","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"LUHkUL8kZw"},{"type":"link","url":"http://model.fit","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"model.fit","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"KinVSPyY7r"}],"urlSource":"http://model.fit","key":"FjIZSVGnuj"},{"type":"text","value":"(), passing the class_weight dictionary if applicable.","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"whem96Z1Io"}],"key":"ej1FDIOzV3"}],"key":"xd9kyelNIN"},{"type":"listItem","spread":true,"position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Loads Best: After training, it explicitly loads the best model saved by ModelCheckpoint to ensure the returned model represents the best validation performance.","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"DslvPuW31B"}],"key":"qE2dBclvV7"}],"key":"rjVliqrtZw"},{"type":"listItem","spread":true,"position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Returns: The trained model object and the training history.","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"aHsB852aid"}],"key":"zH0HRQL3gL"}],"key":"N44Jh3E0aN"}],"key":"e2QdD0Mc7Z"}],"key":"yDGF6hoNjT"},{"type":"listItem","spread":true,"position":{"start":{"line":16,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Main Execution:","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"JDhYjnSReu"}],"key":"sBtlmIBKfe"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":17,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Checks that necessary data (X_train, etc.) and configuration flags exist.","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"YwxGe1IXUK"}],"key":"KGRXJP5xse"}],"key":"AA3WjTlult"},{"type":"listItem","spread":true,"position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Determines the model_type_name based on whether tuning was performed and class weights are used (e.g., “baseline”, “baseline_weighted”, “tuned”, “tuned_weighted”).","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"V1Dmiaehef"}],"key":"XDtRfJ7aVI"}],"key":"O0e1hVQAbB"},{"type":"listItem","spread":true,"position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Calls the train_model function with the appropriate arguments.","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"iyBvkia77n"}],"key":"vnDix0khIU"}],"key":"Hiu9F4LCOg"},{"type":"listItem","spread":true,"position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Stores the returned trained model in the trained_model variable.","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"Jve7oVJrtI"}],"key":"vpFIelhsZL"}],"key":"DQRlCmuUtT"},{"type":"listItem","spread":true,"position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Plots the training/validation loss and accuracy from the returned history object.","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"TWVd7HCMxk"}],"key":"YymfCZxZxi"}],"key":"Fu5SJFBReu"},{"type":"listItem","spread":true,"position":{"start":{"line":22,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Sets a general model_ready_for_eval flag for the next step (Part 10: Baseline Evaluation).","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"E1Tq50IpKg"}],"key":"QISv560ybk"}],"key":"F1ynPmGXSe"}],"key":"VPkbIdXCd7"}],"key":"dFmrRV5CCA"}],"key":"DtdpddBJJA"},{"type":"paragraph","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"text","value":"After running this cell, the trained_model variable will hold the trained LSTM model (either using default or tuned hyperparameters, and potentially trained with class weights), ready for evaluation on the test set in Part 10. The best version of this model is also saved to a file.","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"key":"YcGkQKesqh"}],"key":"gOWSYoj57T"}],"identifier":"n5nvbjk-leet","label":"N5nvBjk-leEt","html_id":"n5nvbjk-leet","key":"OJR9xBJT9P"},{"type":"block","kind":"notebook-code","data":{"ExecuteTime":{"end_time":"2026-01-07T03:44:43.377227200Z","start_time":"2026-01-07T03:37:30.421Z"},"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"pZozx1KVj7t0","outputId":"8ae10d94-45eb-44c1-a42e-9d7ee94305cf"},"children":[{"type":"code","lang":"python","executable":true,"value":"def train_final_model(x_train, y_train, x_val, y_val,\n                      input_shape,\n                      use_class_weight=False,\n                      best_hps=None,\n                      epochs=50,\n                      batch_size=32,\n                      patience=10,\n                      checkpoint_path_template='output/model_{}.keras',\n                      model_name='baseline'):\n\n    print(f\"\\n--- Training Final Model: {model_name} ---\")\n\n    # 1. Build Model\n    # We assume build_lstm_model is available from Step 7\n    if 'build_lstm_model' not in globals():\n        raise NameError(\"build_lstm_model function not defined. Please run Step 7.\")\n\n    model = build_lstm_model(input_shape, hp=best_hps)\n    if model is None:\n        return None, None\n\n    # 2. Calculate Class Weights (Critical for Blooms)\n    class_weights_dict = None\n    if use_class_weight:\n        print(\"Calculating class weights for imbalanced data...\")\n        try:\n            # Flatten to ensure 1D array for weight calculation\n            y_flat = y_train.flatten().astype(int)\n            classes = np.unique(y_flat)\n            weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_flat)\n            class_weights_dict = dict(zip(classes, weights))\n            print(f\"  Class weights: {class_weights_dict}\")\n        except Exception as e:\n            print(f\"  Error calculating weights: {e}. Using default.\")\n\n    # 3. Compile Model with BETTER METRICS\n    # We add AUC, Precision, and Recall to see if it actually finds blooms\n    learning_rate = best_hps.get('learning_rate') if best_hps else DEFAULT_LEARNING_RATE\n\n    # Re-compile to ensure metrics and optimizer are fresh\n    model.compile(\n        optimizer=Adam(learning_rate=learning_rate),\n        loss=BinaryCrossentropy(),\n        metrics=[\n            BinaryAccuracy(name='accuracy'),\n            AUC(name='auc'),           # <--- Critical for Imbalanced Data\n            Precision(name='precision'),\n            Recall(name='recall')\n        ]\n    )\n    print(f\"Model compiled (LR={learning_rate}). Monitoring AUC/Precision/Recall.\")\n\n    # 4. Setup Callbacks\n    model_path = checkpoint_path_template.format(model_name)\n\n    # EarlyStopping: restore_best_weights=True ensures 'model' var is perfect at the end\n    early_stopping = EarlyStopping(\n        monitor='val_loss',\n        patience=patience,\n        verbose=1,\n        restore_best_weights=True\n    )\n\n    model_checkpoint = ModelCheckpoint(\n        filepath=model_path,\n        monitor='val_loss',\n        save_best_only=True,\n        verbose=1\n    )\n\n    # 5. Train\n    print(f\"\\nStarting training for {epochs} epochs...\")\n\n    # Ensure raw numpy arrays (safer than Tensors here)\n    x_train = np.array(x_train).astype('float32')\n    y_train = np.array(y_train).astype('float32')\n    x_val = np.array(x_val).astype('float32')\n    y_val = np.array(y_val).astype('float32')\n\n    history = model.fit(\n        x_train, y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        validation_data=(x_val, y_val),\n        callbacks=[early_stopping, model_checkpoint],\n        class_weight=class_weights_dict,\n        verbose=1\n    )\n\n    print(f\"\\n--- Training Finished. Best model saved to {model_path} ---\")\n\n    # Because restore_best_weights=True, 'model' is already the best version.\n    return model, history\n\n\n# --- Execute Training ---\nif __name__ == \"__main__\":\n    try:\n        # Check prerequisites\n        if 'X_train' not in locals(): raise NameError(\"Run Step 6 (Sequence Creation) first.\")\n\n        # Define Input Shape\n        input_shape_train = (X_train.shape[1], X_train.shape[2])\n\n        # Naming logic\n        model_name = \"tuned\" if (PERFORM_TUNING and best_hps) else \"baseline\"\n        if USE_CLASS_WEIGHT: model_name += \"_weighted\"\n\n        # TRAIN\n        trained_model, training_history = train_final_model(\n            X_train, y_train, X_val, y_val,\n            input_shape=input_shape_train,\n            use_class_weight=USE_CLASS_WEIGHT,\n            best_hps=best_hps if PERFORM_TUNING else None,\n            epochs=TRAIN_EPOCHS,\n            batch_size=TRAIN_BATCH_SIZE,\n            patience=TRAIN_PATIENCE,\n            checkpoint_path_template=MODEL_CHECKPOINT_TEMPLATE,\n            model_name=model_name\n        )\n\n        if trained_model:\n            # --- PUBLICATION QUALITY PLOTTING ---\n            print(f\"\\nGeneratng publication-quality plots for {model_name}...\")\n\n            import matplotlib.pyplot as plt\n            import os  # <--- Need this to create folders\n\n            # 1. CRITICAL FIX: Create the output directory if it doesn't exist\n            os.makedirs('output', exist_ok=True)\n\n            # 2. Global Style Settings for Academic Papers\n            plt.rcdefaults()\n            params = {\n                'font.family': 'serif',\n                'axes.titlesize': 18,\n                'axes.labelsize': 16,\n                'xtick.labelsize': 14,\n                'ytick.labelsize': 14,\n                'legend.fontsize': 14,\n                'figure.figsize': (10, 6),\n                'lines.linewidth': 2.5\n            }\n            plt.rcParams.update(params)\n\n            # --- FIGURE 1: LOSS CURVE ---\n            plt.figure(dpi=300)\n\n            plt.plot(training_history.history['loss'], label='Training Loss', color='#1f77b4', linestyle='-')\n            plt.plot(training_history.history['val_loss'], label='Validation Loss', color='#d62728', linestyle='--')\n\n            plt.title('Model Loss over Epochs')\n            plt.xlabel('Epoch')\n            plt.ylabel('Binary Crossentropy Loss')\n            plt.legend(frameon=True, fancybox=False, edgecolor='black')\n            plt.grid(True, linestyle=':', alpha=0.6)\n            plt.tight_layout()\n\n            # Save safely now that folder exists\n            plt.savefig(f\"output/plot_loss_{model_name}.png\", bbox_inches='tight')\n            plt.show()\n            print(f\"Saved: output/plot_loss_{model_name}.png\")\n\n            # --- FIGURE 2: AUC / PERFORMANCE CURVE ---\n            plt.figure(dpi=300)\n\n            if 'val_auc' in training_history.history:\n                metric_name = 'AUC'\n                train_data = training_history.history['auc']\n                val_data = training_history.history['val_auc']\n                color_train = '#2ca02c'\n                color_val = '#ff7f0e'\n            else:\n                metric_name = 'Accuracy'\n                train_data = training_history.history['accuracy']\n                val_data = training_history.history['val_accuracy']\n                color_train = 'black'\n                color_val = 'gray'\n\n            plt.plot(train_data, label=f'Training {metric_name}', color=color_train, linestyle='-')\n            plt.plot(val_data, label=f'Validation {metric_name}', color=color_val, linestyle='--')\n\n            plt.title(f'Model {metric_name} Performance')\n            plt.xlabel('Epoch')\n            plt.ylabel(f'{metric_name} Score')\n            plt.legend(frameon=True, fancybox=False, edgecolor='black')\n            plt.grid(True, linestyle=':', alpha=0.6)\n            plt.tight_layout()\n\n            plt.savefig(f\"output/plot_{metric_name.lower()}_{model_name}.png\", bbox_inches='tight')\n            plt.show()\n            print(f\"Saved: output/plot_{metric_name.lower()}_{model_name}.png\")\n\n            model_ready_for_eval = True\n        else:\n            model_ready_for_eval = False\n\n    except Exception as e:\n         print(f\"Error: {e}\")\n         model_ready_for_eval = False","identifier":"pzozx1kvj7t0-code","enumerator":"13","html_id":"pzozx1kvj7t0-code","key":"gp6916gB5e"},{"type":"outputs","id":"uQdCb_qrkK9r6gQXQ6arO","children":[{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"\n--- Training Final Model: tuned_weighted ---\nCalculating class weights for imbalanced data...\n  Class weights: {np.int64(0): np.float64(0.6527614571092832), np.int64(1): np.float64(2.1365384615384615)}\nModel compiled (LR=0.01). Monitoring AUC/Precision/Recall.\n\nStarting training for 50 epochs...\nEpoch 1/50\n\u001b[1m33/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7266 - auc: 0.7623 - loss: 0.5681 - precision: 0.4174 - recall: 0.6844\nEpoch 1: val_loss improved from inf to 0.47479, saving model to output_refactored/best_lstm_model_tuned_weighted.keras\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.7291 - auc: 0.7679 - loss: 0.5629 - precision: 0.4231 - recall: 0.6897 - val_accuracy: 0.7149 - val_auc: 0.9209 - val_loss: 0.4748 - val_precision: 0.5921 - val_recall: 0.9677\nEpoch 2/50\n\u001b[1m33/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8109 - auc: 0.9159 - loss: 0.3887 - precision: 0.5893 - recall: 0.9056\nEpoch 2: val_loss improved from 0.47479 to 0.38012, saving model to output_refactored/best_lstm_model_tuned_weighted.keras\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8106 - auc: 0.9141 - loss: 0.3908 - precision: 0.5869 - recall: 0.9001 - val_accuracy: 0.7851 - val_auc: 0.9227 - val_loss: 0.3801 - val_precision: 0.6930 - val_recall: 0.8495\nEpoch 3/50\n\u001b[1m33/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8125 - auc: 0.8956 - loss: 0.4167 - precision: 0.5762 - recall: 0.7902\nEpoch 3: val_loss did not improve from 0.38012\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8129 - auc: 0.8958 - loss: 0.4158 - precision: 0.5765 - recall: 0.7911 - val_accuracy: 0.7500 - val_auc: 0.9261 - val_loss: 0.5218 - val_precision: 0.6200 - val_recall: 1.0000\nEpoch 4/50\n\u001b[1m34/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8166 - auc: 0.9158 - loss: 0.3768 - precision: 0.5881 - recall: 0.8814\nEpoch 4: val_loss did not improve from 0.38012\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8169 - auc: 0.9158 - loss: 0.3764 - precision: 0.5876 - recall: 0.8810 - val_accuracy: 0.7982 - val_auc: 0.9179 - val_loss: 0.4197 - val_precision: 0.6975 - val_recall: 0.8925\nEpoch 5/50\n\u001b[1m33/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8182 - auc: 0.9030 - loss: 0.3871 - precision: 0.5642 - recall: 0.8112\nEpoch 5: val_loss did not improve from 0.38012\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8201 - auc: 0.9041 - loss: 0.3855 - precision: 0.5687 - recall: 0.8137 - val_accuracy: 0.8026 - val_auc: 0.9248 - val_loss: 0.3869 - val_precision: 0.7143 - val_recall: 0.8602\nEpoch 6/50\n\u001b[1m33/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8601 - auc: 0.9436 - loss: 0.3022 - precision: 0.6464 - recall: 0.8500\nEpoch 6: val_loss did not improve from 0.38012\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8589 - auc: 0.9429 - loss: 0.3041 - precision: 0.6444 - recall: 0.8512 - val_accuracy: 0.8070 - val_auc: 0.9126 - val_loss: 0.4281 - val_precision: 0.6992 - val_recall: 0.9247\nEpoch 7/50\n\u001b[1m34/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8743 - auc: 0.9397 - loss: 0.3216 - precision: 0.6860 - recall: 0.8777\nEpoch 7: val_loss did not improve from 0.38012\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8737 - auc: 0.9396 - loss: 0.3214 - precision: 0.6845 - recall: 0.8772 - val_accuracy: 0.8070 - val_auc: 0.9049 - val_loss: 0.4391 - val_precision: 0.7130 - val_recall: 0.8817\nEpoch 8/50\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8945 - auc: 0.9622 - loss: 0.2541 - precision: 0.7110 - recall: 0.9191\nEpoch 8: val_loss did not improve from 0.38012\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8941 - auc: 0.9618 - loss: 0.2551 - precision: 0.7106 - recall: 0.9175 - val_accuracy: 0.7675 - val_auc: 0.8939 - val_loss: 0.4210 - val_precision: 0.6786 - val_recall: 0.8172\nEpoch 9/50\n\u001b[1m34/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8959 - auc: 0.9683 - loss: 0.2292 - precision: 0.6880 - recall: 0.9288\nEpoch 9: val_loss did not improve from 0.38012\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8952 - auc: 0.9677 - loss: 0.2309 - precision: 0.6884 - recall: 0.9272 - val_accuracy: 0.7939 - val_auc: 0.8991 - val_loss: 0.4283 - val_precision: 0.7255 - val_recall: 0.7957\nEpoch 10/50\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9089 - auc: 0.9766 - loss: 0.1960 - precision: 0.7378 - recall: 0.9421\nEpoch 10: val_loss did not improve from 0.38012\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9084 - auc: 0.9763 - loss: 0.1970 - precision: 0.7368 - recall: 0.9415 - val_accuracy: 0.7675 - val_auc: 0.8948 - val_loss: 0.6039 - val_precision: 0.6493 - val_recall: 0.9355\nEpoch 11/50\n\u001b[1m33/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8969 - auc: 0.9649 - loss: 0.2501 - precision: 0.7243 - recall: 0.9167\nEpoch 11: val_loss did not improve from 0.38012\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8975 - auc: 0.9647 - loss: 0.2498 - precision: 0.7251 - recall: 0.9172 - val_accuracy: 0.7807 - val_auc: 0.9039 - val_loss: 0.4826 - val_precision: 0.6720 - val_recall: 0.9032\nEpoch 12/50\n\u001b[1m33/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9140 - auc: 0.9797 - loss: 0.1927 - precision: 0.7393 - recall: 0.9427\nEpoch 12: val_loss did not improve from 0.38012\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9143 - auc: 0.9795 - loss: 0.1926 - precision: 0.7418 - recall: 0.9412 - val_accuracy: 0.8070 - val_auc: 0.8981 - val_loss: 0.4655 - val_precision: 0.7248 - val_recall: 0.8495\nEpoch 12: early stopping\nRestoring model weights from the end of the best epoch: 2.\n\n--- Training Finished. Best model saved to output_refactored/best_lstm_model_tuned_weighted.keras ---\n\nGeneratng publication-quality plots for tuned_weighted...\n"},"children":[],"identifier":"pzozx1kvj7t0-outputs-0","html_id":"pzozx1kvj7t0-outputs-0","key":"z37tt4CZPz"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"<Figure size 3000x1800 with 1 Axes>","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"b86e62b0e945d1d21567d642183f78cb","path":"/build/b86e62b0e945d1d21567d642183f78cb.png"}}},"children":[],"identifier":"pzozx1kvj7t0-outputs-1","html_id":"pzozx1kvj7t0-outputs-1","key":"dgEyEVykFE"},{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"Saved: output/plot_loss_tuned_weighted.png\n"},"children":[],"identifier":"pzozx1kvj7t0-outputs-2","html_id":"pzozx1kvj7t0-outputs-2","key":"Ic1ZBW3YpP"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"<Figure size 3000x1800 with 1 Axes>","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"d6cb9c7ac05f37493a8bf8222db5cc21","path":"/build/d6cb9c7ac05f37493a8bf8222db5cc21.png"}}},"children":[],"identifier":"pzozx1kvj7t0-outputs-3","html_id":"pzozx1kvj7t0-outputs-3","key":"l63VZoPwCD"},{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"Saved: output/plot_auc_tuned_weighted.png\n"},"children":[],"identifier":"pzozx1kvj7t0-outputs-4","html_id":"pzozx1kvj7t0-outputs-4","key":"N8wyQEuYoB"}],"identifier":"pzozx1kvj7t0-outputs","html_id":"pzozx1kvj7t0-outputs","key":"AAPbTUODL0"}],"identifier":"pzozx1kvj7t0","label":"pZozx1KVj7t0","html_id":"pzozx1kvj7t0","key":"W4SO02rWYt"},{"type":"block","kind":"notebook-content","data":{"id":"SwZcbfubl7Br"},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Part 10: Baseline Evaluation","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"v71fJw5dYA"}],"identifier":"part-10-baseline-evaluation","label":"Part 10: Baseline Evaluation","html_id":"part-10-baseline-evaluation","implicit":true,"key":"HpmZFTvPqV"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"This cell defines the evaluation function and applies it to the model trained in Part 9 (trained_model). It calculates and prints various classification metrics, plots the confusion matrix and predictions over time, and assesses the model’s ability to predict bloom onsets.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"xdkpX22btx"}],"key":"DBYjwNi88f"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Explanation:","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"NhOilrzi03"}],"key":"INPCt4Ycz8"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":7,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Function evaluate_model:","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"xKEjnrXuMR"}],"key":"qBGqfKNj2a"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":8,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Takes the model object, test data (x_test, y_test), the unscaled test dataframe test_df_unscaled (needed for correct timestamps), sequence length, forecast horizon, and a model_name string as input.","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"dIii2c94bp"}],"key":"cRHhqpoSc5"}],"key":"p6h5YDOh5L"},{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Predictions: Runs model.predict(x_test) to get probabilities. Converts probabilities to class predictions using a 0.5 threshold.","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"ZzdAPbqeyq"}],"key":"grNfPQH6Zm"}],"key":"IxcfhXHezT"},{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Metrics Calculation: Uses scikit-learn functions to calculate accuracy, precision, recall, F1-score (for the positive class ‘1’), AUC-ROC (using probabilities), and Brier score (using probabilities). Includes error handling for AUC calculation if only one class is present in y_test.","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"tMv82Zgnpy"}],"key":"NmNeq52lYt"}],"key":"TRB350hBUc"},{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Reporting: Prints a formatted summary of the metrics and the full classification report.\\","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"CBXfARJ5wY"}],"key":"mAZvOpnoDR"}],"key":"yrhIfOEhOz"},{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Confusion Matrix: Calculates and plots the confusion matrix using seaborn for better visualization.","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"FTkejB5l8j"}],"key":"Vay7bQWQ6c"}],"key":"Cot9WitibA"},{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Time Series Plot: Determines the correct start index in the test_df_unscaled index based on sequence length and forecast horizon to align the predictions (y_pred_prob) with the actual dates. Plots actuals vs. predicted probabilities. Includes checks for length mismatches.","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"pcqE56Wdua"}],"key":"Fa4ZwVmCkQ"}],"key":"nvJ0cTVlq8"},{"type":"listItem","spread":true,"position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Onset Analysis: Implements the logic to find actual bloom onsets (0 -> 1 transitions in y_test) and checks if the model predicted a bloom (class 1) either in the week before the onset or during the onset week. Reports the hit rate.","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"GisxKy70Hh"}],"key":"cxNIzjjW3V"}],"key":"HwHAYtzxNw"}],"key":"E1kwCn3reV"}],"key":"OTWXs347U4"},{"type":"listItem","spread":true,"position":{"start":{"line":15,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Main Execution:","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"N9Yu34rH4E"}],"key":"p5WaFPNpnH"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":16,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Checks if the necessary variables (trained_model, X_test, y_test, test_df, etc.) exist from previous steps.","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"Nx56c6ieFQ"}],"key":"SYv7ptnbHP"}],"key":"dQZdLFRhFv"},{"type":"listItem","spread":true,"position":{"start":{"line":17,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Calls the evaluate_model function, passing the required arguments. Uses the model_type_trained variable (set at the end of Part 9) to label the output correctly.","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"NLzh9R8BmE"}],"key":"M8FB3ay4qN"}],"key":"AM90NkwcFh"}],"key":"ce0ExxPesJ"}],"key":"bkaCp0ybCl"}],"key":"WzEW6hwiZn"},{"type":"paragraph","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"text","value":"After running this cell, you will have a comprehensive evaluation of the model trained in Part 9, including standard metrics, plots, and the crucial bloom onset performance assessment. This provides the baseline against which the EnKF-enhanced model (Part 12/13) will be compared.","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"ormDIezEph"}],"key":"pDHTyK1dkN"}],"identifier":"swzcbfubl7br","label":"SwZcbfubl7Br","html_id":"swzcbfubl7br","key":"azJQo8R6vy"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"mByYeroPl_Va","outputId":"89218487-72ba-4a9c-c2e5-2a9b6c5912f6","ExecuteTime":{"end_time":"2026-01-07T03:44:43.377227200Z","start_time":"2026-01-07T03:37:37.043182Z"}},"children":[{"type":"code","lang":"python","executable":true,"value":"def evaluate_model(model, x_test, y_test, test_df_unscaled,\n                   seq_len, forecast_horizon, model_name=\"Model\"):\n    \"\"\"\n    Evaluates model with Optimal Threshold detection and Publication Plots.\n    Saves figures to 'output/' folder.\n    \"\"\"\n    if model is None or x_test is None or y_test is None:\n        print(\"Error: Missing model or test data.\")\n        return\n\n    # Ensure output directory exists\n    os.makedirs('output', exist_ok=True)\n\n    print(f\"\\n=== Evaluating Model: {model_name} ===\")\n\n    # 1. Generate Probabilities\n    print(\"1. Generating predictions...\")\n    try:\n        # Get raw probabilities (0.0 to 1.0)\n        y_pred_prob = model.predict(x_test, verbose=0)\n        y_test_eval = y_test.reshape(-1, 1) # Ensure 2D shape\n    except Exception as e:\n        print(f\"Error during prediction: {e}\")\n        return\n\n    # 2. Find Optimal Threshold (Fixed Math)\n    # ------------------------------------------------------\n    precision, recall, thresholds = precision_recall_curve(y_test_eval, y_pred_prob)\n\n    # Calculate F-Score safely (avoid 0/0 division)\n    numerator = 2 * precision * recall\n    denominator = precision + recall\n    with np.errstate(divide='ignore', invalid='ignore'):\n        fscore = np.divide(numerator, denominator)\n    fscore = np.nan_to_num(fscore) # Replace NaNs with 0\n\n    # Locate the index of the largest F score\n    ix = np.argmax(fscore)\n\n    # Safety check for index bounds\n    if ix >= len(thresholds):\n        best_thresh = thresholds[-1]\n    else:\n        best_thresh = thresholds[ix]\n\n    print(f\"\\nOptimal Decision Threshold: {best_thresh:.4f} (Max F1: {fscore[ix]:.4f})\")\n    # ------------------------------------------------------\n\n    # Apply Best Threshold\n    y_pred_class = (y_pred_prob >= best_thresh).astype(int)\n\n    # 3. Calculate Metrics (at Best Threshold)\n    print(\"\\n2. Performance Metrics (at Optimal Threshold):\")\n    try:\n        acc = accuracy_score(y_test_eval, y_pred_class)\n        prec = precision_score(y_test_eval, y_pred_class, zero_division=0)\n        rec = recall_score(y_test_eval, y_pred_class, zero_division=0)\n        f1 = f1_score(y_test_eval, y_pred_class, zero_division=0)\n        auc = roc_auc_score(y_test_eval, y_pred_prob)\n        brier = brier_score_loss(y_test_eval, y_pred_prob)\n\n        print(f\"  Accuracy:    {acc:.4f}\")\n        print(f\"  Precision:   {prec:.4f}\")\n        print(f\"  Recall:      {rec:.4f}\")\n        print(f\"  F1-Score:    {f1:.4f}\")\n        print(f\"  AUC-ROC:     {auc:.4f}\")\n        print(f\"  Brier Score: {brier:.4f}\")\n\n        print(f\"\\nClassification Report:\\n{classification_report(y_test_eval, y_pred_class, target_names=['No Bloom', 'Bloom'])}\")\n\n    except Exception as e:\n        print(f\"Metric calculation error: {e}\")\n\n    # 4. Publication-Quality Plots\n    # ----------------------------\n    # Apply Academic Style\n    plt.rcdefaults()\n    params = {'font.family': 'serif', 'figure.figsize': (8, 6), 'figure.dpi': 300,\n              'axes.labelsize': 14, 'axes.titlesize': 16, 'xtick.labelsize': 12, 'ytick.labelsize': 12}\n    plt.rcParams.update(params)\n\n    # --- Plot A: Confusion Matrix ---\n    try:\n        cm = confusion_matrix(y_test_eval, y_pred_class)\n        plt.figure()\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n                    xticklabels=['Pred: No Bloom', 'Pred: Bloom'],\n                    yticklabels=['Actual: No Bloom', 'Actual: Bloom'],\n                    annot_kws={\"size\": 14})\n        plt.title(f'Confusion Matrix ({model_name})\\nThreshold: {best_thresh:.2f}')\n        plt.tight_layout()\n\n        # SAVE FIGURE\n        save_path = f\"output/confusion_matrix_{model_name}.png\"\n        plt.savefig(save_path, bbox_inches='tight')\n        print(f\"Saved: {save_path}\")\n        plt.show()\n    except Exception as e:\n        print(f\"CM Plot Error: {e}\")\n\n    # --- Plot B: Time Series Predictions ---\n    print(f\"\\n3. Plotting Time Series Predictions...\")\n    start_idx = seq_len + forecast_horizon - 1\n    num_preds = len(y_pred_prob)\n\n    if start_idx + num_preds <= len(test_df_unscaled):\n        dates = test_df_unscaled.index[start_idx : start_idx + num_preds]\n\n        plt.figure(figsize=(12, 6))\n        # Plot Actuals (Black Line)\n        plt.plot(dates, y_test_eval[:num_preds], label='Actual Bloom', color='black', alpha=0.6, linewidth=1.5)\n        # Plot Probabilities (Red Line)\n        plt.plot(dates, y_pred_prob[:num_preds], label='Predicted Probability', color='#d62728', alpha=0.8, linewidth=1.5)\n        # Threshold Line\n        plt.axhline(best_thresh, color='gray', linestyle='--', label=f'Threshold ({best_thresh:.2f})')\n\n        plt.title(f'Bloom Predictions vs Actuals ({forecast_horizon}-Step Horizon)')\n        plt.ylabel('Bloom Probability')\n        plt.xlabel('Date')\n        plt.legend(frameon=True, fancybox=False, edgecolor='black', loc='upper right')\n        plt.grid(True, linestyle=':', alpha=0.5)\n        plt.tight_layout()\n\n        # SAVE FIGURE\n        save_path = f\"output/timeseries_preds_{model_name}.png\"\n        plt.savefig(save_path, bbox_inches='tight')\n        print(f\"Saved: {save_path}\")\n        plt.show()\n    else:\n        print(\"Warning: Could not align dates for plotting.\")\n\n    # 5. Bloom Onset Analysis (Custom Logic)\n    print(f\"\\n4. Bloom Onset Analysis (Did we catch the start?)\")\n    y_test_flat = y_test_eval.flatten()\n    y_pred_flat = y_pred_class.flatten()\n\n    onsets = np.where((y_test_flat[:-1] == 0) & (y_test_flat[1:] == 1))[0] + 1\n    total_onsets = len(onsets)\n\n    hits = 0\n    late_hits = 0\n\n    if total_onsets > 0:\n        for idx in onsets:\n            # Check window: [Week Before, Week Of, Week After]\n            pred_before = y_pred_flat[idx-1] if idx > 0 else 0\n            pred_on     = y_pred_flat[idx]\n            pred_after  = y_pred_flat[idx+1] if idx+1 < len(y_pred_flat) else 0\n\n            if pred_before == 1 or pred_on == 1:\n                hits += 1 # Success (Early or On Time)\n            elif pred_after == 1:\n                late_hits += 1 # Late by 1 week\n\n        print(f\"  Total Onsets: {total_onsets}\")\n        print(f\"  Caught Early/On-Time: {hits} ({hits/total_onsets:.1%})\")\n        print(f\"  Caught 1-Week Late:   {late_hits}\")\n        print(f\"  Missed Completely:    {total_onsets - hits - late_hits}\")\n    else:\n        print(\"  No bloom onsets found in Test Data.\")\n\n# --- Main Execution ---\nif __name__ == \"__main__\":\n    if 'trained_model' in locals() and 'X_test' in locals():\n        # Ensure data is numpy float32\n        X_test = np.array(X_test, dtype='float32')\n        y_test = np.array(y_test, dtype='float32')\n\n        evaluate_model(\n            model=trained_model,\n            x_test=X_test,\n            y_test=y_test,\n            test_df_unscaled=test_df, # Must be defined in Step 4\n            seq_len=SEQUENCE_LENGTH,\n            forecast_horizon=FORECAST_HORIZON,\n            model_name=\"LSTM_Baseline\"\n        )\n    else:\n        print(\"Error: Prerequisites (trained_model, X_test) not found.\")","identifier":"mbyyeropl_va-code","enumerator":"14","html_id":"mbyyeropl-va-code","key":"AHvXW4XHsU"},{"type":"outputs","id":"9KRBcgFUVPAbaDQwD6-_8","children":[{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"\n=== Evaluating Model: LSTM_Baseline ===\n1. Generating predictions...\n\nOptimal Decision Threshold: 0.7803 (Max F1: 0.8662)\n\n2. Performance Metrics (at Optimal Threshold):\n  Accuracy:    0.9087\n  Precision:   0.8395\n  Recall:      0.8947\n  F1-Score:    0.8662\n  AUC-ROC:     0.9532\n  Brier Score: 0.1238\n\nClassification Report:\n              precision    recall  f1-score   support\n\n    No Bloom       0.95      0.92      0.93       154\n       Bloom       0.84      0.89      0.87        76\n\n    accuracy                           0.91       230\n   macro avg       0.89      0.91      0.90       230\nweighted avg       0.91      0.91      0.91       230\n\nSaved: output/confusion_matrix_LSTM_Baseline.png\n"},"children":[],"identifier":"mbyyeropl_va-outputs-0","html_id":"mbyyeropl-va-outputs-0","key":"oF1bvCiYhG"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"<Figure size 2400x1800 with 1 Axes>","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"ef8770c7c1e7a96bc41ace227ceb41d8","path":"/build/ef8770c7c1e7a96bc41ace227ceb41d8.png"}}},"children":[],"identifier":"mbyyeropl_va-outputs-1","html_id":"mbyyeropl-va-outputs-1","key":"tNkE8dok2B"},{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"\n3. Plotting Time Series Predictions...\nSaved: output/timeseries_preds_LSTM_Baseline.png\n"},"children":[],"identifier":"mbyyeropl_va-outputs-2","html_id":"mbyyeropl-va-outputs-2","key":"pyrU6QUJVB"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"<Figure size 3600x1800 with 1 Axes>","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"0e88d802c5f2b8367e0a5d7ed6061a4d","path":"/build/0e88d802c5f2b8367e0a5d7ed6061a4d.png"}}},"children":[],"identifier":"mbyyeropl_va-outputs-3","html_id":"mbyyeropl-va-outputs-3","key":"bQuJ0FnOe1"},{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"\n4. Bloom Onset Analysis (Did we catch the start?)\n  Total Onsets: 10\n  Caught Early/On-Time: 6 (60.0%)\n  Caught 1-Week Late:   2\n  Missed Completely:    2\n"},"children":[],"identifier":"mbyyeropl_va-outputs-4","html_id":"mbyyeropl-va-outputs-4","key":"MwnIiw0gob"}],"identifier":"mbyyeropl_va-outputs","html_id":"mbyyeropl-va-outputs","key":"tdbkz5rrRO"}],"identifier":"mbyyeropl_va","label":"mByYeroPl_Va","html_id":"mbyyeropl-va","key":"O9zPO7K2W5"},{"type":"block","kind":"notebook-content","data":{"id":"TYCWxz1amXpp"},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Part 11: EnKF Setup","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fv6SucYNy1"}],"identifier":"part-11-enkf-setup","label":"Part 11: EnKF Setup","html_id":"part-11-enkf-setup","implicit":true,"key":"au3jeWKBxu"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"This cell defines the EnsembleKalmanFilter class (assuming it’s not in a separate enkf.py file for simplicity here, but importing is better practice) and a function setup_enkf to initialize the filter for the test period. It calculates the initial state, noise matrices (based on the ENKF_NOISE_ESTIMATION setting from Part 1), and identifies the necessary column indices.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"e2VeXMzwXE"}],"key":"mzp6eQAZ5k"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Explanation:","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"nQBnxAFrBI"}],"key":"fyiraaSHsy"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":7,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"EnKF Class: Includes the EnsembleKalmanFilter class definition directly in the cell for convenience (assuming it’s not imported from enkf.py).","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"Y1GmOf0Bed"}],"key":"DHiUf5pRBO"}],"key":"bTOjeh0rmB"},{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"setup_enkf Function:","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"Lt6CSWQppb"}],"key":"kJyxvdAEEz"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":11,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Takes necessary configurations and dataframes as input.","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"Z2KeM3CpLe"}],"key":"tLdpA4anW3"}],"key":"MXQu1FHHvG"},{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Initial State/Covariance: Extracts the state vector values (x_initial) from the unscaled test_df at the correct time index (end of the first sequence’s lookback period). Calculates an initial covariance matrix P_initial based on relative uncertainty assumptions (these ratios are tunable).","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"trhBjRe24M"}],"key":"x9DopvXLxY"}],"key":"wiIEAbbUsO"},{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Noise Matrices (R, Q): Calculates the observation noise R and process noise Q based on the noise_estimation_method.","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"welz16D1X5"}],"key":"QiOjEVrIPo"}],"key":"T2AWYHRvKJ"},{"type":"listItem","spread":true,"position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"If ‘manual’, it uses MANUAL_R_DIAG and MANUAL_Q_DIAG (defined in Part 1).","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"fRo89gWEXT"}],"key":"eONLJmYbap"}],"key":"umVmPF5MpZ"},{"type":"listItem","spread":true,"position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"If ‘basic_stats’, it calculates variances based on the mean (for R) and standard deviation of weekly changes (for Q) from the unscaled train_df, scaled by tunable percentage factors. Includes fallback logic for constant columns.","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"Yk5raijShE"}],"key":"oBtlwCoR43"}],"key":"FqKhUfmpMF"},{"type":"listItem","spread":true,"position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"State Indices: Finds the numerical index corresponding to each ENKF_STATE_VARS within the final_feature_columns_used list (which defines the order in the scaled data and LSTM input). This is crucial for correctly updating the LSTM input later.","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"WvK1v1o0Hd"}],"key":"b0BQW37K3X"}],"key":"pzxKmEczPw"},{"type":"listItem","spread":true,"position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Initialization: Creates an instance of the EnsembleKalmanFilter class using the calculated x_initial, P_initial, and other parameters. Stores the calculated R and Q matrices as attributes of the instance for easy access in the next step.","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"pqFidhMF2p"}],"key":"s9GXFPd4Z3"}],"key":"EMXyjQQHb9"},{"type":"listItem","spread":true,"position":{"start":{"line":18,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Returns: The initialized enkf object and the list of enkf_state_indices.","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"LW3AJsmR0k"}],"key":"hycnb5YHv5"}],"key":"wq4Exl6tf1"}],"key":"oTddNd3t51"}],"key":"EBh0q0ESmc"},{"type":"listItem","spread":true,"position":{"start":{"line":20,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"text","value":"Main Execution:","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"GOrlj1U8pN"}],"key":"OgPF3Y8vsi"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":21,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Checks for prerequisite variables from previous steps.","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"bSdXmO90Nx"}],"key":"sB9tBIZ7db"}],"key":"PFZtLDlvon"},{"type":"listItem","spread":true,"position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Calls setup_enkf with the appropriate arguments from the configuration (Part 1) and dataframes (Parts 4, 5).","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"b7uTetBfoj"}],"key":"roQOk37D9n"}],"key":"XsRqrbwYxJ"},{"type":"listItem","spread":true,"position":{"start":{"line":23,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Stores the returned enkf_instance and enkf_indices for use in the forecasting loop (Part 12).","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"f4GTHjgTtu"}],"key":"Nrz9yqkrr1"}],"key":"Ww2qEaI8KQ"}],"key":"kG2mZxrLwc"}],"key":"ON9QvduNeD"}],"key":"gfY8Uudg6R"},{"type":"paragraph","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"text","value":"After running this cell, the enkf_instance is ready, configured with initial conditions and noise characteristics appropriate for the start of the test period. The enkf_indices list tells us which columns in the LSTM input sequence correspond to the variables being updated by the EnKF.","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"tX4qGRPqeP"}],"key":"Iiv2by6C2n"}],"identifier":"tycwxz1amxpp","label":"TYCWxz1amXpp","html_id":"tycwxz1amxpp","key":"z36rmJd4CK"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"z4EXqf8cmcLW","outputId":"452405b5-d00b-4ffd-c430-335bc6ec6eb0","jupyter":{"is_executing":true}},"children":[{"type":"code","lang":"python","executable":true,"value":"class EnsembleKalmanFilter:\n    \"\"\"\n    Stochastic Ensemble Kalman Filter (Burgers et al., 1998).\n    Includes observation perturbation to maintain ensemble variance.\n    \"\"\"\n    def __init__(self, x_init, P_init, dim_z, N):\n        \"\"\"\n        Args:\n            x_init: Initial state mean (dim_x,)\n            P_init: Initial state covariance (dim_x, dim_x)\n            dim_z:  Dimension of observations\n            N:      Number of ensemble members\n        \"\"\"\n        self.dim_x = len(x_init)\n        self.dim_z = dim_z\n        self.N = N\n        self.x = None # Current state mean\n        self.P = None # Current state covariance\n\n        # Initialize Ensemble\n        # We enforce symmetry and positive-definiteness on P_init\n        P_init = (P_init + P_init.T) / 2\n        self.ensemble = self._multivariate_normal(x_init, P_init, N)\n\n        # Calculate initial stats\n        self.x = np.mean(self.ensemble, axis=0)\n        self.P = np.cov(self.ensemble.T)\n\n        print(f\"EnKF Initialized. Members: {N}, State Dim: {self.dim_x}\")\n\n    def predict(self, F_func, Q, dt=1):\n        \"\"\"\n        Propagate state forward: x = F(x) + Noise\n        \"\"\"\n        # 1. Apply Model Dynamics (The \"Physics\" or \"AI\")\n        # We apply F_func to every member of the ensemble\n        self.ensemble = np.apply_along_axis(F_func, 1, self.ensemble)\n\n        # 2. Add Process Noise (System Error)\n        if Q is not None:\n            Q = (Q + Q.T) / 2\n            noise = self._multivariate_normal(np.zeros(self.dim_x), Q, self.N)\n            self.ensemble += noise\n\n        # Update stats\n        self.x = np.mean(self.ensemble, axis=0)\n        self.P = np.cov(self.ensemble.T)\n\n    def update(self, z, R, H=None):\n        \"\"\"\n        Assimilate observation: x = x + K(z - Hx)\n        \"\"\"\n        if H is None: H = np.eye(self.dim_z, self.dim_x)\n\n        # 1. Project Ensemble to Observation Space\n        # Shape: (N, dim_z)\n        Hx = self.ensemble @ H.T\n\n        # 2. Perturb Observations (CRITICAL for Stochastic EnKF)\n        # We treat the observation as a random variable, not a single truth\n        R = (R + R.T) / 2\n        obs_noise = self._multivariate_normal(np.zeros(self.dim_z), R, self.N)\n        z_perturbed = z + obs_noise\n\n        # 3. Calculate Innovation (Residual)\n        # Difference between \"noisy measurement\" and \"predicted measurement\"\n        D = z_perturbed - Hx\n\n        # 4. Calculate Kalman Gain (K)\n        # P_zz = H P H' + R\n        # K = P H' (P_zz)^-1\n        P_prior = np.cov(self.ensemble.T)\n        P_zz = H @ P_prior @ H.T + R\n\n        # Use Pseudo-Inverse for stability\n        K = P_prior @ H.T @ np.linalg.pinv(P_zz)\n\n        # 5. Update Ensemble\n        # x_new = x_old + K * Innovation\n        self.ensemble = self.ensemble + (D @ K.T)\n\n        # Update stats\n        self.x = np.mean(self.ensemble, axis=0)\n        self.P = np.cov(self.ensemble.T)\n\n        return self.x, self.P\n\n    def _multivariate_normal(self, mean, cov, size):\n        \"\"\"Helper to sample safely, adding jitter if matrix is singular.\"\"\"\n        try:\n            return np.random.multivariate_normal(mean, cov, size)\n        except np.linalg.LinAlgError:\n            # Add small \"jitter\" to diagonal to fix numerical instability\n            print(\"Warning: Matrix not positive definite. Adding jitter.\")\n            epsilon = 1e-6 * np.eye(len(mean))\n            return np.random.multivariate_normal(mean, cov + epsilon, size)\n\n\n# --- 2. Setup Function (Robust with Manual Overrides) ---\ndef setup_enkf(enkf_state_vars, n_enkf, train_df, test_df_unscaled,\n               seq_len, feature_columns, manual_r_diag=None, manual_q_diag=None):\n    \"\"\"\n    Configures the EnKF.\n    Allows manual override of Q (Process) and R (Observation) noise diagonals.\n    \"\"\"\n    print(\"\\n--- Setting up EnKF ---\")\n\n    # Validation\n    if not all(col in test_df_unscaled.columns for col in enkf_state_vars):\n        print(f\"Error: Missing state variables in test data.\")\n        return None, None, None, None\n\n    dim_x = len(enkf_state_vars)\n\n    # A. Initial State (x0)\n    start_idx = seq_len - 1\n    x_init = test_df_unscaled[enkf_state_vars].iloc[start_idx].values.astype(float)\n    print(f\"Initial State Vector (t=0): {x_init}\")\n\n    # B. Initial Covariance (P0)\n    P_init = np.diag((np.abs(x_init) * 0.30) ** 2) + 1e-6 * np.eye(dim_x)\n\n    # C. Noise Matrices (Q and R)\n\n    # --- R MATRIX (Observation Noise) ---\n    if manual_r_diag is not None:\n        if len(manual_r_diag) != dim_x:\n            print(f\"Error: Manual R diag length ({len(manual_r_diag)}) does not match state vars ({dim_x})\")\n            return None, None, None, None\n        print(\"-> Using MANUAL R Matrix values.\")\n        R_matrix = np.diag(manual_r_diag)\n    else:\n        print(\"-> Calculating Automatic R Matrix (1% of mean).\")\n        r_diags = []\n        for col in enkf_state_vars:\n            mean_val = train_df[col].mean()\n            r_diags.append((abs(mean_val) * 0.01) ** 2)\n        R_matrix = np.diag(r_diags)\n\n    # --- Q MATRIX (Process Noise) ---\n    if manual_q_diag is not None:\n        if len(manual_q_diag) != dim_x:\n            print(f\"Error: Manual Q diag length ({len(manual_q_diag)}) does not match state vars ({dim_x})\")\n            return None, None, None, None\n        print(\"-> Using MANUAL Q Matrix values.\")\n        Q_matrix = np.diag(manual_q_diag)\n    else:\n        print(\"-> Calculating Automatic Q Matrix (based on volatility).\")\n        q_diags = []\n        for col in enkf_state_vars:\n            volatility = train_df[col].diff().std()\n            if pd.isna(volatility) or volatility == 0: volatility = 1e-3\n            q_diags.append((volatility * 2.0) ** 2)\n        Q_matrix = np.diag(q_diags)\n\n    print(\"Noise Matrices Finalized:\")\n    print(f\"  R (Obs Noise) diag: {np.diag(R_matrix)}\")\n    print(f\"  Q (Proc Noise) diag: {np.diag(Q_matrix)}\")\n\n    # D. Feature Indices\n    try:\n        enkf_indices = [feature_columns.index(var) for var in enkf_state_vars]\n    except ValueError as e:\n        print(f\"Error mapping variables to features: {e}\")\n        return None, None, None, None\n\n    # E. Create Instance\n    enkf = EnsembleKalmanFilter(x_init, P_init, dim_z=dim_x, N=n_enkf)\n    enkf.R = R_matrix\n    enkf.Q = Q_matrix\n\n    return enkf, enkf_indices, R_matrix, Q_matrix\n\n# --- 3. Execute ---\nif __name__ == \"__main__\":\n    try:\n        # Configuration check\n        REQUIRED = ['ENKF_STATE_VARS', 'N_ENKF', 'train_df', 'test_df',\n                    'SEQUENCE_LENGTH', 'final_feature_columns_used']\n\n        for var in REQUIRED:\n            if var not in locals(): raise NameError(f\"{var} is missing.\")\n\n\n        if (ENKF_NOISE_ESTIMATION == 'manual'):\n          MANUAL_Q = MANUAL_Q_DIAG\n          MANUAL_R = MANUAL_R_DIAG\n        else:\n          MANUAL_Q = None\n          MANUAL_R = None\n\n        enkf_instance, enkf_indices, R_matrix_enkf, Q_matrix_enkf = setup_enkf(\n            enkf_state_vars=ENKF_STATE_VARS,\n            n_enkf=N_ENKF,\n            train_df=train_df,\n            test_df_unscaled=test_df,\n            seq_len=SEQUENCE_LENGTH,\n            feature_columns=final_feature_columns_used,\n            manual_r_diag=MANUAL_R,\n            manual_q_diag=MANUAL_Q\n        )\n\n        if enkf_instance:\n            print(\"\\nEnKF Setup Complete.\")\n\n    except Exception as e:\n        print(f\"EnKF Setup Failed: {e}\")\n\nif 'Q_matrix_enkf' in locals() and 'R_matrix_enkf' in locals() and 'ENKF_STATE_VARS' in locals():\n\n    print(\"\\nGenerating publication-quality heatmaps for Q and R matrices...\")\n\n    # --- STYLE SETTINGS (Academic Standard) ---\n    plt.rcdefaults()\n    params = {\n        'font.family': 'serif',\n        'font.serif': ['Times New Roman', 'Times', 'DejaVu Serif'],\n        'axes.titlesize': 18,\n        'axes.labelsize': 14,\n        'xtick.labelsize': 12,\n        'ytick.labelsize': 12,\n        'figure.dpi': 300\n    }\n    plt.rcParams.update(params)\n\n    # Helper function to plot and save\n    def plot_covariance_matrix(matrix, title, filename, labels, cmap='Blues'):\n        plt.figure(figsize=(8, 7))\n\n        sns.heatmap(matrix, annot=True, fmt='.2e', cmap=cmap,\n                    xticklabels=labels, yticklabels=labels,\n                    square=True, cbar_kws={'label': 'Variance / Covariance'},\n                    linewidths=1, linecolor='black')\n\n        plt.title(title, pad=20)\n        plt.xticks(rotation=45, ha='right')\n        plt.yticks(rotation=0)\n        plt.tight_layout()\n\n        save_path = f\"output/{filename}\"\n        plt.savefig(save_path, bbox_inches='tight')\n        print(f\"Saved: {save_path}\")\n        plt.show()\n\n    # --- 1. Plot Q Matrix (Process Noise) ---\n    plot_covariance_matrix(\n        Q_matrix_enkf,\n        title=\"Process Noise Covariance ($Q$)\",\n        filename=\"matrix_Q_heatmap.png\",\n        labels=ENKF_STATE_VARS,\n        cmap=\"Reds\"\n    )\n\n    # --- 2. Plot R Matrix (Observation Noise) ---\n    plot_covariance_matrix(\n        R_matrix_enkf,\n        title=\"Observation Noise Covariance ($R$)\",\n        filename=\"matrix_R_heatmap.png\",\n        labels=ENKF_STATE_VARS,\n        cmap=\"Blues\"\n    )\n\nelse:\n    print(\"Error: Q and R matrices (or state vars) not found. Run Step 11 first.\")","identifier":"z4exqf8cmclw-code","enumerator":"15","html_id":"z4exqf8cmclw-code","key":"rntHv0otWq"},{"type":"outputs","id":"w7befQzQQExYyPRQSA8F5","children":[{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"\n--- Setting up EnKF ---\nInitial State Vector (t=0): [ 2.98000000e+03  1.84000000e+00  6.50000000e-01  0.00000000e+00\n  7.39418622e-01 -1.05741789e+01]\n-> Using MANUAL R Matrix values.\n-> Using MANUAL Q Matrix values.\nNoise Matrices Finalized:\n  R (Obs Noise) diag: [1.00e+00 4.00e-06 4.00e-08 6.25e-02 6.25e-02 1.00e-06]\n  Q (Proc Noise) diag: [9.00e+04 3.60e-01 3.60e-03 2.25e+02 2.25e+02 1.00e-02]\nEnKF Initialized. Members: 50, State Dim: 6\n\nEnKF Setup Complete.\n\nGenerating publication-quality heatmaps for Q and R matrices...\nSaved: output/matrix_Q_heatmap.png\n"},"children":[],"identifier":"z4exqf8cmclw-outputs-0","html_id":"z4exqf8cmclw-outputs-0","key":"DUU7ioTwfh"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"<Figure size 2400x2100 with 2 Axes>","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"297a56677509f12f09f292ce01693c36","path":"/build/297a56677509f12f09f292ce01693c36.png"}}},"children":[],"identifier":"z4exqf8cmclw-outputs-1","html_id":"z4exqf8cmclw-outputs-1","key":"iviuv8tkyH"},{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"Saved: output/matrix_R_heatmap.png\n"},"children":[],"identifier":"z4exqf8cmclw-outputs-2","html_id":"z4exqf8cmclw-outputs-2","key":"CcrqsQy9nX"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"<Figure size 2400x2100 with 2 Axes>","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"f383b5ede29d94c371eb8d0ec2739f41","path":"/build/f383b5ede29d94c371eb8d0ec2739f41.png"}}},"children":[],"identifier":"z4exqf8cmclw-outputs-3","html_id":"z4exqf8cmclw-outputs-3","key":"Ooo80913E2"}],"identifier":"z4exqf8cmclw-outputs","html_id":"z4exqf8cmclw-outputs","key":"ucAn76x7zC"}],"identifier":"z4exqf8cmclw","label":"z4EXqf8cmcLW","html_id":"z4exqf8cmclw","key":"l9QzbyGuay"},{"type":"block","kind":"notebook-content","data":{"id":"Nb41VBFwmjOe"},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Part 12: EnKF + MC Dropout Forecasting Loop","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zJZNvoiE3k"}],"identifier":"part-12-enkf-mc-dropout-forecasting-loop","label":"Part 12: EnKF + MC Dropout Forecasting Loop","html_id":"part-12-enkf-mc-dropout-forecasting-loop","implicit":true,"key":"inlC1rIYOP"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"This section implements the core forecasting engine, which synthesizes the data assimilation capabilities of the Ensemble Kalman Filter (EnKF) with the predictive power and uncertainty estimation of the LSTM. The loop iterates through the test period, sequentially updating environmental states and generating probabilistic forecasts.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"fkZpcuxsMb"}],"key":"irFThd6Rds"},{"type":"heading","depth":4,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Functionality: ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"ZIsHSmaHpz"},{"type":"inlineCode","value":"run_enkf_mc_forecast","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"slmWTyPARo"}],"identifier":"functionality-run-enkf-mc-forecast","label":"Functionality: run_enkf_mc_forecast","html_id":"functionality-run-enkf-mc-forecast","implicit":true,"key":"h1yOq9rp1q"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"The forecasting simulation is encapsulated in the ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"HqRRJN1em0"},{"type":"inlineCode","value":"run_enkf_mc_forecast","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"Ef4n18DFPU"},{"type":"text","value":" function, which manages the interaction between real-time observations and the deep learning model.","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"rnnIUuH6JA"}],"key":"b9fQiyjpmD"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":9,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"State Assimilation (EnKF):","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"tPtpFaKeS2"}],"key":"YiWRLIluIb"},{"type":"text","value":" For each time step, the function retrieves the current observations from the unscaled test data. If ","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"gjrKMup59w"},{"type":"inlineCode","value":"perform_enkf","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"jsHZk9XgHZ"},{"type":"text","value":" is enabled, the EnKF instance performs a prediction and update step. This process corrects the hydrological state variables (such as river discharge and nutrient levels) by balancing the model’s internal state with actual observed data, effectively reducing the “drift” often found in long-term time-series forecasting.","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"OVuYj6VO9o"}],"key":"Q665hDaCfc"}],"key":"RFl8I3uZL4"},{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Input Sequence Modification:","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"waP1jHjUZ4"}],"key":"aCcbelXhPg"},{"type":"text","value":" Once the EnKF provides an updated state, these values are passed through the fitted scaler. The function then takes the standard input sequence from ","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"YDKn3IMpPx"},{"type":"inlineCode","value":"X_test","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"fRAHHu0EQJ"},{"type":"text","value":" and replaces the state variables in the most recent time step with these assimilated values. This ensures the LSTM makes its next prediction based on the most accurate, ground-truth-adjusted data available.","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"UYLE2To8Jg"}],"key":"U8HhsmBSHB"}],"key":"wXuKLWdOuR"},{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"Probabilistic Prediction (MC Dropout):","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"FpuzqQNYMK"}],"key":"Pd9uu48O6t"}],"key":"YWd5I8adod"}],"key":"zF7wsV38jR"}],"key":"KN69hduILB"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":12,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"Uncertainty Quantification:","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"aNLg1z9fqA"}],"key":"oBNcWS0BGt"},{"type":"text","value":" If ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"N3OMjPB5ev"},{"type":"inlineCode","value":"perform_mc","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"DFLV5QEBFl"},{"type":"text","value":" is active, the model performs multiple forward passes (defined by ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"QdxeiHjF0M"},{"type":"inlineCode","value":"n_mc_samples","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"bgKajw0e8m"},{"type":"text","value":") for the same input sequence. By keeping dropout layers active during inference (","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"uEuRUefJH6"},{"type":"inlineCode","value":"training=True","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"b8FFnLnVHH"},{"type":"text","value":"), each pass produces a slightly different prediction, allowing us to capture the model’s predictive uncertainty.","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"NTAigpXgXi"}],"key":"Twn2ItldK4"}],"key":"CERGCngCDp"},{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"Standard Inference:","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"SiJeG7gjtz"}],"key":"dm9q0nQibG"},{"type":"text","value":" If MC Dropout is disabled, the model performs a standard ","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"xt6VWQBXco"},{"type":"inlineCode","value":"predict()","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"Xx30jx1lbn"},{"type":"text","value":" call, providing a single deterministic output.","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"KV5jUpEpLe"}],"key":"QLdSJh5nza"}],"key":"XBY4rLSaHn"}],"key":"sMB6F9bIkY"},{"type":"heading","depth":4,"position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"text","value":"Execution and Robustness","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"eTDe0tBBkb"}],"identifier":"execution-and-robustness","label":"Execution and Robustness","html_id":"execution-and-robustness","implicit":true,"key":"FvtVvpk9eu"},{"type":"paragraph","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"text","value":"The execution block verifies that all prerequisites—including the trained model, initialized EnKF, and noise matrices—are present before starting the loop.","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"rnWvPlr3m8"}],"key":"c84NFmhZ0I"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":21,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"Error Handling:","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"dFjSeYMpZz"}],"key":"kLVgAvRIuY"},{"type":"text","value":" The loop is wrapped in try-except blocks to manage potential issues during matrix inversions in the EnKF or scaling errors. If a critical failure occurs, the loop captures the error and returns the results generated up to that point.","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"lM0EwEzxPC"}],"key":"JZ9D4Eg7Yv"}],"key":"ya2B9VR16p"},{"type":"listItem","spread":true,"position":{"start":{"line":22,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"text","value":"Result Trimming:","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"FhGpboszpc"}],"key":"ZQgPS4Rl51"},{"type":"text","value":" To maintain data integrity, both the prediction results and the corresponding ground truth labels (","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"I4vF9DYhbw"},{"type":"inlineCode","value":"y_test","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"qlgX25ApPv"},{"type":"text","value":") are trimmed to the exact number of successfully completed iterations.","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"IDIQLVaPYD"}],"key":"KyqLjLSDHQ"}],"key":"OG1OwDXSrm"}],"key":"yH1qYkTUbI"},{"type":"paragraph","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"text","value":"Upon completion, the results are stored in ","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"key":"xCbN9OmwdG"},{"type":"inlineCode","value":"mc_predictions_results","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"key":"BOHfC5LSDz"},{"type":"text","value":". These raw probability outputs (either as single points or ensembles) serve as the foundation for the performance metrics and uncertainty analysis performed in ","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"key":"hFufVNnBft"},{"type":"strong","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"text","value":"Part 13: EnKF + MC Dropout Evaluation","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"key":"cTPwH4aC4q"}],"key":"Y4TFJDrrc2"},{"type":"text","value":".","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"key":"Ng1eqGpWVo"}],"key":"uAECgAOpvH"}],"identifier":"nb41vbfwmjoe","label":"Nb41VBFwmjOe","html_id":"nb41vbfwmjoe","key":"O96t4TTMcy"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FrayQMEpmkUG","outputId":"9ee71178-1ffc-4194-c866-8d12bcfef2d8","ExecuteTime":{"end_time":"2026-01-07T03:44:43.379234900Z","start_time":"2026-01-07T03:37:39.208476Z"}},"children":[{"type":"code","lang":"python","executable":true,"value":"def run_enkf_mc_forecast(model, enkf_instance, enkf_indices, R_matrix, Q_matrix,\n                         x_test, y_test, test_df_unscaled, scaler,\n                         seq_len, perform_enkf=True, perform_mc=True, n_mc_samples=50):\n    \"\"\"\n    Optimized Forecasting Loop with Vectorized MC Dropout.\n    \"\"\"\n    # 1. Validation & Setup\n    if model is None or x_test is None: return None, None\n    print(f\"\\n--- Starting Forecast (EnKF={perform_enkf}, MC={perform_mc}) ---\")\n\n    num_steps = len(y_test)\n    n_features = x_test.shape[2]\n\n    # Pre-allocate output array\n    # If MC is off, we still use shape (N, 1) for consistency\n    samples_col = n_mc_samples if perform_mc else 1\n    predictions = np.zeros((num_steps, samples_col))\n\n    # 2. Pre-fetch Scaler Parameters (Speed Optimization)\n    # Accessing scaler attributes inside the loop is slow. Do it once here.\n    if perform_enkf:\n        try:\n            # Handle StandardScaler vs RobustScaler\n            if hasattr(scaler, 'mean_'):\n                mu, sigma = scaler.mean_, scaler.scale_\n            elif hasattr(scaler, 'center_'):\n                mu, sigma = scaler.center_, scaler.scale_\n            else:\n                print(\"Error: Scaler not fitted.\"); return None, None\n\n            # Keep only the stats for the EnKF variables to avoid indexing in loop\n            enkf_mu = mu[enkf_indices]\n            enkf_sigma = sigma[enkf_indices]\n        except Exception as e:\n            print(f\"Scaler Error: {e}\"); return None, None\n\n    # 3. Main Loop\n    # EnKF must be sequential (step t depends on step t-1)\n    for i in tqdm(range(num_steps), desc=\"Forecasting\"):\n\n        # A. Current Timestamp Index\n        # The test_df index corresponding to the *end* of the current sequence\n        df_idx = i + seq_len - 1\n        if df_idx >= len(test_df_unscaled): break\n\n        # B. EnKF Update Step\n        updated_state = None\n        if perform_enkf:\n            # Get Observation (z)\n            z = test_df_unscaled[ENKF_STATE_VARS].iloc[df_idx].values\n\n            # 1. Predict (Move Ensemble Forward)\n            # Note: In a full Hybrid EnKF, the 'F' function would run the LSTM here.\n            # Here we use the persistence assumption for the state transition.\n            enkf_instance.predict(lambda x: x, Q=Q_matrix)\n\n            # 2. Update (Correct with Observation)\n            updated_state, _ = enkf_instance.update(z, R_matrix)\n\n        # C. Update LSTM Input\n        # We copy the sequence so we don't overwrite the original data\n        current_seq = x_test[i].copy() # Shape: (Seq_Len, Features)\n\n        if perform_enkf and updated_state is not None:\n            # Scale the updated state (Vectorized Math)\n            # (Raw - Mean) / Scale\n            scaled_state = (updated_state - enkf_mu) / enkf_sigma\n\n            # Inject into the LAST time step of the sequence\n            # We replace only the columns corresponding to EnKF variables\n            current_seq[-1, enkf_indices] = scaled_state\n\n        # D. LSTM Prediction (Vectorized MC Dropout)\n        # Prepare Tensor\n        # Shape: (1, Seq_Len, Features)\n        input_tensor = tf.convert_to_tensor([current_seq], dtype=tf.float32)\n\n        if perform_mc:\n            # OPTIMIZATION: Tile the input N times to create a batch\n            # New Shape: (n_mc_samples, Seq_Len, Features)\n            batch = tf.tile(input_tensor, [n_mc_samples, 1, 1])\n\n            # ONE single call to the model for all samples\n            # training=True enables Dropout\n            preds = model(batch, training=True)\n\n            # Store results (flatten to 1D array of probabilities)\n            predictions[i, :] = preds[:, 0].numpy()\n        else:\n            # Standard Inference (No Dropout)\n            pred = model(input_tensor, training=False)\n            predictions[i, 0] = pred[0, 0].numpy()\n\n    return predictions, y_test[:len(predictions)]\n\n# --- Execute ---\nif __name__ == \"__main__\":\n    if 'trained_model' in locals() and 'enkf_instance' in locals():\n        if PERFORM_ENKF and enkf_instance is None:\n            print(\"Warning: PERFORM_ENKF is True, but 'enkf_instance' not found.\")\n            print(\"Running in pure LSTM mode (EnKF disabled for this run).\")\n            # Temporarily disable for this function call to prevent crash\n            current_perform_enkf = False\n        else:\n            current_perform_enkf = PERFORM_ENKF\n\n        mc_preds, y_test_trimmed = run_enkf_mc_forecast(\n            model=trained_model,\n            enkf_instance=enkf_instance,\n            enkf_indices=enkf_indices,\n            R_matrix=R_matrix_enkf,\n            Q_matrix=Q_matrix_enkf,\n            x_test=X_test,\n            y_test=y_test,\n            test_df_unscaled=test_df,\n            scaler=scaler, # Ensure this matches your scaler variable name\n            seq_len=SEQUENCE_LENGTH,\n            perform_enkf=True,         # Set to False to test Baseline again\n            perform_mc=True,           # Set to True for Uncertainty\n            n_mc_samples=50            # 50 is a good balance for speed/accuracy\n        )\n\n        if mc_preds is not None:\n            print(f\"✅ Forecast Complete. Prediction Shape: {mc_preds.shape}\")\n    else:\n        print(\"Error: Prerequisites (Part 9 Model or Part 11 EnKF) missing.\")","identifier":"frayqmepmkug-code","enumerator":"16","html_id":"frayqmepmkug-code","key":"V8s74JgoCi"},{"type":"outputs","id":"3atdnadoKiu3jwu5XEx4q","children":[{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"\n--- Starting Forecast (EnKF=True, MC=True) ---\n"},"children":[],"identifier":"frayqmepmkug-outputs-0","html_id":"frayqmepmkug-outputs-0","key":"b5wWe0btHO"},{"type":"output","jupyter_data":{"output_type":"stream","name":"stderr","text":"Forecasting: 100%|██████████| 230/230 [00:05<00:00, 42.55it/s]"},"children":[],"identifier":"frayqmepmkug-outputs-1","html_id":"frayqmepmkug-outputs-1","key":"FNHAxssW42"},{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"✅ Forecast Complete. Prediction Shape: (230, 50)\n"},"children":[],"identifier":"frayqmepmkug-outputs-2","html_id":"frayqmepmkug-outputs-2","key":"SiVSJ9TqbJ"},{"type":"output","jupyter_data":{"output_type":"stream","name":"stderr","text":"\n"},"children":[],"identifier":"frayqmepmkug-outputs-3","html_id":"frayqmepmkug-outputs-3","key":"Jkl9GklJNz"}],"identifier":"frayqmepmkug-outputs","html_id":"frayqmepmkug-outputs","key":"x1lFh2aKg9"}],"identifier":"frayqmepmkug","label":"FrayQMEpmkUG","html_id":"frayqmepmkug","key":"OyU5PTZoBc"},{"type":"block","kind":"notebook-content","data":{"id":"xD0ko6o6mwsw"},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Part 13: EnKF + MC Dropout Evaluation","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fjZY41dnRa"}],"identifier":"part-13-enkf-mc-dropout-evaluation","label":"Part 13: EnKF + MC Dropout Evaluation","html_id":"part-13-enkf-mc-dropout-evaluation","implicit":true,"key":"P28Xsv7KUT"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"This final analytical section focuses on quantifying the performance of the integrated forecasting system. By evaluating the predictions generated in Part 12, we assess how the combination of Ensemble Kalman Filter (EnKF) state updates and Monte Carlo (MC) Dropout uncertainty estimation improves the model’s ability to predict red tide events.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Mx3nnXHxgl"}],"key":"AJnqplkR4C"},{"type":"heading","depth":4,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Core Evaluation Components","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"o6pVIvm25u"}],"identifier":"core-evaluation-components","label":"Core Evaluation Components","html_id":"core-evaluation-components","implicit":true,"key":"tEo35JGiPv"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"The evaluation framework is designed to move beyond simple point-accuracy and instead provide a comprehensive view of model reliability and confidence.","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"yBVeCsnQSK"}],"key":"Lb3z7WPKRa"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":9,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"Ensemble Statistics and Uncertainty:","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"Z9fwYKnIzc"}],"key":"N67snifUs4"},{"type":"text","value":"\nFor sessions where ","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"W6FqykYuT5"},{"type":"inlineCode","value":"PERFORM_MC_DROPOUT","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"UnhxxSHERi"},{"type":"text","value":" was enabled, the evaluation calculates the mean and standard deviation across all MC samples for each time step. The mean serves as the primary probabilistic forecast, while the standard deviation provides a quantifiable measure of model uncertainty. This allows us to identify periods where the model is highly confident versus periods where environmental volatility leads to wider prediction intervals.","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"oUX59Vcrbj"}],"key":"nxeHjXH2rK"}],"key":"wUsFPFzS3K"},{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"Binary Classification Performance:","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"aym0IxYmaz"}],"key":"md88m6jrx6"},{"type":"text","value":"\nSince the ultimate goal is to predict bloom occurrences, the probabilistic outputs are converted into binary classifications using an optimized threshold. Key performance metrics include:","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"wPWOkVqFtj"}],"key":"WIOWNRADKx"}],"key":"uIPnsWligs"}],"key":"ZB5ecQJB2X"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":13,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"Precision and Recall:","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"AlsTLKNbU7"}],"key":"x8ImnwwwWa"},{"type":"text","value":" Determining the model’s ability to correctly identify blooms while minimizing false alarms.","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"B3b2FLTsjo"}],"key":"bpYkOR2PF8"}],"key":"AJBV4i4y8l"},{"type":"listItem","spread":true,"position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"F1-Score:","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"IS5nk36c7O"}],"key":"AtSzVjR5Vl"},{"type":"text","value":" Providing a balanced metric that accounts for the inherent class imbalance in red tide data.","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"lzB3saKaf1"}],"key":"ZoIj9ya56X"}],"key":"KO4qoZqLL9"},{"type":"listItem","spread":true,"position":{"start":{"line":15,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"text","value":"Confusion Matrix:","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"ORWDBFhIu6"}],"key":"h5Ilmt73Th"},{"type":"text","value":" Visualizing the distribution of True Positives, True Negatives, False Positives, and False Negatives.","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"kcID2p6xKa"}],"key":"Yg8O0m3NZW"}],"key":"Y6Fxu2gtxP"}],"key":"gYNAEiPz79"},{"type":"list","ordered":true,"start":3,"spread":false,"position":{"start":{"line":18,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":18,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"text","value":"Probabilistic Analysis:","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"N7Ca5Kb2gj"}],"key":"rSb5jD3XHE"},{"type":"text","value":"\nThe evaluation includes the calculation of the ","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"h3cylGVQoI"},{"type":"strong","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"text","value":"Brier Score","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"EYpgGhuxzW"}],"key":"pKWPTxssYC"},{"type":"text","value":" to measure the accuracy of the probability forecasts and the ","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"RwJcvpp5G3"},{"type":"strong","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"text","value":"Area Under the Precision-Recall Curve (AUPRC)","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"xK6YQa8zHn"}],"key":"ASiC5Wcij8"},{"type":"text","value":". These metrics are particularly useful for evaluating models where the event of interest (the bloom) is relatively rare compared to non-bloom periods.","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"Xay4Xmnl80"}],"key":"eOybDVSH6E"}],"key":"IlMfAbxMaX"}],"key":"tvXLRD86so"},{"type":"heading","depth":4,"position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"Visualizing the Results","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"gh9qvo0iKl"}],"identifier":"visualizing-the-results","label":"Visualizing the Results","html_id":"visualizing-the-results","implicit":true,"key":"iM79VWjbPd"},{"type":"paragraph","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"The section generates a series of diagnostic plots to illustrate the system’s effectiveness:","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"Eycnx6YuBP"}],"key":"B468nvMKgb"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":25,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"text","value":"Forecast vs. Actuals:","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"ZT6ZJh7uQ8"}],"key":"PsTXqiAh4Z"},{"type":"text","value":" A time-series plot comparing the predicted probabilities (with uncertainty bands) against the actual observed bloom markers.","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"QeM3oFEzTE"}],"key":"BC5Chu72ys"}],"key":"AzS7XXehFc"},{"type":"listItem","spread":true,"position":{"start":{"line":26,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"children":[{"type":"text","value":"EnKF Impact Analysis:","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"key":"xTa6iQ0J2f"}],"key":"SCNuU4TltH"},{"type":"text","value":" Comparative visualizations showing how the state assimilation influenced the trajectory of the predictions compared to a baseline model without EnKF updates.","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"key":"cr6qK1uM6S"}],"key":"AAmT6GpTMC"}],"key":"NL1forBKws"}],"key":"AEe620ibNE"},{"type":"paragraph","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"children":[{"type":"text","value":"By the end of this evaluation, we can determine the specific “value-add” of the EnKF + MC Dropout approach, providing a clear picture of how well the hybrid system handles the complexities of ecological forecasting.","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"key":"mo1Xq6dpXv"}],"key":"P3V6lxWbbX"}],"identifier":"xd0ko6o6mwsw","label":"xD0ko6o6mwsw","html_id":"xd0ko6o6mwsw","key":"ofq3uzAW8x"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"BqhJYg-Mmy-5","outputId":"de544cc7-afaf-4f7e-cf08-f9443341df80","ExecuteTime":{"end_time":"2026-01-07T03:44:43.380234200Z","start_time":"2026-01-07T03:37:50.405109Z"}},"children":[{"type":"code","lang":"python","executable":true,"value":"from sklearn.calibration import calibration_curve\ndef evaluate_mc_predictions(mc_preds, y_test, test_df_unscaled, seq_len,\n                            forecast_horizon, model_name=\"EnKF_MC\",\n                            output_dir=\"output\", manual_threshold=None):\n    \"\"\"\n    Evaluates probabilistic predictions (Mean, Uncertainty, Calibration).\n\n    Args:\n        mc_preds: Array of shape (n_samples, n_mc_iterations) or (n_samples, 1)\n        y_test: True labels (n_samples,)\n        test_df_unscaled: DataFrame with timestamps\n        seq_len: Input sequence length (for aligning dates)\n        forecast_horizon: Forecast horizon (for aligning dates)\n        model_name: String for labeling files/plots\n        output_dir: Folder to save results\n        manual_threshold: Float (0.0-1.0) to force a specific decision boundary.\n                          If None, calculates optimal F1 threshold.\n    \"\"\"\n    print(f\"\\n=== Evaluating Uncertainty: {model_name} ===\")\n\n    # 1. Setup Output Directory\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Validation\n    if mc_preds is None or y_test is None: return None\n\n    # 2. Statistics Calculation\n    # -------------------------------------------------------\n    # Mean: The \"Best Guess\"\n    y_mean = np.mean(mc_preds, axis=1)\n\n    # Std Dev: The \"Disagreement\" (Model Uncertainty)\n    y_std = np.std(mc_preds, axis=1)\n\n    # 95% Confidence Interval\n    ci_lower = np.percentile(mc_preds, 2.5, axis=1)\n    ci_upper = np.percentile(mc_preds, 97.5, axis=1)\n\n    # Entropy (Uncertainty)\n    epsilon = 1e-10\n    y_mean_clipped = np.clip(y_mean, epsilon, 1-epsilon)\n    entropy = -(y_mean_clipped * np.log(y_mean_clipped) + (1-y_mean_clipped) * np.log(1-y_mean_clipped))\n\n    # 3. Threshold Logic\n    # -------------------------------------------------------\n    if manual_threshold is not None:\n        best_thresh = manual_threshold\n        print(f\"Using Manual Decision Threshold: {best_thresh:.4f}\")\n    else:\n        # Calculate Optimal F1 Threshold\n        precision, recall, thresholds = precision_recall_curve(y_test, y_mean)\n        numerator = 2 * precision * recall\n        denominator = precision + recall\n        with np.errstate(divide='ignore', invalid='ignore'):\n            fscore = np.nan_to_num(np.divide(numerator, denominator))\n\n        ix = np.argmax(fscore)\n        # Safety check if index is out of bounds\n        best_thresh = thresholds[ix] if ix < len(thresholds) else 0.5\n        print(f\"Optimal Threshold (Auto-F1): {best_thresh:.4f} (Max F1: {fscore[ix]:.4f})\")\n\n    # Apply threshold\n    y_pred_class = (y_mean >= best_thresh).astype(int)\n\n    # 4. Calculate Metrics\n    # -------------------------------------------------------\n    try:\n        metrics = {\n            'accuracy': float(accuracy_score(y_test, y_pred_class)),\n            'auc': float(roc_auc_score(y_test, y_mean)),\n            'brier': float(brier_score_loss(y_test, y_mean)),\n            'precision': float(precision_score(y_test, y_pred_class, zero_division=0)),\n            'recall': float(recall_score(y_test, y_pred_class, zero_division=0)),\n            'f1_score': float(f1_score(y_test, y_pred_class, zero_division=0)),\n            'avg_entropy': float(np.mean(entropy)),\n            'threshold_used': float(best_thresh)\n        }\n\n        print(f\"\\nPerformance Metrics:\")\n        print(f\"  Accuracy:    {metrics['accuracy']:.4f}\")\n        print(f\"  AUC-ROC:     {metrics['auc']:.4f}\")\n        print(f\"  Recall:      {metrics['recall']:.4f} (Caught {metrics['recall']*100:.1f}% of blooms)\")\n        print(f\"  Precision:   {metrics['precision']:.4f}\")\n        print(f\"  Brier Score: {metrics['brier']:.4f}\")\n    except Exception as e:\n        print(f\"Error calculating metrics: {e}\")\n        metrics = {}\n\n    # 5. PLOTTING (Publication Quality)\n    # -------------------------------------------------------\n    # Apply Style\n    plt.rcdefaults()\n    params = {'font.family': 'serif', 'figure.figsize': (10, 6), 'figure.dpi': 300}\n    plt.rcParams.update(params)\n\n    # Plot A: Forecast Time Series\n    start_idx = seq_len + forecast_horizon - 1\n    num_preds = len(y_mean)\n\n    if start_idx + num_preds <= len(test_df_unscaled):\n        dates = test_df_unscaled.index[start_idx : start_idx + num_preds]\n\n        plt.figure()\n        # Plot Actuals\n        plt.plot(dates, y_test[:num_preds], color='black', alpha=0.6, label='Actual Bloom', linewidth=1.5)\n        # Plot Mean Prediction\n        plt.plot(dates, y_mean[:num_preds], color='#d62728', label='Ensemble Mean', linewidth=2)\n        # Plot Confidence Interval\n        plt.fill_between(dates, ci_lower[:num_preds], ci_upper[:num_preds], color='#d62728', alpha=0.2, label='95% CI')\n        # Plot Threshold\n        plt.axhline(best_thresh, color='gray', linestyle='--', label=f'Threshold ({best_thresh:.2f})')\n\n        plt.title(f'Probabilistic Forecast ({model_name})')\n        plt.ylabel('Bloom Probability')\n        plt.legend(loc='upper right', frameon=True)\n        plt.grid(True, linestyle=':', alpha=0.5)\n        plt.tight_layout()\n\n        save_path = os.path.join(output_dir, f\"forecast_{model_name}.png\")\n        plt.savefig(save_path)\n        print(f\"Saved Plot: {save_path}\")\n        plt.show()\n\n    # Plot B: Calibration Curve\n    try:\n        prob_true, prob_pred = calibration_curve(y_test, y_mean, n_bins=10)\n        plt.figure(figsize=(6, 6))\n        plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly Calibrated')\n        plt.plot(prob_pred, prob_true, marker='o', color='blue', label=model_name)\n        plt.title('Calibration Curve (Reliability)')\n        plt.xlabel('Mean Predicted Probability')\n        plt.ylabel('Fraction of Positives (Actual)')\n        plt.legend()\n        plt.grid(True, linestyle=':', alpha=0.5)\n        plt.tight_layout()\n\n        save_path = os.path.join(output_dir, f\"calibration_{model_name}.png\")\n        plt.savefig(save_path)\n        print(f\"Saved Plot: {save_path}\")\n        plt.show()\n    except Exception as e:\n        print(f\"Skipping Calibration Plot: {e}\")\n\n    # Plot C: Entropy Distribution\n    try:\n        correct = (y_pred_class == y_test)\n        plt.figure(figsize=(8, 5))\n        sns.kdeplot(entropy[correct], fill=True, color='green', label='Correct Predictions')\n        sns.kdeplot(entropy[~correct], fill=True, color='red', label='Wrong Predictions')\n        plt.title('Predictive Entropy (Uncertainty Distribution)')\n        plt.xlabel('Entropy (Higher = More Uncertain)')\n        plt.legend()\n        plt.tight_layout()\n\n        save_path = os.path.join(output_dir, f\"entropy_{model_name}.png\")\n        plt.savefig(save_path)\n        print(f\"Saved Plot: {save_path}\")\n        plt.show()\n    except Exception as e:\n        print(f\"Skipping Entropy Plot: {e}\")\n\n    # 6. Return structured data\n    return {\n        'metrics': metrics,  # Clean dictionary for JSON\n        'arrays': {          # Arrays for CSV/Debug\n            'y_mean': y_mean,\n            'y_std': y_std,\n            'entropy': entropy,\n            'y_actual': y_test,\n            'ci_lower': ci_lower,\n            'ci_upper': ci_upper\n        }\n    }\n\n# --- Execute ---\nif __name__ == \"__main__\":\n    if 'mc_preds' in locals() and mc_preds is not None:\n\n        # 1. Run Evaluation (Try manual_threshold=0.3 if you want to fix lag)\n        results = evaluate_mc_predictions(\n            mc_preds=mc_preds,\n            y_test=y_test_trimmed,\n            test_df_unscaled=test_df,\n            seq_len=SEQUENCE_LENGTH,\n            forecast_horizon=FORECAST_HORIZON,\n            model_name=\"EnKF_LSTM_MC\",\n            output_dir=\"output\",\n            manual_threshold=None  # <--- Change to 0.3 to reduce lag!\n        )\n\n        # 2. Save Metrics to JSON\n        if results:\n            json_path = os.path.join(\"output\", \"metrics_enkf_mc.json\")\n            try:\n                with open(json_path, 'w') as f:\n                    json.dump(results['metrics'], f, indent=4)\n                print(f\"✅ Saved Metrics to: {json_path}\")\n            except Exception as e:\n                print(f\"Error saving JSON: {e}\")\n\n            # 3. Save Raw Data to CSV\n            csv_path = os.path.join(\"output\", \"predictions_enkf_mc.csv\")\n            try:\n                df_results = pd.DataFrame(results['arrays'])\n                # Add timestamp index if possible\n                start_idx = SEQUENCE_LENGTH + FORECAST_HORIZON - 1\n                if start_idx + len(df_results) <= len(test_df):\n                    df_results.index = test_df.index[start_idx : start_idx + len(df_results)]\n\n                df_results.to_csv(csv_path)\n                print(f\"✅ Saved Raw Data to: {csv_path}\")\n            except Exception as e:\n                print(f\"Error saving CSV: {e}\")\n\n    else:\n        print(\"Error: 'mc_preds' not found. Run Step 12 first.\")","identifier":"bqhjyg-mmy-5-code","enumerator":"17","html_id":"bqhjyg-mmy-5-code","key":"tWzCEjfHnQ"},{"type":"outputs","id":"eGj2CI0EryhGPc9u4s3e0","children":[{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"\n=== Evaluating Uncertainty: EnKF_LSTM_MC ===\nOptimal Threshold (Auto-F1): 0.8037 (Max F1: 0.8649)\n\nPerformance Metrics:\n  Accuracy:    0.9130\n  AUC-ROC:     0.9529\n  Recall:      0.8421 (Caught 84.2% of blooms)\n  Precision:   0.8889\n  Brier Score: 0.1236\nSaved Plot: output/forecast_EnKF_LSTM_MC.png\n"},"children":[],"identifier":"bqhjyg-mmy-5-outputs-0","html_id":"bqhjyg-mmy-5-outputs-0","key":"KDNWp3zqHd"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"<Figure size 3000x1800 with 1 Axes>","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"ea44aa14eb6f9f5bc8e9257ee43bc189","path":"/build/ea44aa14eb6f9f5bc8e9257ee43bc189.png"}}},"children":[],"identifier":"bqhjyg-mmy-5-outputs-1","html_id":"bqhjyg-mmy-5-outputs-1","key":"zC66cxeWbh"},{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"Saved Plot: output/calibration_EnKF_LSTM_MC.png\n"},"children":[],"identifier":"bqhjyg-mmy-5-outputs-2","html_id":"bqhjyg-mmy-5-outputs-2","key":"Qtxib3cUeg"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"<Figure size 1800x1800 with 1 Axes>","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"b775b9326344335f9f26b7411f2a23aa","path":"/build/b775b9326344335f9f26b7411f2a23aa.png"}}},"children":[],"identifier":"bqhjyg-mmy-5-outputs-3","html_id":"bqhjyg-mmy-5-outputs-3","key":"rnsjlw2efA"},{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"Saved Plot: output/entropy_EnKF_LSTM_MC.png\n"},"children":[],"identifier":"bqhjyg-mmy-5-outputs-4","html_id":"bqhjyg-mmy-5-outputs-4","key":"PLawXHnb9Z"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"<Figure size 2400x1500 with 1 Axes>","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"d016f356bf7188afa2fb7e4c31ac2c0c","path":"/build/d016f356bf7188afa2fb7e4c31ac2c0c.png"}}},"children":[],"identifier":"bqhjyg-mmy-5-outputs-5","html_id":"bqhjyg-mmy-5-outputs-5","key":"R2igfvAHuH"},{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"✅ Saved Metrics to: output/metrics_enkf_mc.json\n✅ Saved Raw Data to: output/predictions_enkf_mc.csv\n"},"children":[],"identifier":"bqhjyg-mmy-5-outputs-6","html_id":"bqhjyg-mmy-5-outputs-6","key":"raEW6xVi8R"}],"identifier":"bqhjyg-mmy-5-outputs","html_id":"bqhjyg-mmy-5-outputs","key":"Z8Z7eK8PnE"}],"identifier":"bqhjyg-mmy-5","label":"BqhJYg-Mmy-5","html_id":"bqhjyg-mmy-5","key":"cAiG5gyopU"},{"type":"block","kind":"notebook-content","data":{"id":"zIx8Y878hU4O"},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Part 14: SHAP Analysis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HMR5jJFSlR"}],"identifier":"part-14-shap-analysis","label":"Part 14: SHAP Analysis","html_id":"part-14-shap-analysis","implicit":true,"key":"HQs2M2fNWp"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"This section utilizes ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"ftT9UyfokX"},{"type":"strong","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"SHAP (SHapley Additive exPlanations)","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"OUk5RsKxkh"}],"key":"bMx39RisFW"},{"type":"text","value":" to interpret the LSTM model’s decision-making process. While deep learning models are often viewed as “black boxes,” SHAP values provide transparency by quantifying the contribution of each environmental feature to the final prediction.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"RDi7tKcsSk"}],"key":"QE7yXi6Knk"},{"type":"heading","depth":4,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Core Interpretability Components","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"Y4jgQ7Cku0"}],"identifier":"core-interpretability-components","label":"Core Interpretability Components","html_id":"core-interpretability-components","implicit":true,"key":"osnrCxDjv2"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"The analysis focuses on local and global feature importance to understand which drivers—such as nutrient concentrations, river discharge, or lagged bloom states—are most influential in triggering a red tide forecast.","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"tNU2kpeXET"}],"key":"paVbrx0M6p"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":9,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"DeepExplainer for LSTM:","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"LqogEHvTRV"}],"key":"sBu2Tj5SHE"},{"type":"text","value":"\nWe employ ","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"oAniJcKUvP"},{"type":"inlineCode","value":"shap.DeepExplainer","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"f5oR5990Iw"},{"type":"text","value":" to handle the complex, 3-dimensional temporal data characteristic of LSTMs. This approach approximates SHAP values by comparing the model’s output for specific test sequences against a “background” dataset (typically a representative subset of the training data). This reveals how specific fluctuations in environmental conditions shift the probability of a bloom.","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"kbtlGzeo0B"}],"key":"haAUDUq2P1"}],"key":"dwXEOzZJJt"},{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"Temporal Feature Contribution:","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"dXCAOcoL7a"}],"key":"tGNVQllYfG"},{"type":"text","value":"\nSince the LSTM processes sequences over time, SHAP analysis allows us to visualize which time steps within the input window are most critical. We can identify whether the model is reacting to immediate spikes in river discharge or if it is recognizing long-term patterns in nutrient accumulation across the entire sequence length.","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"ePBz1pFKs3"}],"key":"DcNYc5zljt"}],"key":"ENshoj0scb"},{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"Global Feature Importance:","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"ZioQx8ax5f"}],"key":"aFZ3ymv5RK"},{"type":"text","value":"\nBy aggregating the absolute SHAP values across the entire test set, we generate a global ranking of features. This is vital for ecological validation, as it allows us to verify if the model’s “reasoning” aligns with known biological drivers of ","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"AGQuKJ8spl"},{"type":"emphasis","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"Karenia brevis","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"saMJZrzElr"}],"key":"CzLhjbvlBp"},{"type":"text","value":", such as specific salinity levels or nitrogen-to-phosphorus ratios.","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"sIzYvIBXys"}],"key":"YIdFjElDU4"}],"key":"oNVax1oLn8"}],"key":"WshEX47vo8"},{"type":"heading","depth":4,"position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"Visualization and Insights","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"UiHOMDrmOX"}],"identifier":"visualization-and-insights","label":"Visualization and Insights","html_id":"visualization-and-insights","implicit":true,"key":"wYabu7bODL"},{"type":"paragraph","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"text","value":"The section produces high-impact visualizations to communicate these findings:","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"DtwSJzRB4w"}],"key":"Z8RqchhKOU"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":20,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"text","value":"SHAP Summary Plots:","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"UcMl2eosw4"}],"key":"aFaQoUQmRp"},{"type":"text","value":" A comprehensive view showing the magnitude and direction of each feature’s impact. For instance, it might reveal that high values of a specific lagged nutrient feature consistently push the model toward a “bloom” prediction.","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"ovUBT4VrZM"}],"key":"C37JkpnP7e"}],"key":"eDkSq0Zr8P"},{"type":"listItem","spread":true,"position":{"start":{"line":21,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"Force Plots / Waterfall Plots:","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"NXo5499Ej9"}],"key":"Fgo71c4JA3"},{"type":"text","value":" Detailed breakdowns of individual prediction samples, showing how different features “pushed” the model’s output from the base value to the final predicted probability.","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"ftvmqJ7oAS"}],"key":"ElvjbisjJw"}],"key":"AQyX3dQezF"}],"key":"qJEL0ZLpE6"},{"type":"paragraph","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"By integrating SHAP analysis, we transition from simply knowing ","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"IpZJMLfHRi"},{"type":"emphasis","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"what","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"y1Ak5rU7or"}],"key":"e4XuGWD5f6"},{"type":"text","value":" the model predicted to understanding ","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"WyS5OKiiZY"},{"type":"emphasis","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"why","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"vNuVVmas2M"}],"key":"zz832w04LI"},{"type":"text","value":" it made that prediction, providing essential context for environmental managers and researchers.","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"w1n4DOQFxk"}],"key":"wuIjphv1Gn"}],"identifier":"zix8y878hu4o","label":"zIx8Y878hU4O","html_id":"zix8y878hu4o","key":"SCKRwFF8A3"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["0eef66dc7b3b45f581fc334c6d33db34","03534a03d35d49f983c66f8bc1572b9c","eff0a5292d484968ba7e7bf3cef090c3","eecb5d87e890498487275b62e90adcd4","8b1224ba29d34b84b1981718c4b70ab0","c9dff54d4d6f4fde817049a8bc267cd1","f7a5380eec4d426ab054cfbfe967b776","b27cac4bb2d64bd090640e6a725c0057","486fa3ac2e2e4b2681e63ff0ba064f8e","ac29d24173054a929fc194828a9f1e60","e5f7e2ea050041ed9cc2891b4d3c3eea"]},"id":"94FzvL8sJeXN","outputId":"a28ceafe-8701-4021-8127-60c0c4016ead","ExecuteTime":{"end_time":"2026-01-07T03:44:43.385025500Z","start_time":"2026-01-07T03:37:51.737884Z"}},"children":[{"type":"code","lang":"python","executable":true,"value":"try:\n    # 1. Check & Fix Data Dimensions\n    # ---------------------------------------------------------\n    if 'X_test' not in locals() or 'trained_model' not in locals():\n        raise NameError(\"Data or Model missing. Please restart kernel and run Steps 1-9.\")\n\n    true_n_steps = X_test.shape[1]    # e.g., 12\n    true_n_features = X_test.shape[2] # e.g., 26\n\n    print(f\"Data Shape: {true_n_steps} steps, {true_n_features} features\")\n\n    # Fix the feature name list if it doesn't match the data\n    current_names = final_feature_columns_used if 'final_feature_columns_used' in locals() else []\n\n    if len(current_names) != true_n_features:\n        print(f\"Mismatch: {len(current_names)} names vs {true_n_features} data features.\")\n        diff = true_n_features - len(current_names)\n        # We assume the extra columns are usually appended at the end\n        plotting_feature_names = current_names + [f\"Extra_Feat_{i+1}\" for i in range(diff)]\n    else:\n        plotting_feature_names = current_names\n\n    # 2. Define Wrapper (The Fix)\n    # ---------------------------------------------------------\n    def model_wrapper(flat_data):\n        \"\"\"\n        Converts flat SHAP input back to 3D and queries the model\n        using Direct Call (not .predict) to avoid TF Graph errors.\n        \"\"\"\n        # Convert Numpy -> Tensor\n        # Reshape to (Samples, Time, Features)\n        reshaped = flat_data.reshape(-1, true_n_steps, true_n_features)\n        tensor_input = tf.convert_to_tensor(reshaped, dtype=tf.float32)\n\n        # Direct call (Eager Mode) - Faster and safer for SHAP\n        probs = trained_model(tensor_input, training=False)\n\n        # Return as Numpy flattened array\n        return probs.numpy().flatten()\n\n    # 3. Optimize Background (K-Means)\n    # ---------------------------------------------------------\n    print(\"Summarizing background data...\")\n    # Flatten train data: (Samples, Time*Feats)\n    X_train_flat = X_train.reshape(X_train.shape[0], -1)\n\n    # Summarize training data into 10 weighted points (Centroids)\n    background_summary = shap.kmeans(X_train_flat, 10)\n\n    # 4. Initialize KernelExplainer\n    # ---------------------------------------------------------\n    print(\"Initializing KernelExplainer...\")\n    explainer = shap.KernelExplainer(model_wrapper, background_summary)\n\n    # 5. Calculate SHAP Values\n    # ---------------------------------------------------------\n    explain_size = 20  # Keep small for speed\n    X_test_flat = X_test.reshape(X_test.shape[0], -1)\n\n    # Pick random samples to explain\n    idxs = np.random.choice(X_test_flat.shape[0], explain_size, replace=False)\n    X_test_sample = X_test_flat[idxs]\n\n    print(f\"Calculating SHAP values for {explain_size} instances...\")\n    # nsamples=auto usually works best, or try 500 if slow\n    shap_values = explainer.shap_values(X_test_sample, nsamples=500)\n\n    print(f\"SHAP calculation complete.\")\n\n    # 6. Reshape for Visualization\n    # ---------------------------------------------------------\n    # (Samples, Flat_Features) -> (Samples, Time, Features)\n    shap_values_3d = np.array(shap_values).reshape(-1, true_n_steps, true_n_features)\n    X_test_sample_3d = X_test_sample.reshape(-1, true_n_steps, true_n_features)\n\n    # 7. PLOT A: Global Feature Importance\n    # ---------------------------------------------------------\n    # Sum SHAP impact across all time steps\n    shap_sum_over_time = np.sum(shap_values_3d, axis=1) # (Samples, Features)\n    features_mean = np.mean(X_test_sample_3d, axis=1)   # (Samples, Features)\n\n    plt.figure(figsize=(10, 8), dpi=300)\n    shap.summary_plot(shap_sum_over_time, features_mean,\n                      feature_names=plotting_feature_names, show=False)\n    plt.title(\"Global Feature Importance\")\n    plt.tight_layout()\n    plt.savefig(\"output/shap_summary_global.png\")\n    plt.show()\n\n    # 8. PLOT B: Temporal Heatmap\n    # ---------------------------------------------------------\n    # Average absolute impact per time step\n    temporal_importance = np.mean(np.abs(shap_values_3d), axis=0) # (Time, Feats)\n\n    plt.figure(figsize=(12, 6), dpi=300)\n    sns.heatmap(temporal_importance.T, cmap='viridis',\n                yticklabels=plotting_feature_names,\n                xticklabels=range(true_n_steps))\n\n    plt.title(\"Temporal Feature Importance\\n(Brighter = Higher Impact at that Lag)\")\n    plt.xlabel(\"Time Lag (Steps into Sequence)\")\n    plt.ylabel(\"Feature\")\n    plt.tight_layout()\n    plt.savefig(\"output/shap_temporal_heatmap.png\")\n    plt.show()\n\n    print(\"✅ SHAP Analysis Completed Successfully.\")\n\nexcept Exception as e:\n    print(f\"❌ SHAP Failed: {e}\")\n    import traceback\n    traceback.print_exc()","identifier":"94fzvl8sjexn-code","enumerator":"18","html_id":"id-94fzvl8sjexn-code","key":"MXfBz0AXcD"},{"type":"outputs","id":"i6Vg9qAXD4HJ5ULlqS78c","children":[{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"Data Shape: 12 steps, 26 features\nMismatch: 25 names vs 26 data features.\nSummarizing background data...\nInitializing KernelExplainer...\nCalculating SHAP values for 20 instances...\n"},"children":[],"identifier":"94fzvl8sjexn-outputs-0","html_id":"id-94fzvl8sjexn-outputs-0","key":"mGep7XdwHh"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"  0%|          | 0/20 [00:00<?, ?it/s]","content_type":"text/plain"},"application/vnd.jupyter.widget-view+json":{"content":"{\"version_major\":2,\"version_minor\":0,\"model_id\":\"0eef66dc7b3b45f581fc334c6d33db34\"}","content_type":"application/vnd.jupyter.widget-view+json"}}},"children":[],"identifier":"94fzvl8sjexn-outputs-1","html_id":"id-94fzvl8sjexn-outputs-1","key":"GJZDo2XYyl"},{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"SHAP calculation complete.\n"},"children":[],"identifier":"94fzvl8sjexn-outputs-2","html_id":"id-94fzvl8sjexn-outputs-2","key":"YML9F6vhCx"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"<Figure size 2400x2850 with 2 Axes>","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"2f3397b85fc3a9579412e046c615d152","path":"/build/2f3397b85fc3a9579412e046c615d152.png"}}},"children":[],"identifier":"94fzvl8sjexn-outputs-3","html_id":"id-94fzvl8sjexn-outputs-3","key":"HbkvDHMJ6j"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"<Figure size 3600x1800 with 2 Axes>","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"06e8a9bca8f286989d47cee486aaa748","path":"/build/06e8a9bca8f286989d47cee486aaa748.png"}}},"children":[],"identifier":"94fzvl8sjexn-outputs-4","html_id":"id-94fzvl8sjexn-outputs-4","key":"JY76iqlBxf"},{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"✅ SHAP Analysis Completed Successfully.\n"},"children":[],"identifier":"94fzvl8sjexn-outputs-5","html_id":"id-94fzvl8sjexn-outputs-5","key":"zVe4bxKELb"}],"identifier":"94fzvl8sjexn-outputs","html_id":"id-94fzvl8sjexn-outputs","key":"lO3RTm81h8"}],"identifier":"94fzvl8sjexn","label":"94FzvL8sJeXN","html_id":"id-94fzvl8sjexn","key":"Xxo3IOVQQJ"},{"type":"block","kind":"notebook-content","data":{"id":"Zkmoj-onIxNy"},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Part 15: Time-Lagged Cross-Correlation (TLCC) Analysis","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DsdyQEB7G7"}],"identifier":"part-15-time-lagged-cross-correlation-tlcc-analysis","label":"Part 15: Time-Lagged Cross-Correlation (TLCC) Analysis","html_id":"part-15-time-lagged-cross-correlation-tlcc-analysis","implicit":true,"key":"ktlaBWL8lm"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"This section introduces ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Xu8H6Ktpnk"},{"type":"strong","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Time-Lagged Cross-Correlation (TLCC)","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"bWCHd4wnqf"}],"key":"J5z03RoL4N"},{"type":"text","value":" to investigate the dynamic relationships between environmental drivers and red tide concentrations. Unlike standard correlation, TLCC identifies leads and lags in the data, revealing how long it takes for a change in an environmental factor (like a nutrient spike) to manifest as a biological response (a bloom).","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"pWNs7xa8yr"}],"key":"nAXhHtXa77"},{"type":"heading","depth":4,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Core Analytical Components","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"HWLwLfXgZB"}],"identifier":"core-analytical-components","label":"Core Analytical Components","html_id":"core-analytical-components","implicit":true,"key":"mdCoZItoa6"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"The analysis provides a temporal roadmap of the ecosystem’s behavior, helping to validate the choice of lookback windows used in the LSTM model.","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"yK6VzPZmrD"}],"key":"KvUQtTPMf7"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":9,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"Identifying Temporal Offsets:","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"tmpeqShfHh"}],"key":"NfiYDAt4s4"},{"type":"text","value":"\nTLCC calculates the correlation between two time series—such as river discharge and ","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"f38kY2FmmG"},{"type":"emphasis","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"Karenia brevis","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"Ic7k4CNcIq"}],"key":"GkmmpshxtF"},{"type":"text","value":" abundance—at various time shifts. By finding the “peak” correlation, we can determine the specific latency of the system. For example, if the peak correlation with Phosphorus occurs at a lag of 14 days, it suggests a two-week window for biological uptake and population growth.","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"omS4qMegkH"}],"key":"iY5bUP2uuE"}],"key":"kCvBNAUDK3"},{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"Directionality of Influence:","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"KtSP5cHK5g"}],"key":"OW9fBdMVPC"},{"type":"text","value":"\nBy observing whether the peak correlation occurs at a positive or negative lag, we can confirm the causal direction. A positive lag (where the environmental variable precedes the bloom) confirms the variable as a leading indicator or “driver,” whereas a zero lag suggests a simultaneous response.","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"ZqiRSi4QHH"}],"key":"llQ4111tMj"}],"key":"IJa3SowqSk"},{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"Stability and Seasonality:","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"U638n8FKHt"}],"key":"lN2ZqJEYuA"},{"type":"text","value":"\nThe analysis can be extended to ","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"sPufDnFmfd"},{"type":"strong","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"Windowed TLCC","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"j4Bl8D1LbO"}],"key":"AaL5FyZb8X"},{"type":"text","value":", which evaluates how these correlations change over time. This is particularly relevant for Florida’s coastlines, where the relationship between rainfall and red tide may shift between the wet and dry seasons.","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"e1ouW4Fl76"}],"key":"gAUrZGNm6p"}],"key":"LYyS3VqJen"}],"key":"kZK7WQduDh"},{"type":"heading","depth":4,"position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"Insights and Model Refinement","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"GP22PjAIqk"}],"identifier":"insights-and-model-refinement","label":"Insights and Model Refinement","html_id":"insights-and-model-refinement","implicit":true,"key":"eYGtZCjmgJ"},{"type":"paragraph","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"text","value":"The results from this section serve as a critical bridge between data science and marine biology:","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"k3mOTY8ckj"}],"key":"WdyFWH0LJQ"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":20,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"text","value":"Feature Engineering Validation:","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"uxgV3m7FKx"}],"key":"Q9PIkjkngS"},{"type":"text","value":" We use these findings to refine the ","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"qJc1Hsug1f"},{"type":"inlineCode","value":"lag_days","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"apZskay2ah"},{"type":"text","value":" parameters in Part 4, ensuring the model focuses on the most biologically relevant timeframes.","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"w0aL5iBiK5"}],"key":"vmR1HGzguo"}],"key":"kTmCgCwcpH"},{"type":"listItem","spread":true,"position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"System Memory:","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"dSyjk02khC"}],"key":"liPeYhjNVb"},{"type":"text","value":" TLCC helps quantify the “memory” of the ecosystem, indicating how long an environmental disturbance continues to influence bloom dynamics.","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"txMi5r3kWI"}],"key":"sAV10TkNSn"}],"key":"Rgz3emsVkD"},{"type":"listItem","spread":true,"position":{"start":{"line":22,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"text","value":"Predictive Confidence:","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"YkCWO0paND"}],"key":"Do7lni6DAf"},{"type":"text","value":" Identifying features with strong, stable leads increases our confidence in the LSTM’s ability to provide early warnings before a bloom is visible in satellite or field data.","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"q1L4yuFWHC"}],"key":"BZ12KhtQRu"}],"key":"ubSAMW2eoi"}],"key":"ZaMlY32uJJ"},{"type":"paragraph","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"text","value":"By incorporating TLCC, we move beyond static snapshots to a fluid, temporal understanding of the drivers behind red tide events.","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"key":"oJLfPRS7Uj"}],"key":"arTTwVI9w5"}],"identifier":"zkmoj-onixny","label":"Zkmoj-onIxNy","html_id":"zkmoj-onixny","key":"VRGDa5FeST"},{"type":"block","kind":"notebook-code","data":{"id":"lOJZAAjMLdQx","ExecuteTime":{"end_time":"2026-01-07T03:44:43.386027800Z","start_time":"2026-01-07T03:37:59.138408Z"},"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"dd624703-0f2d-419f-bed5-d143620904e8"},"children":[{"type":"code","lang":"python","executable":true,"value":"def analyze_onset_lag(y_true, y_pred_prob, threshold=0.3, tolerance=4):\n    \"\"\"pynb i\n    Calculates exactly how many weeks late (or early) the model predicts blooms.\n\n    Args:\n        y_true: Binary actuals (0/1)\n        y_pred_prob: Continuous probabilities (0.0-1.0)\n        threshold: Decision threshold\n        tolerance: Max weeks to look for a matching predicted onset\n    \"\"\"\n    print(f\"\\n--- Onset Latency Analysis (Threshold: {threshold}) ---\")\n\n    y_pred_class = (y_pred_prob >= threshold).astype(int)\n\n    # 1. Identify \"Start\" events (0 -> 1 transitions)\n    # We use diff() to find where value changes from 0 to 1\n    actual_onsets = np.where(np.diff(y_true, prepend=0) == 1)[0]\n    pred_onsets = np.where(np.diff(y_pred_class, prepend=0) == 1)[0]\n\n    print(f\"Actual Bloom Starts found: {len(actual_onsets)}\")\n    print(f\"Predicted Bloom Starts found: {len(pred_onsets)}\")\n\n    if len(actual_onsets) == 0:\n        print(\"No blooms in test set to analyze.\")\n        return\n\n    # 2. Match Actual Starts to Nearest Predicted Start\n    lags = []\n\n    for t_actual in actual_onsets:\n        # Find predicted onsets within 'tolerance' weeks window\n        # We look for the closest prediction around the actual event\n        nearby_preds = pred_onsets[np.abs(pred_onsets - t_actual) <= tolerance]\n\n        if len(nearby_preds) > 0:\n            # Find the closest one\n            closest_pred = nearby_preds[np.argmin(np.abs(nearby_preds - t_actual))]\n\n            # Lag = Predicted Time - Actual Time\n            # Positive = Late (Lag)\n            # Negative = Early Warning (Lead)\n            lag = closest_pred - t_actual\n            lags.append(lag)\n\n            status = \"LATE\" if lag > 0 else \"EARLY\" if lag < 0 else \"PERFECT\"\n            print(f\"  Event at Week {t_actual}: Model is {status} by {abs(lag)} weeks.\")\n        else:\n            print(f\"  Event at Week {t_actual}: MISSED COMPLETELY (No prediction within {tolerance} weeks)\")\n            lags.append(np.nan) # Missed event\n\n    # 3. Summary Stats\n    lags_clean = [l for l in lags if not np.isnan(l)]\n    if lags_clean:\n        avg_lag = np.mean(lags_clean)\n        print(f\"\\n>>> AVERAGE LATENCY: {avg_lag:.2f} Weeks\")\n        if avg_lag > 0:\n            print(\"    (Positive = The model is LAGGING)\")\n        else:\n            print(\"    (Negative = The model gives EARLY WARNING)\")\n    else:\n        print(\"\\n>>> No matched events found.\")\n\n    # 4. Visualizing the Lag\n    # Cross-Correlation Plot (Statistical Proof)\n    # Shift predictions back/forward and see where correlation is highest\n    shifts = range(-5, 6) # Shift -5 to +5 weeks\n    correlations = []\n    for s in shifts:\n        # Shift predicted probabilities\n        if s < 0:\n            p_shifted = y_pred_prob[-s:]\n            y_shifted = y_true[:s]\n        elif s > 0:\n            p_shifted = y_pred_prob[:-s]\n            y_shifted = y_true[s:]\n        else:\n            p_shifted = y_pred_prob\n            y_shifted = y_true\n\n        # Calculate correlation\n        if len(y_shifted) > 0:\n            corr = np.corrcoef(y_shifted, p_shifted)[0, 1]\n            correlations.append(corr)\n        else:\n            correlations.append(0)\n\n    plt.figure(figsize=(8, 5), dpi=300)\n    plt.bar(shifts, correlations, color='skyblue', edgecolor='black')\n\n    # Highlight the max\n    max_idx = np.argmax(correlations)\n    best_lag = shifts[max_idx]\n    plt.bar(best_lag, correlations[max_idx], color='red', label=f'Peak Correlation (Lag={best_lag})')\n\n    plt.title(\"Time-Lagged Cross-Correlation (TLCC)\")\n    plt.xlabel(\"Lag (Weeks)\\n<-- Model Leads (Good) | Model Lags (Bad) -->\")\n    plt.ylabel(\"Correlation with Actuals\")\n    plt.axvline(0, color='black', linestyle='--')\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(\"output/lag_analysis.png\")\n    plt.show()\n\n# --- Execute ---\nif __name__ == \"__main__\":\n    if 'mc_preds' in locals() and 'y_test_trimmed' in locals():\n        # Get mean probs\n        probs = np.mean(mc_preds, axis=1)\n\n        # Run with your chosen threshold (e.g., 0.3)\n        analyze_onset_lag(y_test_trimmed.flatten(), probs.flatten(), threshold=0.3)","identifier":"lojzaajmldqx-code","enumerator":"19","html_id":"lojzaajmldqx-code","key":"a5J0yJ0D1u"},{"type":"outputs","id":"De6kxsm2jT-A2qTCSBLRS","children":[{"type":"output","jupyter_data":{"output_type":"stream","name":"stdout","text":"\n--- Onset Latency Analysis (Threshold: 0.3) ---\nActual Bloom Starts found: 10\nPredicted Bloom Starts found: 4\n  Event at Week 8: Model is PERFECT by 0 weeks.\n  Event at Week 70: MISSED COMPLETELY (No prediction within 4 weeks)\n  Event at Week 98: MISSED COMPLETELY (No prediction within 4 weeks)\n  Event at Week 103: MISSED COMPLETELY (No prediction within 4 weeks)\n  Event at Week 112: MISSED COMPLETELY (No prediction within 4 weeks)\n  Event at Week 168: Model is LATE by 1 weeks.\n  Event at Week 179: MISSED COMPLETELY (No prediction within 4 weeks)\n  Event at Week 182: MISSED COMPLETELY (No prediction within 4 weeks)\n  Event at Week 190: MISSED COMPLETELY (No prediction within 4 weeks)\n  Event at Week 196: MISSED COMPLETELY (No prediction within 4 weeks)\n\n>>> AVERAGE LATENCY: 0.50 Weeks\n    (Positive = The model is LAGGING)\n"},"children":[],"identifier":"lojzaajmldqx-outputs-0","html_id":"lojzaajmldqx-outputs-0","key":"Oc1kCgNGFf"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"<Figure size 2400x1500 with 1 Axes>","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"03025f3dd8a708372714dfae593a7247","path":"/build/03025f3dd8a708372714dfae593a7247.png"}}},"children":[],"identifier":"lojzaajmldqx-outputs-1","html_id":"lojzaajmldqx-outputs-1","key":"RKDFabfrlL"}],"identifier":"lojzaajmldqx-outputs","html_id":"lojzaajmldqx-outputs","key":"gWxF4vMId5"}],"identifier":"lojzaajmldqx","label":"lOJZAAjMLdQx","html_id":"lojzaajmldqx","key":"x5Jq5iYAnh"}],"key":"AtFpzAbabX"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{}},"domain":"http://localhost:3000"}